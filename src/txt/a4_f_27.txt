procedures or comments). The basic types of events are creations, deletions, or modifications. Given a procedure P it can be annotated as follows:P: (Ei,Ej)The semantics for this notation are that P exists from after event Ei to before event Ej. In fact, an object can have an ordered sequence of intervals attached to it, and most environmental changes are realized very simply in terms of these annotations. The representation allows project objects (i.e. entities, relationships and attributes—see Figure 9.5) to be shared across any number of project states. It also delivers a mapping between the project objects and events, and this is used to provide a flexible way of referring to an event or state: only the project objects which it must or must not contain need be mentioned. The user can jump to any previous version of the system and continue on from there (a generalization of part of the undo function of INTERLISP) by referring to the desired state in any one of a number of different ways.  Page 228Figure 9.5 A Taxonomy of software development concepts in ESDE-P (from Trenouth, 1990b)The implemented prototype of this approach to Exploratory System Development Environments (ESDEs),ESDE-P, has a number of other features such as hierarchical change logging, plan and problem posting, stepwise abstraction support, and intelligent housekeeping advice to the developer (e.g. "You should write code the for parent/2—mentioned in ancestor"). These other features take account of the context in which they are invoked. Thus the output of the advice component will depend on the current position of the programmer in the version tree.In addition to providing us with another example of a formally based approach to environment development, Trenouth's system is an example of an attempt to build a basic framework to support exploratory programming. This sort of emphasis comes with a well-defined basis and can be contrasted with approaches that can be characterized as collections of functional components integrated into a coherent whole. Both types of approach are required, and are required to be properly integrated in order to provide the software support environments that the AI-software engineer sorely needs.  Page 229An engineering toolboxA second metaphor often used in conjunction with attempts to get the computer to assist the programmer is that of an environment being a repository for powerful 'tools'—"Power tools for programmers" (Sheil, 1983) is the classic paper here. The 'assistant' metaphor emphasizes the the active role that the computer may play, offering unsolicited help, advice and words of caution. The 'toolbox' metaphor promotes the more passive possibilities: a homogeneous collection of powerful tools just sitting there in the environment simply waiting for the system developer to select one and use it. And, although each individual tool may or may not involve Al, the toolbox approach is the less contentious one in that the human system designer initiates tool usage, whereas the software assistant has the scope to interrupt normal proceedings—the desirability of such functionality is a debatable issue. But, the toolbox approach also contains less potential for exploiting the environment idea. Having separated these two conceptions of how we might develop support environments, let me say that this convenient distinction is not adhered to by exponents of these notions—the terminology is mixed and muddled, so be warned.Clearly some of the functions of our moderately stupid assistant are (or can be) entirely passive tools. The supersecretary function, for example, can be an extensive database management function that only retrieves specific information on request. This would be simply a tool under my classification scheme.The seminal exposition of this general viewpoint (as mentioned earlier) is that of Sheil (1983). He doesn't mention Al much but talks instead of "large, complex programs whose implementations will require significant resources. Their more interesting similarity, however, is that it is extremely difficult to give complete specifications for any of them. The reasons range from sheer complexity (the circuit designer can't anticipate all the ways in which his design rules will interact), through continually changing requirements (the equipment in the oil rig changes, as do the information bases that the government department is required to consult), to the subtle human factors issues that determine the effectiveness of [say] an interactive graphics interface" p. 574 of Rich and Waters, 1986, reprint (as are all further quotations from Sheil). If you recall our earlier discussion of the software-specification problem (Chapter 3) and the myth of complete specification, then you will realize that I don't fully share Sheil's view of the basis for lack of complete specifications. But to whatever cause we attribute the problem, there is agreement that there is  Page 230a need to build software systems in the absence of a complete and precise specification.Given this need, he suggests that "some applications are best thought of as design problems, rather than implementation projects. These problems require programming systems that allow the design to emerge from experimentation with the program, so that design and program develop together" (p. 577).He notes that the first advance towards a power-packed personal programming environment was the inclusion within a single shell (i.e. the environment) of standard features of the programming milieu—text editors, file management software, electronic mail, etc.As Barstow and Shrobe (1984) say in their concluding paper in Interactive Programming Environments (Barstow, Shrobe and Sandewall, 1984): "the idea of a programming environment is clearly an evolving one; what we today consider to be a programming environment will probably be considered primitive and limited by future standards" (p. 559). They provide a perspective on the developmental history of programming environments. It portrays this development as a move from initially just harnessing the raw power of the computer to handle large volumes of data rapidly and accurately (i.e. the supersecretary function) to producing moulds for how system development should be structured. The "mould" metaphor is much too static; modern environments provide a flexible and dynamic set of constraints that favor certain styles of software development. Barstow and Shrobe offer us a tripartite classification:1. Environments concerned with the entire life cycle of a program or system. These environments are generally focused on managing the development of large systems built by many programmers. They serve as the repository of all information on the system and thus contain databases of the relevant information from each stage of the development process. The authors add: "Finally, they assume that a system's specifications are clearly understood before coding begins" (p. 560).2. Environments concerned primarily with the coding phase and whose tools are relatively independent from each other. This is the classic 'toolbox' environment—a collection of goodies bundled together.3. Environments which regard coding, debugging, testing and maintenance as a single process of program development through incremental enrichment. This type of environment is  Page 231contrasted with (1) above within which these subprocesses (or sub-tasks) are viewed as separate modules with distinct boundaries.It is clearly the type 3 environment that is demanded when we want to engineer AI software, but we would also want to see such type 3 environments extended to include also the type 1 capability of being able to accommodate large systems that many people have worked on. Barstow and Shrobe focus their discussion on state-of-the-art LISP environments which are by no means the whole story but they do constitute the most advanced robust and reliable environments (as opposed to fragile demonstration systems) currently available.A survey of the state of the art in the UK is provided by Beretons (1988) edited collection Software Engineering Environments which, as its name tells us, concentrates on the use of IPSEs (Integrated Project Support Environments) for conventional software development—roughly, type 1 environments.One last point about software support environments is that it appears to be the case that support for effective exploratory programming is also support for speculative code hacking. For example, the Interlisp-D environment allows broken computations to be back-stepped and restarted after making, say, some minor code alteration. There is no need to restart the computation from the beginning and no need to recompile after a modification to the code. This sort of support makes a code-and-fix approach incredibly quick and easy, and thus tempting (in the short term). So one of the (many) challenges for the software support environment developer is to decouple these two modes of software development such that the environment both encourages good practice and discourages, or ideally prohibits, bad practice. The environment needs to function as an intelligent filter, letting good software-development activities pass smoothly through but straining off and thereby blocking the undesirable ones. But can this be done? It may not be easy, however it's probably not impossible; it might even be quite easy to eliminate certain elements of bad practice, and any step in this direction has to be beneficial in the long run.As support environments become more comprehensive the filter role should be easier to realize. If, for example, program abstractions such as design documents are represented within the system, then an appropriate filtering strategy might be to allow substantial code changes only after design representation changes. This sort of thing is quite easy to say but difficult to implement in a satisfactory manner, but bear in mind that a good partial implementation could be valuable—it's not an all-or-nothing  Page 232strategy. What we see here is a second (and perhaps secondary) role for software support environments: they should reduce the effective complexity of software development, and they might also be used to curb bad practice.Self-reflective softwareApart from a drive to realize sophisticated support environments, are there other targets to aim for in our efforts to engineer practical AI software? There seem to be at least two major features of the AI software itself that will repay close attention—one highly speculative, and the other quite down to earth. The title of this section introduces the more nebulous possibility.One of the reasons for suggesting that complete life-cycle environments have the potential to do much to alleviate the problems of AI-soft-ware development is that if the environment mediates in the system design, say, then there is the possibility of syphoning off 'design knowledge' for later reuse in subsequent activities, such as system maintenance. What we hope to capitalize on here is an environment that can assist in system maintenance because it 'knows about' how and why the system was designed the way it was. If this is viewed as a viable (although quite long-term) possibility, then the next step is to move this knowledge about the whys and wherefores of a given software system's structure from the environment and into the software system itself. Such a system would then possess the basic information to become self-reflective (the term metaknowledge is used in the expert systems' world). Such a system would contain the potential to consider and reflect upon its own functioning and structure. If this begins to suggest that rather awkward and doomed situation which involves ever-decreasing circles, then remember that there is no requirement of completeness here. Such a system would not have to know everything about itself (whatever that might mean). It would be very handy for it to know just some useful things about itself, although probably the more the better.Why might we want such self-reflective software? The obvious reason is exactly the same as the reason for wanting design and development knowledge within a complete life-cycle environment, viz. it should provide a basis for automating support of all subsequent operations on the software system. The extra step that we've now taken is to place this metaknowledge within the software system itself rather than leave it in the development environment. The advantage of this is that the software  Page 233system is self-contained, with its own 'power pack' as it were, but the disadvantage is that every piece of software would be much bigger and much more complex as a result.In order to bring the discussion in this section down to earth, let me suggest that such self-reflectivity (in a crude form, I admit) is manifest in many expert systems. A number of expert systems boast a self-explanation capability, as we saw in Chapter 8. When the system asks a question, the user can ask 'why' in response, and the expert system will then explain the reasoning behind the particular question that was originally asked. In addition, it is typically possible for the user to query an expert system's decision by asking 'how'. In which case the expert system will explain the line of reasoning that led to the specific decision that was queried. In truth, this is only a rather straightforward trace capability which just happens to work quite impressively in conjunction with a logical, rule-based inferencing mechanism. Rather more generously, we might say that it is one of the triumphs of current expert systems' technology that a useful self-reflective capability can be so easily implemented.Overengineering softwareThe more down-to-earth approach that I want to explore a little is that of overengineering AI software. If we can't prove that a piece of software is correct, and we can't (you must know my biases by now), then we should do what is done with every other engineered product: we should build in some redundancy. We've already touched on this possibility when we discussed differences between software as an engineered artefact, and the more traditional products of the engineer (Chapter 1).The main problem is that, unlike most products of traditional engineering, we don't know how to overengineer software systems. The terms 'overengineering' or 'adding redundancy' might be the same in these two domains, but it seems that the processes needed to implement them are totally different. Addition of redundancy seems to score a bullseye on several of the major differences between software systems and bridges, buildings, or motor cars. The functional complexity of software systems, together with the narrow bounds of acceptable variation associated with some of these functions, seems to mean that significant general redundancy cannot be added to software systems by any one (or few) simple expedient—like doubling the size of main structural beams say.  Page 234A number of approaches to this problem have been tried, and they all help to some extent, but what they have all failed to do is to provide a straightforward general strategy for increasing software reliability through redundancy.The addition of extra checks and traps is one route to software reliability that has been, and always should be, employed. In its simplest form it amounts to the system developer overengineering the system by threading the basic system with a mesh of detailed check statements and error-trapping functions. For example, if a variable AV is supposed to contain the average of a set of values, say, A1 through to A10 then there are a number of checks that could be implemented. We might check that the value of AV is larger than or equal to the smallest value averaged, and that it is smaller than or equal to the largest. This check does not, of course, guarantee that the value of AV is indeed the correct average, but it will catch certain errors that may occur in the averaging procedure, and so does provide some measure of redundancy.This thoroughly pragmatic approach to programming, which assumes that errors will inevitably occur, has long been sanctioned and encouraged in software engineering. Yourdon (1975), for example, calls it ''antibugging" or "defensive programming", and devotes a whole chapter to the programming principles and guidelines that will contribute to this practice. As a strategy to promote software reliability, it may be viewed as good practice, which the programmer must institute making the best of whatever facilities a particular programming language contains. But, more positively, programming languages can be designed to provide sophisticated support explicitly for defensive programming. This defensive programming can be, and has been, given official status in certain (usually unofficial versions of) programming languages. Thus an ASSERT statement has been added to the language Pascal. This is a statement that allows the programmer to assert that a certain relationship is true. So the earlier example might be programmed as:ASSERT(Aleast <= AV <= Amost)The semantics of this sort of statement are that when during program execution the control flow encounters an ASSERT statement the boolean expression it contains is evaluated. The value should be TRUE or FALSE. If the value is TRUE then no action is taken and control flow passes on to the next statement. But if the expression evaluates as FALSE then an error (or at: least warning) interrupt is initiated.  Page 235The use of such a strategy for overengineering programs can be 'enforced' by the development of appropriate principles of programming, such as: one fifth of computation statements in a program should be ASSERT statements.This example also provides us with an example of why such ad hoc overengineering is not completely satisfactory. Firstly, the check is at the level of fine implementation detail. In order to add this check you have to get right down to the fine details of the implemented system. Secondly, it is an ad hoc test. There are many other similar ones that could have been used instead or as well—which do you choose and where do you stop? Thirdly, such checks will make the system larger and slower, and are quite likely to miss the real problems in the software. This is because the very fact that the system designer (or implementor) dreamed up and added this check means that he or she consciously thought about the possibility of the averaging function malfunctioning in this particular way. But all too often, the source of the insidious system errors is the unusual combination of circumstances that no one consciously foresaw. So, by its very nature, it is unlikely that a check for this error condition will have been built in.A relevant, blossoming field of logic programming is that of constraint logic programming (CLP)—see Cohen (1990) for a survey of CLP. There are many rather different routes that development of this notion can, and probably will, take, but of interest here is the view that CLP encompasses the notion of automatically asserted constraints in a Prolog-style language. Put simplistically, instead of proving the truth of, say, A (from A:-B and the truth of B as in standard Prolog), some constraints to the truth of this statement may be automatically generated. So A might be true when, say, "X < 23" if we assume that B (and perhaps A) contains the variable "X". In general, we thus generate, not simply truth values, but truth subject to certain constraints. To return to our sort example, the query:sort([Xl,X2,X3l, S)?would fail in standard Prolog, because unbound variables (i.e. X1,X2 and X3) cannot be sorted—i.e, compared using the <= operator. But in CLP we can expect the result:S=[X1,X2,X3], XI<=X2, X2<=X3  Page 236That is, the sorted list S is [X1,X2,X3] when the constraints XI<=X2 and X2<=X3 are satisfied.One of the promises of this enhancement of standard Prolog is that it provides a neat and natural way for the system to automatically tell us the detailed conditions under which the computation is correct. Instead of the programmer having to devise these detailed constraints on the correctness of the algorithm (i.e. the ASSERT statements in effect), they are generated automatically as part of the computation. In CLP we get the result of the computation and an explicit statement of the detailed conditions under which it is correct. And although these constraints are not redundant in the sense that ASSERT statements are, they do represent additional information which the programmer can use to ensure system reliability. The drawback is, of course, that the correctness of the constraints depends upon the correctness of the algorithms from which they were generated rather than upon some (at least partially) independent source, like the programmer's second thoughts. In this sense they are a weaker strategy than assertional programming.In an effort to provide a more independent checking mechanism, a result can be recomputed and the two values can be compared to see if we get exactly the same value twice. Nothing is easier. We can make a copy of the code and compute all significant values twice, but this would not solve our problem: it will just mean that the system makes all the same errors twice. We must add useful redundancy; not just any redundancy. If the programmer were to redesign and recode critical functions twice over then the results of these two different computations could indeed be usefully compared.A frequently used technique for improving software system reliability in this manner is through replication of the critical software components together with a 'comparator' module that cross-checks for consistency and might accept, for example, agreement in two out of three replicated channels. The replication envisaged is not simple duplication, but an exploitation of software diversity "which is a particular form of redundancy involving the design and coding of separate software for each of the replicated channels" (Smith and Wood, 1989, p. 161). This approach to reliability through redundancy is also called N version programming. Figure 9.6 illustrates this sort of approach to software system redundancy.  Page 237Figure 9.6 Simple software diversity providing system redundancyBetter still, have two different programmers independently code alternative versions of the same function. This should eliminate idiosyncratic oversights. Even better have the two programmers use totally different languages, say a procedural language like Pascal and a declarative language like Prolog. This should result in totally different computations, and hence eliminate errors associated with a particular 'style' of computation as imposed by the specific nature of a given programming language.This last strategy has been explored in the context of developing a bifacial programming language (Smartt, 1983, and Adams and Smartt, 1985). The prototype language was a loose amalgam of Pascal and Prolog such that a given problem could be implemented in each language, and at strategic points within the calculations intermediate values could be checked for equivalence. This is a neat-sounding idea for overengineering software. It is tree that 'double-programming' each problem is not quite as simple and straightforward as, say, doubling the size of the main supports of a bridge, but, given the nature of software artefacts, it may be pointless to hope for anything that simple. Unfortunately, this nice idea of having two very different, intermeshed computations cross-checking each other does not work too well. And the reason is simple: if you have two totally different computations (one procedural and one declarative, in this particular case) computing the same overall function, there may be few intermediate stages within the two computations that are usefully comparable, and even fewer that are easily identified as comparable. The  Page 238bifacial language used abstract data types to solve the problem of determining when two results were equal when their concrete representations differed. But this approach to the when-to-compare problem did put an excessive burden on the programmer, who first had to master a non-trivial algebra for abstract data types in order to properly exploit the consistency-checking feature of the language. Figure 9.7 is a schematic illustration of the redundancy promised by such a biracial language. Figure 9.7 Redundancy through 'bifacial' computationA subsequent thought was that a Prolog program might be used to check a given result, a result given by a procedural, say Pascal, computation. This is a sequential use of double-programming redundancy, but it is perhaps not quite as bad as a doubling of the simple-program computation time, for, intuitively, one might expect checking a given result to be faster than actually computing the result. This project then evolved into a study of the relative efficiency of checking a given result in Prolog and computing that result. Guthrie (1985) showed that the time-complexity of verifying a result (i.e. checking a given result) is never greater than the time-complexity of finding that result (i.e. computing the result) with the same program. She also characterized a substantial class of Prolog programs for which these two time-complexities are the same, and showed that there is no algorithm for deciding whether or not a Prolog program will verify a result in less time that it would take to compute that same result. Figure 9.8 provides a schematic illustration of this possibility for achieving software reliability through useful redundancy.  Page 239Figure 9.8 A declarative check on a procedural computationIn an effort to avoid the problems of this type of overengineering at the level of programming detail, there have been attempts to introduce the necessary redundancy at an earlier stage in the overall process. The general idea is to design useful redundancy into the system long before the questions of implementation details are addressed. This sort of approach to software reliability, quite outside of concerns for Al software, has long been advocated by some software specialists (see, for example, Randell, 1975). Randell suggests a scheme of "stand-by sparing" in which there are built-in checks for critical system components, and when a check fails a 'spare' software component is switched in to deal with the particular set of circumstances that caused the failure. Such failures, which are assumed to be exceptional, are taken to be due to "residual design inadequacies" (Randell, 1975, p. 438).He goes on to describe two important characteristics of a "recovery block scheme for achieving software fault tolerance by means of standby sparing":1. It incorporates a general solution to the problem of switching to the use of the spare component, i.e. of repairing any damage done by the erroneous main component, and of transferring control to the appropriate spare component.2. It provides a method of explicitly structuring the software system which has the effect of ensuring that the extra software involved in  Page 240the error detection and in the spare components does not add to the complexity of the system, and so reduce rather than increase overall system reliability. (Randell, 1975, p. 438)So, what we now see as crucial to the production of reliable AI software (i.e. designing in useful redundancy) has in some sectors of the conventional software community long been viewed as a route to software reliability. It is interesting to see in these old papers the hedge that this approach is needed until such time as proofs of program correctness can be applied to practical software systems. It is worth pointing out, at this juncture, that a belief in the possibility of formal verification of algorithms undermines the incentives to explore methods for designing in useful redundancy. The provision of any form of redundancy has a cost (in system size, complexity, or execution time), and if the system can be guaranteed to be correct then its plain silly to pay the price of redundancy. (The reader might also recall the conspicuous lack of testing strategies in treatises on the 'science' of programming—see Chapter 3 for a memory jog if necessary.) Clearly, if the ship is unsinkable, then lifeboats are an unnecessary expense. No one would accept this argument about ships. Why is it any less unpalatable when applied to software systems? I'm not sure, but I suspect it is something to do with the confusion between mathematical abstractions and software systems as engineered artefacts.Once more the object-oriented paradigm provides us with an up-to-date perspective on the assertion-based approach to software reliability. The programming language Eiffel and its environment, from Interactive Software Engineering Inc., "apply the concepts of object-oriented design and programming to the construction of high-quality software" (Meyer, 1989, in abstract). Eiffel offers us reusability (as we've already considered within OOP), extendibility (Eiffel programs are readily modifiable—another OOP goodie), and reliability through various means including assertions and the traditional approach of strict static type checking. It is, of course, reliability through systematic use of assertions that I shall focus on here.  Page 241In Eiffel, assertions may be used in the following roles:1. As routine preconditions to express conditions that must be satisfied whenever a routine is called.2. As routine postconditions that express conditions which are guaranteed to be true when a routine terminates (if the precondition was satisfied on entry).3. As class invariants that must be satisfied by objects of the class at all times.In order to avoid the run-time penalty of assertion-checking, such monitoring, at two levels (preconditions only or all assertions), may be enabled or not individually for each particular class. A violated assertion will trigger an exception which will cause an error message and termination unless customized exception-handling has been provided.At first sight, it seems that Eiffel offers us merely an integrated and slightly more elaborate assertional capability than we had in Pascal many years ago. But there is at least one substantial advance which begins to address the problem of the low-level of traditional assertional checks. This particular innovation results from a combination of assertional checking and the OOP notion of inheritance, which (in Eiffel) encompasses the idea of deferred classes. A deferred class is a class that contains at least one deferred routine, which is a routine whose implementations will only be provided by descendents (and thus need not be provided until later). Deferred classes allow Eiffel to function as a high-level design language, i.e. certain general features of the deferred class can be specified without the necessity to provide a specific, detailed implementation. Yet pre- and post-condition assertions can be associated with a deferred routine, and invariant assertions with deferred classes. Clearly, there is some scope here for a high-level assertional capability at the design stage, and, in addition, because Eiffel is an efficiently executable language, we gain the benefits of a fast prototyping tool.Meyer (1989, p. 10) provides the following example of the use of Eiffel; it illustrates the design-level assertional capability.  Page 242deferred class VEHICLE exportdues paid, valid plate, register, ...featuredues_paid(year:INTEGER):BOOLEAN is...end; - duespaidvalid_plate(year:INTEGER) :BOOLEAN is...end; - valid-plateregister(year:INTEGER) is- Register vehicle for yearrequiredues\_paid(year)deferredensure valid_plate(year)end;—register... Other features ...end—class VEHICLEIn this example there is no commitment to the details of the registration algorithm for a vehicle, or indeed to the assumption that a single algorithm will apply to all vehicles—cars, motorcycles, etc. But whatever the details of the actual registration procedures, the same assertions can be made (a precondition introduced by require, and a postcondition introduced by ensure).So Eiffel can be employed as a high-level design language by exploiting the notion of deferred classes and routines. A first version may be composed of deferred classes and routines (in effect, a design). Later refinements will then supply details to replace the deferred components. Further features such as polymorphism and late binding,which permit considerable type flexibility, give the programmer significant freedom to design systems using deferred classes and routines.Let me conclude this chapter by drawing your attention to the fact that, although overengineering (under various alternative names) has long been recognized as a route to reliable software systems, we have yet to see good generalized approaches to the problem emerge. And finally, let me point you back to the section Reusable software in Chapter 6, and in particular to Halpern's (1990) argument for the avoidance of "fresh" code as a key to software reliability: in terms of this approach the redundancy resides in the repeated successful usage of a piece of code and not in the  Page 243structure of the code itself. And the associated costs are removed from the actual software system itself; they manifest themselves in the proper maintenance of the library of reusable software components, which seems like a much better place to put them.  Page 245CHAPTER 10Summary and What the Future HoldsIn this final, wrap-up chapter, I shall draw together all that has been discussed in the previous chapters and, in addition, I shall bring some other, broader issues to your attention. Software development is not a purely technical issue. Software products operate in society, but general acceptance of this viewpoint has been sluggish, to say the least.Let's take what we might call the internal societal problems of software development first. Many software systems are built by a team of people, and not by just one lone hacker, as we often seem to tacitly assume. Once a group of people are involved certain problems are reduced (e.g. each programmer has less code to write and something less than the full system to manage conceptually), but certain new problems emerge (e.g. the task of maintaining consistency between modules designed and developed by different people). The pros and cons of software teams versus single virtuoso performances is a much debated issue, and even more so is the optimum size of team for particular sorts of tasks. I shall not reopen the debate here. My point is just to bring to your attention the fact that the awkward technical problems that we have been grappling with throughout this book can become further aggravated by the presence of a software development team in place of a single-handed approach.Outside of the team, the society of software developers, there is yet another world, and an even bigger and more problematic one. But before we plunge into it, I shall spare a few words for the essential link man, the manager who is typically the project's interface with the external world.  Page 246This person, who is typically not a computer technologist, can be a major source of headaches to the project team. System reliability, which is in no small part founded on extensive careful testing and subsequent examination of the outcomes, can be threatened by the manager who sees only a deadline for delivery. The manager also knows that by the time many of the residual bugs are making their presence known to the community of users, he or she will be somewhere else and not indisputably responsible for any particular bug anyway. So the team-manager interaction can be an important determinant of the quality of the software produced.Good software is software that satisfies intangible user needs, and it's almost axiomatic that a 'frozen' software system, however good, will soon fail to give satisfaction. Most users are only human (some software vendors might dispute this), and given a good piece of software their attention and interest quickly moves on to extensions of the current task that are possibly, or almost, solvable with the system at hand. We can hope that self-adaptive software, when it arrives, will allow the investigative user to retrain the current version of the system so as to satisfy this desire for something ''just a little bit different".The unfortunate fact that software systems have to operate in conjunction with some sectors of humanity and are supposed to satisfy their demands raises a further important point of difference between software as an engineered artefact and buildings, bridges,and tin-openers. In Chapter 1 we looked at exactly this question of similarity and difference, but we did not consider the impact of human inquisitiveness (which is perhaps just a nice way of saying "human perversity").Give a man (or woman for that matter) a good tool for the job at hand and he or she will soon find a smart way to misuse it, or will break it in the attempt. Some would claim that this sort of activity is the hallmark of intelligence—man is not only the tool-using animal par excellence, he and she are the creative tool-misusing animals. Well, our behavior with software tools is no different, except that our propensity for change is more easily satisfied: there are many, more readily apparent ways to misuse a software tool than say a tin-opener. A bridge is a bridge, and although it's true that you can hang banners on it and jump off it, etc., there are only so many different ways to (mis)use it. More importantly, very few of these misuses will threaten the designed function of the bridge (target practice from a battleship springs to mind as an exception, but there are few of them).But give a user a new piece of software and it will soon be being used in ways that the system designer never dreamed of—a database system, for example, will be required to manipulate data structures that the  Page 247system designer could not have imagined even in the wildest of dreams. The malleability of software systems can contribute to this problem because the user is tempted to make small (but potentially fundamental) changes in the software. The delivery of object code rather than source code is the means by which this particular embellishment to the general perversity is stamped out. But, given that the software is itself effectively unalterable, there will be many ways to misuse it, and ways that misuse its basic functionality—i.e, not just hanging banners on a bridge. Software is soft—but it's also brittle. It is easy to modify, but difficult to modify correctly.This is the problem of the open functionality of software systems, and it goes some way towards accounting for the seeming unreliability of software when compared to the products of conventional engineering. The wide variety of innovative uses (or misuses) of a software system will, sooner or later (usually sooner), uncover a weakness (if not an honest-to-goodness dormant bug) in the system. And sometimes, the reason is that the system designer did not foresee, and could not reasonably have foreseen, the particular set of circumstances under which the software has crashed. As one system designer succinctly put it: "Writing code isn't the problem; understanding the problem is the problem" (quoted by Curtis, Krasner and Iscoe, 1988, p. 1271). A thorough understanding of a software system within its full range of possible user environments is an open-ended problem. So, yet again, we see that the difference in reliability is not due to a lack of proper developmental principles on the part of software developers; it is, in no small part, due to the nature of human users combined with the open functionality of software systems—a property that few conventionally engineered artefacts happen to possess.This societally driven problem seems, at first sight, to point to a contradiction in my attempted characterization of software-engineered artefacts vis-a-vis conventional artefacts. In Chapter 1 I claimed that a distinguishing characteristic of software systems was their tight functionality—i.e. software only works properly when component functions are operating within certain, narrow bounds. But, in reality, the open functionality of software does not contradict the tight-functionality characteristic. The former refers to the opportunities, almost an invitation, to try to misuse software systems, and the latter refers to the problems that this may cause with respect to certain functions within the software. There's no contradiction, but there is interaction. Users will invariably use the system in ways never intended by the system designer: sometimes they get away with and sometimes they don't. The big problems occur when they think they are getting away with it, but in fact they are not.  Page 248Use of sophisticated life-cycle environments is, I have advocated, a (perhaps the) 'answer' to our problems in trying to engineer Al software—although it's not much of an answer, more another set of problems as I'm sure you're well aware by now. An environment that persists throughout the use and maintenance stages of a software system does, of course, provide a context for that software. Such an environment can therefore be constructed to 'filter' the impacts of the external users. This should help alleviate some of the worst problems, but that is perhaps all, for the environment itself is just another software system and will thus exhibit open-functionality vulnerability.As an example of current concerns about these larger issues, the ones beyond the purely technical problems, we can look at the work of Curtis and "The Design Process Group" at MCC. Curtis, Krasner, Shen and Iscoe (1987) criticize software development models for focusing on the objects that exist at the end of each phase of the model (e.g. requirements specification, detailed design) and for failing to describe the actual processes that occur during software development—i.e, models, such as the waterfall model, fail to treat software development as a problem-solving process. "It is our primary thesis that this focus should be on activities that account for the most variation in software productivity and quality'' (p. 98). And for Curtis et al. this means empirical research based on large system development projects which are developed within a context of individuals, teams, projects, and even companies. In Curtis, Krasner and Iscoe (1988), they illustrate these successively wider contexts as a series of layers (see Figure 10.1) and present the results of a field study of seventeen large software projects.In sum, they maintain that the traditional, artefact-oriented, software development model must be integrated with a model that "analyzes the behavioral processes of systems development at the cognitive, team, project, and company levels. When we layer these behavioral processes on the growth process of artefacts, we begin to see the causes for bottlenecks and inefficiencies in the systems development process" (p. 98). "The process of developing large software systems must be treated, at least in part, as a learning and communication process... Design is in part an educational process, trial and error learning (prototyping) is only one approach to education, albeit a powerful one... From observations gathered on actual projects, we have concluded that processes such as learning, technical communication, negotiation, and customer interaction are among those crucial to project success which are poorly provided for in most existing models" (p.102-103). Curtis et al. (1988) present one  Page 249potential set of relationships that may be present when building software systems (Figure 10.2). Figure 10.1 "The layered behavioral model of software development" from Curtis et al. (1968), Figure 1, p. 1269To take another, but methodologically similar viewpoint, one that again suggests that no small part of the answer to the software reliability problem is to be found in the people concerned rather than in purely technical considerations, an IBM employee told me when questioned about his software team's approach to the reliability problem: "Well-trained, talented, motivated and dedicated programmers are the only solution" (an opinion that occurs repeatedly in the field studies of Curtis et al., 1988). This may be a silver bullet, but one that's difficult to lay your hands on most of the time. This particular problem is aggravated by the fact that programming is typically not viewed by companies as a long-term career job which involves an apprenticeship and substantial training, and, until it is, this particular road to reliable software will remain little used.  Page 250Figure 10.2 "Knowledge domains involved in system building" from Curtis et al., 1988, Figure 4, p. 1273Educating software engineers is problematic. The root of the problems is that of 'size'. Example projects in a formal educational environment are necessarily small-scale and of short duration. A move to the real world of large-scale, long-term, widely used software causes completely new problems to emerge. The successful code hacker learns to view actual coding as about 90 per cent of the problem, whereas in reality it is more like 1 per cent of the large-scale commercial problem. Similarly, in the practice situation the system builder is usually also the user. When these two roles are filled by different individuals, who have no direct contact with each other, the fundamental problem of satisfying user needs changes beyond recognition. Again societal issues, which are difficult to address within the constraints of a formal educational context, distort and disfigure the purely technical ones.Returning once more to the extra problems raised by the possibility of AI-software, I should mention the philosophy espoused by Winograd  Page 251  We can calculate the approximate dynamics of this increase and decrease in schema instances as follows. LetH be a schema with at least one instance present in the population at time t. Let m(H,t) be the number ofChapter 1: Genetic Algorithms: An Overview21instances of H at time t, and let Û (H,t) be the observed average fitness of H at time t (i.e., the average fitnessof instances of H in the population at time t). We want to calculate E(m(H, t + 1)), the expected number ofinstances of H at time t + 1. Assume that selection is carried out as described earlier: the expected number ofoffspring of a string x is equal to ƒ(x)/ where ƒ(x)is the fitness of x and   is the average fitness of the population at time t. Then, assuming x isin the population at time t, letting x Î H denote "x is an instance of H," and (for now) ignoring the effects ofcrossover and mutation, we have(1.1)by definition, since Û(H, t) = (£xÎH ƒ(x))/m(H,t)for x in the population at time t. Thus even though the GA doesnot calculate Û(H,t) explicitly, the increases or decreases of schema instances in the population depend on thisquantity.Crossover and mutation can both destroy and create instances of H. For now let us include only thedestructive effects of crossover and mutation—those that decrease the number of instances of H. Includingthese effects, we modify the right side of equation 1.1 to give a lower bound on E(m(H,t + 1)). Let pc be theprobability that single-point crossover will be applied to a string, and suppose that an instance of schema H ispicked to be a parent. Schema H is said to "survive" under singlepoint crossover if one of the offspring is alsoan instance of schema H. We can give a lower bound on the probability Sc(H) that H will survive single-pointcrossover:where d(H) is the defining length of H and l is the length of bit strings in the search space. That is, crossoversoccurring within the defining length of H can destroy H (i.e., can produce offspring that are not instances ofH), so we multiply the fraction of the string that H occupies by the crossover probability to obtain an upperbound on the probability that it will be destroyed. (The value is an upper bound because some crossoversinside a schema's defined positions will not destroy it, e.g., if two identical strings cross with each other.)Subtracting this value from 1 gives a lower bound on the probability of survival Sc(H). In short, theprobability of survival under crossover is higher for shorter schemas.These disruptive effects can be used to amend equation 1.1:(1.2)Chapter 1: Genetic Algorithms: An Overview22This is known as the Schema Theorem (Holland 1975; see also Goldberg 1989a). It describes the growth of aschema from one generation to the next. The Schema Theorem is often interpreted as implying that short,low-order schemas whose average fitness remains above the mean will receive exponentially increasingnumbers of samples (i.e., instances evaluated) over time, since the number of samples of those schemas thatare not disrupted and remain above average in fitness increases by a factor of Û(H,t)/ƒ(t) at each generation.(There are some caveats on this interpretation; they will be discussed in chapter 4.)The Schema Theorem as stated in equation 1.2 is a lower bound, since it deals only with the destructiveeffects of crossover and mutation. However, crossover is believed to be a major source of the GA's power,with the ability to recombine instances of good schemas to form instances of equally good or betterhigher-order schemas. The supposition that this is the process by which GAs work is known as the BuildingBlock Hypothesis (Goldberg 1989a). (For work on quantifying this "constructive" power of crossover, seeHolland 1975, Thierens and Goldberg 1993, and Spears 1993.)In evaluating a population of n strings, the GA is implicitly estimating the average fitnesses of all schemasthat are present in the population, and increasing or decreasing their representation according to the SchemaTheorem. This simultaneous implicit evaluation of large numbers of schemas in a population of n strings isknown as implicit paralelism (Holland 1975). The effect of selection is to gradually bias the samplingprocedure toward instances of schemas whose fitness is estimated to be above average. Over time, theestimate of a schema's average fitness should, in principle, become more and more accurate since the GA issampling more and more instances of that schema. (Some counterexamples to this notion of increasingaccuracy will be discussed in chapter 4.)The Schema Theorem and the Building Block Hypothesis deal primarily with the roles of selection andcrossover in GAs. What is the role of mutation? Holland (1975) proposed that mutation is what prevents theloss of diversity at a given bit position. For example, without mutation, every string in the population mightcome to have a one at the first bit position, and there would then be no way to obtain a string beginning with azero. Mutation provides an "insurance policy" against such fixation.The Schema Theorem given in equation 1.1 applies not only to schemas but to any subset of strings in thesearch space. The reason for specifically focusing on schemas is that they (in particular, short,high-average-fitness schemas) are a good description of the types of building blocks that are combinedeffectively by single-point crossover. A belief underlying this formulation of the GA is that schemas will be agood description of the relevant building blocks of a good solution. GA researchers have defined other typesof crossover operators that deal with different types of building blocks, and have analyzed the generalized"schemas" that a given crossover operator effectively manipulates (Radcliffe 1991; Vose 1991).The Schema Theorem and some of its purported implications for the behavior of GAs have recently been thesubject of much critical discussion in the GA community. These criticisms and the new approaches to GAtheory inspired by them will be reviewed in chapter 4.THOUGHT EXERCISES1. How many Prisoner's Dilemma strategies with a memory of three games are there that arebehaviorally equivalent to TIT FOR TAT? What fraction is this of the total number of strategies witha memory of three games?2. Chapter 1: Genetic Algorithms: An Overview23What is the total payoff after 10 games of TIT FOR TAT playing against (a) a strategy that alwaysdefects; (b) a strategy that always cooperates; (c) ANTI-TIT-FOR-TAT, a strategy that starts out bydefecting and always does the opposite of what its opponent did on the last move? (d) What is theexpected payoff of TIT FOR TAT against a strategy that makes random moves? (e) What are the totalpayoffs of each of these strategies in playing 10 games against TIT FOR TAT? (For the randomstrategy, what is its expected average payoff?)3. How many possible sorting networks are there in the search space defined by Hillis's representation?4. Prove that any string of length l is an instance of 2l different schemas.5. Define the fitness ƒ of bit string x with l = 4 to be the integer represented by the binary number x.(e.g., f(0011) = 3, ƒ(1111) = 15). What is the average fitness of the schema 1*** under ƒ? What is theaverage fitness of the schema 0*** under ƒ?6. Define the fitness of bit string x to be the number of ones in x. Give a formula, in terms of l (the stringlength) and k, for the average fitness of a schema H that has k defined bits, all set to 1.7. When is the union of two schemas also a schema? For example, {0*}*{1*} is a schema (**), but{01}*{10} is not. When is the intersection of two schemas also a schema? What about the differenceof two schemas?8. Are there any cases in which a population of n l-bit strings contains exactly n × 2l different schemas?COMPUTER EXERCISES(Asterisks indicate more difficult, longer-term projects.)1. Implement a simple GA with fitness-proportionate selection, roulettewheel sampling, population size100, single-point crossover rate pc = 0.7, and bitwise mutation rate pm = 0.001. Try it on thefollowing fitness function: ƒ(x) = number of ones in x, where x is a chromosome of length 20.Perform 20 runs, and measure the average generation at which the string of all ones is discovered.Perform the same experiment with crossover turned off (i.e., pc = 0). Do similar experiments, varyingthe mutation and crossover rates, to see how the variations affect the average time required for the GAto find the optimal string. If it turns out that mutation with crossover is better than mutation alone,why is that the case?2. Implement a simple GA with fitness-proportionate selection, roulettewheel sampling, population size100, single-point crossover rate pc = 0.7, and bitwise mutation rate pm = 0.001. Try it on the fitnessfunction ƒ(x) = the integer represented by the binary number x, where x is a chromosome of length 20.Chapter 1: Genetic Algorithms: An Overview24Run the GA for 100 generations and plot the fitness of the best individual found at each generation aswell as the average fitness of the population at each generation. How do these plots change as youvary the population size, the crossover rate, and the mutation rate? What if you use only mutation(i.e., pc = 0)?3. Define ten schemas that are of particular interest for the fitness functions of computer exercises 1 and2 (e.g., 1*···* and 0*···*). When running the GA as in computer exercises 1 and 2, record at eachgeneration how many instances there are in the population of each of these schemas. How well do thedata agree with the predictions of the Schema Theorem?4. Compare the GA's performance on the fitness functions of computer exercises 1 and 2 with that ofsteepest-ascent hill climbing (defined above) and with that of another simple hill-climbing method,"random-mutation hill climbing" (Forrest and Mitchell 1993b):1. Start with a single randomly generated string. Calculate its fitness.2. Randomly mutate one locus of the current string.3. If the fitness of the mutated string is equal to or higher than the fitness of the original string,keep the mutated string. Otherwise keep the original string.4. Go to step 2.Iterate this algorithm for 10,000 steps (fitness-function evaluations). This is equal to thenumber of fitness-function evaluations performed by the GA in computer exercise 2 (withpopulation size 100 run for 100 generations). Plot the best fitness found so far at every 100evaluation steps (equivalent to one GA generation), averaged over 10 runs. Compare this witha plot of the GA's best fitness found so far as a function of generation. Which algorithm findshigher-fitness chromosomes? Which algorithm finds them faster? Comparisons like these areimportant if claims are to be made that a GA is a more effective search algorithm than otherstochastic methods on a given problem.5. *Implement a GA to search for strategies to play the Iterated Prisoner's Dilemma, in which the fitnessof a strategy is its average score in playin 100 games with itself and with every other member of thepopulation. Each strategy remembers the three previous turns with a given player. Use a population of20 strategies, fitness-proportional selection, single-point crossover with pc = 0.7, and mutation withpm = 0.001.a. See if you can replicate Axelrod's qualitative results: do at least 10 runs of 50 generationseach and examine the results carefully to find out how the best-performing strategies workand how they change from generation to generation.b. Chapter 1: Genetic Algorithms: An Overview25Turn off crossover (set pc = 0) and see how this affects the average best fitness reached andthe average number of generations to reach the best fitness. Before doing these experiments, itmight be helpful to read Axelrod 1987.c. Try varying the amount of memory of strategies in the population. For example, try a versionin which each strategy remembers the four previous turns with each other player. How doesthis affect the GA's performance in finding high-quality strategies? (This is for the veryambitious.)d. See what happens when noise is added—i.e., when on each move each strategy has a smallprobability (e.g., 0.05) of giving the opposite of its intended answer. What kind of strategiesevolve in this case? (This is for the even more ambitious.)6. *a. Implement a GA to search for strategies to play the Iterated Prisoner's Dilemma as incomputer exercise 5a, except now let the fitness of a strategy be its score in 100 games withTIT FOR TAT. Can the GA evolve strategies to beat TIT FOR TAT?b. Compare the GA's performance on finding strategies for the Iterated Prisoner's Dilemma withthat of steepest-ascent hill climbing and with that of random-mutation hill climbing. Iteratethe hill-climbing algorithms for 1000 steps (fitness-function evaluations). This is equal to thenumber of fitness-function evaluations performed by a GA with population size 20 run for 50generations. Do an analysis similar to that described in computer exercise 4.Chapter 1: Genetic Algorithms: An Overview26Chapter 2: Genetic Algorithms in Problem SolvingOverviewLike other computational systems inspired by natural systems, genetic algorithms have been used in twoways: as techniques for solving technological problems, and as simplified scientific models that can answerquestions about nature. This chapter gives several case studies of GAs as problem solvers; chapter 3 givesseveral case studies of GAs used as scientific models. Despite this seemingly clean split between engineeringand scientific applications, it is often not clear on which side of the fence a particular project sits. Forexample, the work by Hillis described in chapter 1 above and the two other automatic-programming projectsdescribed below have produced results that, apart from their potential technological applications, may be ofinterest in evolutionary biology. Likewise, several of the "artificial life" projects described in chapter 3 havepotential problem-solving applications. In short, the "clean split" between GAs for engineering and GAs forscience is actually fuzzy, but this fuzziness—and its potential for useful feedback between problem-solvingand scientific-modeling applications—is part of what makes GAs and other adaptive-computation methodsparticularly interesting.2.1 EVOLVING COMPUTER PROGRAMSAutomatic programming—i.e., having computer programs automatically write computer programs—has along history in the field of artificial intelligence. Many different approaches have been tried, but as yet nogeneral method has been found for automatically producing the complex and robust programs needed for realapplications.Some early evolutionary computation techniques were aimed at automatic programming. The evolutionaryprogramming approach of Fogel, Owens, and Walsh (1966) evolved simple programs in the form offinite-state machines. Early applications of genetic algorithms to simple automatic-programming tasks wereperformed by Cramer (1985) and by Fujiki and Dickinson (1987), among others. The recent resurgence ofinterest in automatic programming with genetic algorithms has been, in part, spurred by John Koza's work onevolving Lisp programs via "genetic programming."The idea of evolving computer programs rather than writing them is very appealing to many. This isparticularly true in the case of programs for massively parallel computers, as the difficulty of programmingsuch computers is a major obstacle to their widespread use. Hillis's work on evolving efficient sortingnetworks is one example of automatic programming for parallel computers. My own work with Crutchfield,Das, and Hraber on evolving cellular automata to perform computations is an example of automaticprogramming for a very different type of parallel architecture.Evolving Lisp ProgramsJohn Koza (1992,1994) has used a form of the genetic algorithm to evolve Lisp programs to perform varioustasks. Koza claims that his method— "genetic programming" (GP)—has the potential to produce programs ofthe necessary complexity and robustness for general automatic programming. Programs in Lisp can easily beexpressed in the form of a "parse tree," the object the GA will work on.27As a simple example, consider a program to compute the orbital period P of a planet given its averagedistance A from the Sun. Kepler's Third Law states that P2 = cA3, where c is a constant. Assume that P isexpressed in units of Earth years and A is expressed in units of the Earth's average distance from the Sun, so c= 1. In FORTRAN such a program might be written asPROGRAM ORBITAL_PERIODC       # Mars #        A = 1.52        P = SQRT(A * A * A)        PRINT PEND ORBITAL_PERIODwhere * is the multiplication operator and SQRT is the square-root operator. (The value for A for Mars isfrom Urey 1952.) In Lisp, this program could be written as(defun orbital_period ()       ; Mars ;       (setf A 1.52)       (sqrt (* A (* A A))))In Lisp, operators precede their arguments: e.g., X * Y is written (* X Y). The operator "setf" assigns itssecond argument (a value) to its first argument (a variable). The value of the last expression in the program isprinted automatically.Assuming we know A, the important statement here is (SQRT (* A (* A A))). A simple task for automaticprogramming might be to automatically discover this expression, given only observed data for P and A.Expressions such as (SQRT (* A (* A A)) can be expressed as parse trees, as shown in figure 2.1. In Koza'sGP algorithm, a candidate solution is expressed as such a tree rather than as a bit string. Each tree consists offuntions and terminals. In the tree shown in figure 2.1, SQRT is a function that takes one argument, * is afunction that takes two arguments, and A is a terminal. Notice that the argument to a function can be the resultof another function—e.g., in the expression above one of the arguments to the top-level * is (* A A).Figure 2.1: Parse tree for the Lisp expression (SQRT (* A (* A * A A))).Koza's algorithm is as follows:1. Choose a set of possible functions and terminals for the program. The idea behind GP is, of course, toevolve programs that are difficult to write, and in general one does not know ahead of time preciselywhich functions and terminals will be needed in a successful program. Thus, the user of GP has toChapter 2: Genetic Algorithms in Problem Solving2. Generate an initial population of random trees (programs) using the set of possible functions andterminals. These random trees must be syntactically correct programs—the number of branchesextending from each function node must equal the number of arguments taken by that function. Threeprograms from a possible randomly generated initial population are displayed in figure 2.2. Noticethat the randomly generated programs can be of different sizes (i.e., can have different numbers ofnodes and levels in the trees). In principle a randomly generated tree can be any size, but in practiceKoza restricts the maximum size of the initially generated trees.Figure 2.2: Three programs from a possible randomly generated initial population for theorbital-period task. The expression represented by each tree is printed beneath the tree. Also printedis the fitness f (number of outputs within 20% of correct output) of each tree on the given set offitness cases. A is given in units of Earth's semimajor axis of orbit; P is given in units of Earth years.(Planetary data from Urey 1952.)3. Calculate the fitness of each program in the population by running it on a set of "fitness cases" (a setof inputs for which the correct output is known). For the orbital-period example, the fitness casesmight be a set of empirical measurements of P and A. The fitness of a program is a function of thenumber of fitness cases on which it performs correctly. Some fitness functions might give partialcredit to a program for getting close to the correct output. For example, in the orbital-period task, wecould define the fitness of a program to be the number of outputs that are within 20% of the correctvalue. Figure 2.2 displays the fitnesses of the three sample programs according to this fitness functionon the given set of fitness cases. The randomly generated programs in the initial population are notlikely to do very well; however, with a large enough population some of them will do better thanothers by chance. This initial fitness differential provides a basis for "natural selection."4. Chapter 2: Genetic Algorithms in Problem Solving29Apply selection, crossover, and mutation to the population to form a new population. In Koza'smethod, 10% of the trees in the population (chosen probabilistically in proportion to fitness) arecopied without modification into the new population. The remaining 90% of the new population isformed by crossovers between parents selected (again probabilistically in proportion to fitness) fromthe current population. Crossover consists of choosing a random point in each parent and exchangingthe subtrees beneath those points to produce two offspring. Figure 2.3 displays one possible crossoverevent. Notice that, in contrast to the simple GA, crossover here allows the size of a program toincrease or decrease. Mutation might performed by choosing a random point in a tree and replacingthe subtree beneath that point by a randomly generated subtree. Koza (1992) typically does not use amutation operator in his applications; instead he uses initial populations that are presumably largeenough to contain a sufficient diversity of building blocks so that crossover will be sufficient to puttogether a working program.Figure 2.3: An example of crossover in the genetic programming algorithm. The two parents are shown at thetop of the figure, the two offspring below. The crossover points are indicated by slashes in the parent trees.Steps 3 and 4 are repeated for some number of generations.It may seem difficult to believe that this procedure would ever result in a correct program—the famousexample of a monkey randomly hitting the keys on a typewriter and producing the works of Shakespearecomes to mind. But, surprising as it might seem, the GP technique has succeeded in evolving correctprograms to solve a large number of simple (and some not-so-simple) problems in optimal control, planning,sequence induction, symbolic regression, image compression, robotics, and many other domains. Oneexample (described in detail in Koza 1992) is the block-stacking problem illustrated in figure 2.4. The goalwas to find a program that takes any initial configuration of blocks—some on a table, some in a stack—andplaces them in the stack in the correct order. Here the correct order spells out the word "universal." ("Toy"problems of this sort have been used extensively to develop and test planning methods in artificialintelligence.) The functions and terminals Koza used for this problem were a set of sensors and actionsdefined by Nilsson (1989). The terminals consisted of three sensors (available to a hypothetical robot to becontrolled by the resulting program), each of which returns (i.e., provides the controlling Lisp program with) apiece of information:Chapter 2: Genetic Algorithms in Problem Solving30Figure 2.4: One initial state for the block-stacking problem (adapted from Koza 1992). The goal is to find aplan that will stack the blocks correctly (spelling "universal") from any initial state.CS ("current stack") returns the name of the top block of the stack. If the stack is empty, CS returns NIL(which means "false" in Lisp).TB ("top correct block") returns the name of the topmost block on the stack such that it and all blocks below itare in the correct order. If there is no such block, TB returns NIL.NN ("next needed") returns the name of the block needed immediately above TB in the goal "universal." If nomore blocks are needed, this sensor returns NIL.In addition to these terminals, there were five functions available to GP:MS(x) ("move to stack") moves block x to the top of the stack if x is on the table, and returns x. (In Lisp, everyfunction returns a value. The returned value is often ignored.)MT(x) ("move to table") moves the block at the top of the stack to the table if block x is anywhere in the stack,and returns x.DU (expression1, expression2) ("do until") evaluates expression1 until expression2 (a predicate) becomesTRUE.NOT (expression1) returns TRUE if expression1 is NIL; otherwise it returns NIL.EQ (expression1,expression2) returns TRUE if expression1 and expression2 are equal (i.e., return the samevalue).The programs in the population were generated from these two sets. The fitness of a given program was thenumber of sample fitness cases (initial configurations of blocks) for which the stack was correct after theprogram was run. Koza used 166 different fitness cases, carefully constructed to cover the various classes ofpossible initial configurations.The initial population contained 300 randomly generated programs. Some examples (written in Lisp stylerather than tree style) follow:(EQ (MT CS) NN)"Move the current top of stack to the table, and see if it is equal to the next needed." This clearly does notmake any progress in stacking the blocks, and the program's fitness was 0.(MS TB)"Move the top correct block on the stack to the stack." This program does nothing, but doing nothing allowedChapter 2: Genetic Algorithms in Problem Solving31it to get one fitness case correct: the case where all the blocks were already in the stack in the correct order.Thus, this program's fitness was 1.(EQ (MS NN) (EQ (MS NN) (MS NN)))"Move the next needed block to the stack three times." This program made some progress and got four fitnesscases right, giving it fitness 4. (Here EQ serves merely as a control structure. Lisp evaluates the firstexpression, then evaluates the second expression, and then compares their value. EQ thus performs the desiredtask of executing the two expressions in sequence—we do not actually care whether their values are equal.)By generation 5, the population contained some much more successful programs. The best one was (DU (MSNN) (NOT NN)) (i.e., "Move the next needed block to the stack until no more blocks are needed"). Here wehave the basics of a reasonable plan. This program works in all cases in which the blocks in the stack arealready in the correct order: the program moves the remaining blocks on the table into the stack in the correctorder. There were ten such cases in the total set of 166, so this program's fitness was 10. Notice that thisprogram uses a building block—(MS NN)—that was discovered in the first generation and found to be usefulthere.In generation 10 a completely correct program (fitness 166) was discovered:(EQ (DU (MT CS) (NOT CS)) (DU (MS NN) (NOT NN))).This is an extension of the best program of generation 5. The program empties the stack onto the table andthen moves the next needed block to the stack until no more blocks are needed. GP thus discovered a plan thatworks in all cases, although it is not very efficient. Koza (1992) discusses how to amend the fitness functionto produce a more efficient program to do this task.The block stacking example is typical of those found in Koza's books in that it is a relatively simple sampleproblem from a broad domain (planning). A correct program need not be very long. In addition, the necessaryfunctions and terminals are given to the program at a fairly high level. For example, in the block stackingproblem GP was given the high-level actions MS, MT, and so on; it did not have to discover them on its own.Could GP succeed at the block stacking task if it had to start out with lower-level primitives? O'Reilly andOppacher (1992), using GP to evolve a sorting program, performed an experiment in which relativelylow-level primitives (e.g., "if-less-than" and "swap") were defined separately rather than combined a prioriinto "if-less-than-then-swap" Under these conditions, GP achieved only limited success. This indicates apossible serious weakness of GP, since in most realistic applications the user will not know in advance whatthe appropriate high-level primitives should be; he or she is more likely to be able to define a larger set oflower-level primitives.Genetic programming, as originally defined, includes no mechanism for automatically chunking parts of aprogram so they will not be split up under crossover, and no mechanism for automatically generatinghierarchical structures (e.g., a main program with subroutines) that would facilitate the creation of newhigh-level primitives from built-in low-level primitives. These concerns are being addressed in more recentresearch. Koza (1992, 1994) has developed methods for encapsulation and automatic definition of functions.Angeline and Pollack (1992) and O'Reilly and Oppacher (1992) have proposed other methods for theencapsulation of useful subtrees.Koza's GP technique is particularly interesting from the standpoint of evolutionary computation because itallows the size (and therefore the complexity) of candidate solutions to increase over evolution, rather thankeeping it fixed in the standard GA. However, the lack of sophisticated encapsulation mechanisms has so farChapter 2: Genetic Algorithms in Problem Solving32limited the degree to which programs can usefully grow. In addition, there are other open questions about thecapabilities of GP. Does it work well because the space of Lisp expressions is in some sense "dense" withcorrect programs for the relatively simple tasks Koza and other GP researchers have tried? This was given asone reason for the success of the artificial intelligence program AM (Lenat and Brown 1984), which evolvedLisp expressions to discover "interesting" conjectures in mathematics, such as the Goldbach conjecture (everyeven number is the sum of two primes). Koza refuted this hypothesis about GP by demonstrating how difficultit is to randomly generate a successful program to perform some of the tasks for which GP evolves successfulprograms. However, one could speculate that the space of Lisp expressions (with a given set of functions andterminals) is dense with useful intermediate-size building blocks for the tasks on which GP has beensuccessful. GP's ability to find solutions quickly (e.g., within 10 generations using a population of 300) lendscredence to this speculation.GP also has not been compared systematically with other techniques that could search in the space of parsetrees. For example, it would be interesting to know if a hill climbing technique could do as well as GP on theexamples Koza gives. One test of this was reported by O'Reilly and Oppacher (1994a,b), who defined amutation operator for parse trees and used it to compare GP with a simple hill-climbing technique similar torandom-mutation hill climbing (see computer exercise 4 of chapter 1) and with simulated annealing (a moresophisticated hill-climbing technique). Comparisons were made on five problems, including the blockstacking problem described above. On each of the five, simulated annealing either equaled or significantlyoutperformed GP in terms of the number of runs on which a correct solution was found and the averagenumber of fitness-function evaluations needed to find a correct program. On two out of the five, the simplehill climber either equaled or exceeded the performance of GP.Though five problems is not many for such a comparison in view of the number of problems on which GP hasbeen tried, these results bring into question the claim (Koza 1992) that the crossover operator is a majorcontributor to GP's success. O'Reilly and Oppacher (1994a) speculate from their results that the parse-treerepresentation "may be a more fundamental asset to program induction than any particular search technique,"and that "perhaps the concept of building blocks is irrelevant to GP." These speculations are well worthfurther investigation, and it is imperative to characterize the types of problems for which crossover is a usefuloperator and for which a GA will be likely to outperform gradient-ascent strategies such as hill climbing andsimulated annealing. Some work toward those goals will be described in chapter 4.Some other questions about GP:Will the technique scale up to more complex problems for which larger programs are needed?Will the technique work if the function and terminal sets are large?How well do the evolved programs generalize to cases not in the set of fitness cases? In most of Koza'sexamples, the cases used to compute fitness are samples from a much larger set of possible fitness cases. GPvery often finds a program that is correct on all the given fitness cases, but not enough has been reported onhow well these programs do on the "out-of-sample" cases. We need to know the extent to which GPproduces programs that generalize well after seeing only a small fraction of the possible fitness cases.To what extent can programs be optimized for correctness, size, and efficiency at the same time?Genetic programming's success on a wide range of problems should encourage future research addressingthese questions. (For examples of more recent work on GP, see Kinnear 1994.)Chapter 2: Genetic Algorithms in Problem Solving33Evolving Cellular AutomataA quite different example of automatic programming by genetic algorithms is found in work done by JamesCrutchfield, Rajarshi Das, Peter Hraber, and myself on evolving cellular automata to perform computations(Mitchell, Hraber, and Crutchfield 1993; Mitchell, Crutchfield, and Hraber 1994a; Crutchfield and Mitchell1994; Das, Mitchell, and Crutchfield 1994). This project has elements of both problem solving and scientificmodeling. One motivation is to understand how natural evolution creates systems in which "emergentcomputation" takes place—that is, in which the actions of simple components with limited information andcommunication give rise to coordinated global information processing. Insect colonies, economic systems, theimmune system, and the brain have all been cited as examples of systems in which such emergentcomputation occurs (Forrest 1990; Langton 1992). However, it is not well understood how these naturalsystems perform computations. Another motivation is to find ways to engineer sophisticated emergentcomputation in decentralized multi-processor systems, using ideas from how natural decentralized systemscompute. Such systems have many of the desirable properties for computer systems mentioned in chapter 1:they are sophisticated, robust, fast, and adaptable information processors. Using ideas from such systems todesign new types of parallel computers might yield great progress in computer science.One of the simplest systems in which emergent computation can be studied is a one-dimensional binary-statecellular automaton (CA)—a one-dimensional lattice of N two-state machines ("cells"), each of whichchanges its state as a function only of the current states in a local neighborhood. (The well-known "game ofLife" (Berlekamp, Conway, and Guy 1982) is an example of a two-dimensional CA.) A one-dimensional CAis illustrated in figure 2.5. The lattice starts out with an initial configuration of cell states (zeros and ones) andthis configuration changes in discrete time steps in which all cells are updated simultaneously according to theCA "rule" Æ. (Here I use the term "state" to refer to refer to a local state si—the value of the single cell at sitei. The term "configuration" will refer to the pattern of local states over the entire lattice.)Figure 2.5: Illustration of a one-dimensional, binary-state, nearest-neighbor (r = 1) cellular automaton withN = 11. Both the lattice and the rule table for updating the lattice are illustrated. The lattice configuration isshown over one time step. The cellular automaton has periodic boundary conditions: the lattice is viewed as acircle, with the leftmost cell the right neighbor of the rightmost cell, and vice versa.A CA rule Æ can be expressed as a lookup table ("rule table") that lists,for each local neighborhood, the update state for the neighborhood's central cell. For a binary-state CA, theupdate states are referred to as the "output bits" of the rule table. In a one-dimensional CA, a neighborhoodconsists of a cell and its r ("radius") neighbors on either side. The CA illustrated in figure 2.5 has r = 1. ItChapter 2: Genetic Algorithms in Problem Solving34illustrates the "majority" rule: for each neighborhood of three adjacent cells, the new state is decided by amajority vote among the three cells. The CA illustrated in figure 2.5, like all those I will discuss here, hasperiodic boundary conditions: si = si + N. In figure 2.5 the lattice configuration is shown iterated over one timestep.Cellular automata have been studied extensively as mathematical objects, as models of natural systems, and asarchitectures for fast, reliable parallel computation. (For overviews of CA theory and applications, see Toffoliand Margolus 1987 and Wolfram 1986.) However, the difficulty of understanding the emergent behavior ofCAs or of designing CAs to have desired behavior has up to now severely limited their use in science andengineering and for general computation. Our goal is to use GAs as a method for engineering CAs to performcomputations.Typically, a CA performing a computation means that the input to the computation is encoded as an initialconfiguration, the output is read off the configuration after some time step, and the intermediate steps thattransform the input to the output are taken as the steps in the computation. The "program" emerges from theCA rule being obeyed by each cell. (Note that this use of CAs as computers differs from the impracticalthough theoretically interesting method of constructing a universal Turing machine in a CA; see Mitchell,Crutchfield, and Hraber 1994b for a comparison of these two approaches.)The behavior of one-dimensional CAs is often illustrated by a "space-time diagram"—a plot of latticeconfigurations over a range of time steps, with ones given as black cells and zeros given as white cells andwith time increasing down the page. Figure 2.6 shows such a diagram for a binary-state r = 3 CA in which therule table's output bits were filled in at random. It is shown iterating on a randomly generated initialconfiguration. Random-looking patterns, such as the one shown, are typical for the vast majority of CAs. Toproduce CAs that can perform sophisticated parallel computations, the genetic algorithm must evolve CAs inwhich the actions of the cells are not random-looking but are coordinated with one another so as to producethe desired result. This coordination must, of course, happen in the absence of any central processor ormemory directing the coordination.Figure 2.6: Space-time diagram for a randomly generated r = 3 cellular automaton, iterating on a randomlygenerated initial configuration. N = 149 sites are shown, with time increasing down the page. Here cells withstate 0 are white and cells with state 1 are black. (This and the other space-time diagrams given here weregenerated using the program "la1d" written by James P. Crutchfield.)Chapter 2: Genetic Algorithms in Problem Solving35Some early work on evolving CAs with genetic algorithms was done by Norman Packard and his colleagues(Packard 1988; Richards, Meyer, and Packard 1990). John Koza (1992) also applied the GP paradigm toevolve CAs for simple random-number generation.Our work builds on that of Packard (1988). As a preliminary project, we used a form of the GA to evolveone-dimensional, binary-state r = 3 CAs to perform a density-classification task. The goal is to find a CAthat decides whether or not the initial configuration contains a majority ofones (i.e., has high density). If it does, the whole lattice should eventually go to an unchanging configurationof all ones; all zeros otherwise. More formally, we call this task the   task. Here Á denotes the density ofones in a binary-state CA configuration and Ác denotes a "critical" or threshold density for classification. LetÁ0 denote the density of ones in the initial configuration (IC). If Á0 > Ác, then within M time steps the CAshould go to the fixed-point configuration of all ones (i.e., all cells in state 1 for all subsequent t); otherwise,within M time steps it should go to the fixed-point configuration of all zeros. M is a parameter of the task thatdepends on the lattice size N.It may occur to the reader that the majority rule mentioned above might be a good candidate for solving thistask. Figure 2.7 gives space-time diagrams for the r = 3 majority rule (the output bit is decided by a majorityvote of the bits in each seven-bit neighborhood) on two ICs, one with   and one with   As can be seen,local neighborhoods with majority ones map to regions of all ones and similarly for zeros, but when anall-ones region and an all-zeros region border each other, there is no way to decide between them, and bothpersist. Thus, the majority rule does not perform the   task.Figure 2.7: Space-time diagrams for the r = 3 majority rule. In the left diagram,   in the right diagram,Designing an algorithm to perform the   task is trivial for a system with a central controller or centralstorage of some kind, such as a standard computer with a counter register or a neural network in which allinput units are connected to a central hidden unit. However, the task is nontrivial for a small-radius (r << N)CA, since a small-radius CA relies only on local interactions mediated by the cell neighborhoods. In fact, itcan be proved that no finite-radius CA with periodic boundary conditions can perform this task perfectlyacross all lattice sizes, but even to perform this task well for a fixed lattice size requires more powerfulcomputation than can be performed by a single cell or any linear combination of cells (such as the majorityrule). Since the ones can be distributed throughout the CA lattice, the CA must transfer information over largedistances (H N). To do this requires the global coordination of cells that are separated by large distances andthat cannot communicate directly. How can this be done? Our interest was to see if the GA could devise oneor more methods.The chromosomes evolved by the GA were bit strings representing CA rule tables. Each chromosomeconsisted of the output bits of a rule table, listed in lexicographic order of neighborhood (as in figure 2.5). Thechromosomes representing rules were thus of length 22r + 1 = 128 (for binary r = 3 rules). The size of the ruleChapter 2: Genetic Algorithms in Problem Solving36space the GA searched was thus 2128—far too large for any kind of exhaustive search.In our main set of experiments, we set N = 149 (chosen to be reasonably large but not computationallyintractable). The GA began with a population of 100 randomly generated chromosomes (generated with someinitial biases—see Mitchell, Crutchfield, and Hraber 1994a, for details). The fitness of a rule in the populationwas calculated by (i) randomly choosing 100 ICs (initial configurations) that are uniformly distributed over ÁÎ [0.0,1.0], with exactly half with ÁÁc and half with ÁÁc, (ii) running the rule on each IC either until it arrivesat a fixed point or for a maximum of approximately 2N time steps, and (iii) determining whether the finalpattern is correct—i.e., N zeros for Á0Ác and N ones for Á0Ác. The initial density, Á0, was never exactly   sinceN was chosen to be odd. The rule's fitness, f100, was the fraction of the 100 ICs on which the rule produced thecorrect final pattern. No partial credit was given for partially correct final configurations.A few comments about the fitness function are in order. First, as was the case in Hillis's sorting-networksproject, the number of possible input cases (2149 for N = 149) was far too large to test exhaustively. Instead,the GA sampled a different set of 100 ICs at each generation. In addition, the ICs were not sampled from anunbiased distribution (i.e., equal probability of a one or a zero at each site in the IC), but rather from a flatdistribution across Á Î [0,1] (i.e., ICs of each density from Á = 0 to Á = 1 were approximately equallyrepresented). This flat distribution was used because the unbiased distribution is binomially distributed andthus very strongly peaked at  . The ICs selected from such a distribution will likely all have  , thehardest cases to classify. Using an unbiased sample made it too difficult for the GA to ever find anyhigh-fitness CAs. (As will be discussed below, this biased distribution turns out to impede the GA in latergenerations: as increasingly fit rules are evolved, the IC sample becomes less and less challenging for theGA.)Our version of the GA worked as follows. In each generation, (i) a new set of 100 ICs was generated, (ii) f100was calculated for each rule in the population, (iii) the population was ranked in order of fitness, (iv) the 20highest-fitness ("elite") rules were copied to the next generation without modification, and (v) the remaining80 rules for the next generation were formed by single-point crossovers between randomly chosen pairs ofelite rules. The parent rules were chosen from the elite with replacement—that is, an elite rule was permittedto be chosen any number of times. The offspring from each crossover were each mutated twice. This processwas repeated for 100 generations for a single run of the GA. (More details of the implementation are given inMitchell, Crutchfield, and Hraber 1994a.)Note that this version of the GA differs from the simple GA in several ways. First, rather than selectingparents with probability proportional to fitness, the rules are ranked and selection is done at random from thetop 20% of the population. Moreover, all of the top 20% are copied without modification to the nextgeneration, and only the bottom 80% are replaced. This is similar to the selection method—called "(¼ +»)"—used in some evolution strategies; see Back, Hoffmeister, and Schwefel 1991.This version of the GA was the one used by Packard (1988), so we used it in our experiments attempting toreplicate his work (Mitchell, Hraber, and Crutchfield 1993) and in our subsequent experiments. Selectingparents by rank rather than by absolute fitness prevents initially stronger individuals from quickly dominatingthe population and driving the genetic diversity down too early. Also, since testing a rule on 100 ICs providesonly an approximate gauge of the true fitness, saving the top 20% of the rules was a good way of making a"first cut" and allowing rules that survive to be tested over more ICs. Since a new set of ICs was producedevery generation, rules that were copied without modification were always retested on this new set. If a ruleperformed well and thus survived over a large number of generations, then it was likely to be a genuinelybetter rule than those that were not selected, since it was tested with a large set of ICs. An alternative methodwould be to test every rule in each generation on a much larger set of ICs, but this would waste computationtime. Too much effort, for example, would go into testing very weak rules, which can safely be weeded outearly using our method. As in most applications, evaluating the fitness function (here, iterating each CA) takesChapter 2: Genetic Algorithms in Problem Solving37up most of the computation time.Three hundred different runs were performed, each starting with a different random-number seed. On mostruns the GA evolved a nonobvious but rather unsophisticated class of strategies. One example, a rule herecalled Æa, is illustrated in figure 2.8a This rule had f100H0.9 in the generation in which it was discovered (i.e.,Æa correctly classified 90% of the ICs in that generation). Its "strategy" is the following: Go to the fixed pointof all zeros unless there is a sufficiently large block of adjacent (or almost adjacent) ones in the IC. If so,expand that block. (For this rule, "sufficiently large" is seven or more cells.) This strategy does a fairly goodjob of classifying low and high density under f100: it relies on the appearance or absence of blocks of ones tobe good predictors of Á0, since high-density ICs are statistically more likely to have blocks of adjacent onesthan lowdensity ICs.Figure 2.8: Space-time diagrams from four different rules discovered by the GA (adapted from Das, Mitchell,and Crutchfield 1994 by permission of the authors). The left diagrams have  ; the right diagrams have. All are correctly classified. Fitness increases from (a) to (d). The "gray" area in (d) is actually acheckerboard pattern of alternating zeros and ones.Similar strategies were evolved in most runs. On approximately half the runs, "expand ones" strategies wereevolved, and on approximately half the runs, the opposite "expand zeros" strategies were evolved. Theseblock-expanding strategies were initially surprising to us and even seemed clever, but they do not count assophisticated examples of computation in CAs: all the computation is done locally in identifying and thenexpanding a "sufficiently large" block. There is no notion of global coordination or interesting informationflow between distant cells—two things we claimed were necessary to perform well on the task.In Mitchell, Crutchfield, and Hraber 1994a we analyzed the detailed mechanisms by which the GA evolvedsuch block-expanding strategies. This analysis uncovered some quite interesting aspects of the GA, includinga number of impediments that, on most runs, kept the GA from discovering better-performing rules. TheseChapter 2: Genetic Algorithms in Problem Solving38included the GA's breaking the   task's symmetries for short-term gains in fitness, as well as an"overfitting" to the fixed lattice size and the unchallenging nature of the samples of ICs. These impedimentsare discussed in detail in Mitchell, Crutchfield, and Hraber 1994a, but the last point merits some elaborationhere.