The biased, flat distribution of ICs over Á Î [0,1] helped the GA get a leg up in the early generations. Wefound that calculating fitness on an unbiased distribution of ICs made the problem too difficult for the GAearly on—it was unable to find improvements to the rules in the initial population. However, the biaseddistribution became too easy for the improved CAs later in a run, and these ICs did not push the GA hardenough to find better solutions. Recall that the same problem plagued Hillis's GA until he introducedhost-parasite coevolution. We are currently exploring a similar coevolution scheme to improve the GA'sperformance on this problem.The weakness of Æa and similar rules is clearly seen when they are tested using an unbiased distribution ofICs. We defined a rule Æ's "unbiased performance"   as the fraction of correct classifications producedby Æ within approximately 2N time steps on 10,000 ICs on a lattice of length N, chosen from an unbiaseddistribution over Á. As mentioned above, since the distribution is unbiased, the ICs are very likely to have Á H0.5. These are the very hardest cases to classify, so   gives a lower bound on Æ's overall performance.Table 2.1 gives   values for several different rules each for three values of N. The majority rule,unsurprisingly, has   for all three values of N. The performance of Æa (the block-expanding rule offigure 2.8a) decreases significantly as N is increased. This was true for all the block-expanding rules: theperformance of these rules decreased dramaticallyTable 2.1: Measured values of   at various values of N for six different r = 3 rules: the majority rule, fourrules discovered by the GA in different runs and the GKL rule . The subscripts for the rulesdiscovered by the GA indicate the pair of space-time diagrams illustrating their behavior in figure 2.8. Thestandard deviation ofp149, when calculated 100 times for the same rule, is approximately 0.004. The standarddeviations for  ; for larger N are higher. (The actual lookup tables for these and other rules are given inCrutchfield and Mitchell 1994.)CA Symbol N = 149 N = 599 N = 999Majority Æmaj 0.000 0.000 0.000Expand 1-blocks Æa 0.652 0.515 0.503Particle-based Æb 0.697 0.580 0.522Particle-based Æc 0.742 0.718 0.701Particle-based Æd 0.769 0.725 0.714GKL ÆGKL 0.816 0.766 0.757for larger N, since the size of block to expand was tuned by the GA for N=149.Despite these various impediments and the unsophisticated rules evolved on most runs, on several differentruns in our initial experiment the GA discovered rules with significantly higher performance and rathersophisticated strategies. The typical space-time behavior of three such rules (each from a different run) areillustrated in figure 2.8b–2.8d Some   values for these three "particle-based" rules are given in table 2.1.As can be seen,   is significantly higher for these rules than for the typical block-expanding rule Æa. Inaddition, the performances of the most highly fit rules remain relatively constant as N is increased, meaningthat these rules can generalize better than can Æa.Chapter 2: Genetic Algorithms in Problem Solving39Why does Æd, for example, perform relatively well on the   task? In figure 2.8d it can be seen that,although the patterns eventually converge to fixed points, there is a transient phase during which spatial andtemporal transfer of information about the density in local regions takes place. This local information interactswith other local information to produce the desired final state. Roughly, Æd successively classifies "local"densities with a locality range that increases with time. In regions where there is some ambiguity, a "signal" ispropagated. This is seen either as a checkerboard pattern propagated in both spatial directions or as a verticalblack-to-white boundary. These signals indicate that the classification is to be made at a larger scale. (Suchsignals for resolving ambiguities are precisely what was lacking in the majority rule on this task.) Note thatregions centered about each signal locally have   The consequence is that the signal patterns canpropagate, since the density of patterns with   is neither increased nor decreased under the rule. Thecreation and interactions of these signals can be interpreted as the locus of the computation being performedby the CA—they form its emergent program.The above explanation of how Æd performs the   task is an informal one obtained by careful scrutiny ofmany space-time patterns. Can we understand more rigorously how the rules evolved by the GA perform thedesired computation? Understanding the results of GA evolution is a general problem—typically the GA isasked to find individuals that achieve high fitness but is not told how that high fitness is to be attained. Onecould say that this is analogous to the difficulty biologists have in understanding the products of naturalevolution (e.g., us). We computational evolutionists have similar problems, since we do not specify whatsolution evolution is supposed to create; we ask only that it find some solution. In many cases, particularly inautomatic-programming applications, it is difficult to understand exactly how an evolved high-fitnessindividual works. In genetic programming, for example, the evolved programs are often very long andcomplicated, with many irrelevant components attached to the core program performing the desiredcomputation. It is usually a lot of work—and sometimes almost impossible—to figure out by hand what thatcore program is. The problem is even more difficult in the case of cellular automata, since the emergent"program" performed by a given CA is almost always impossible to extract from the bits of the rule table.A more promising approach is to examine the space-time patterns created by the CA and to "reconstruct"from those patterns what the algorithm is. Crutchfield and Hanson have developed a general method forreconstructing and understanding the "intrinsic" computation embedded in space-time patterns in terms of"regular domains," "particles" and "particle interactions" (Hanson and Crutchfield, 1992; Crutchfield andHanson 1993). This method is part of their "computational mechanics" framework for understandingcomputation in physical systems. A detailed discussion of computational mechanics and particle-basedcomputation is beyond the scope of this chapter. Very briefly, for those familiar with formal language theory,regular domains are regions of spacetime consisting of words in the same regular language—that is, they areregions that are computationally simple. Particles are localized boundaries between regular domains. Incomputational mechanics, particles are identified as information carriers, and collisions between particles areidentified as the loci of important information processing. Particles and particle interactions form a high-levellanguage for describing computation in spatially extended systems such as CAs. Figure 2.9 hints at this higherlevel of description: to produce it we filtered the regular domains from the space-time behavior of aGA-evolved CA to leave only the particles and their interactions, in terms of which the emergent algorithm ofthe CA can be understood.The application of computational mechanics to the understanding of rules evolved by the GA is discussedfurther in Crutchfield and Mitchell 1994, in Das, Mitchell, and Crutchfield 1994, and in Das, Crutchfield,Mitchell, and Hanson 1995. In the last two papers, we used particles andChapter 2: Genetic Algorithms in Problem Solving40Figure 2.9: A space-time diagram of a GA-evolved rule for the   task, and the same diagram with theregular domains filtered out, leaving only the particles and particle interactions (two of which are magnified).(Reprinted from Crutchfield and Mitchell 1994 by permission of the authors.)particle interactions to describe the temporal stages by which highly fit rules were evolved by the GA.Interestingly, it turns out that the behavior of the best rules discovered by the GA (such as Æd) is very similarto the behavior of the well-known Gacs-Kurdyumov-Levin (GKL) rule (Gacs, Kurdyumov, and Levin,1978; Gonzaga de Sá and Maes 1992). Figure 2.10 is a space-time diagram illustrating its typical behavior.The GKL rule (ÆGKL) was designed by hand to study reliable computation and phase transitions inone-dimensional spatially extended systems, but before we started our project it was also the rule with thebest-known performance (for CAs with periodic boundary conditions) on the   task. Its unbiasedperformance is given in the last row of table 2.1. The difference in performance between Æd and ÆGKL is dueto asymmetries in Æd that are not present in ÆGKL. Further GA evolution of Æd (using an increased number ofICs) has produced an improved version that approximately equals the performance of the ÆGKL. Rajarshi Das(personal communication) has gone further and, usingFigure 2.10: Space-time diagram for the GKL rule with the aforementioned particle analysis, has designed by hand a rule that slightly outperforms ÆGKL.The discovery of rules such as is significant, since it is the first example of a GA's producingsophisticated emergent computation in decentralized, distributed systems such as CAs. It is encouraging forChapter 2: Genetic Algorithms in Problem Solving41the prospect of using GAs to automatically evolve computation in more complex systems. Moreover, evolvingCAs with GAs also gives us a tractable framework in which to study the mechanisms by which anevolutionary process might create complex coordinated behavior in natural decentralized distributed systems.For example, by studying the GA's behavior, we have already learned how evolution's breaking of symmetriescan lead to suboptimal computational strategies; eventually we may be able to use such computer models totest ways in which such symmetry breaking might occur in natural evolution.2.2 DATA ANALYSIS AND PREDICTIONA major impediment to scientific progress in many fields is the inability to make sense of the huge amounts ofdata that have been collected via experiment or computer simulation. In the fields of statistics and machinelearning there have been major efforts to develop automatic methods for finding significant and interestingpatterns in complex data, and for forecasting the future from such data; in general, however, the success ofsuch efforts has been limited, and the automatic analysis of complex data remains an open problem. Dataanalysis and prediction can often be formulated as search problems—for example, a search for a modelexplaining the data, a search for prediction rules, or a search for a particular structure or scenario wellpredicted by the data. In this section I describe two projects in which a genetic algorithm is used to solve suchsearch problems —one of predicting dynamical systems, and the other of predicting the structure of proteins.Predicting Dynamical SystemsNorman Packard (1990) has developed a form of the GA to address this problem and has applied his methodto several data analysis and prediction problems. The general problem can be stated as follows: A series ofobservations from some process (e.g., a physical system or a formal dynamical system) take the form of a setof pairs,where  are independent variables and yi is a dependent variable (1didN). For example,in a weather prediction task, the independent variables might be some set of features of today's weather (e.g.,average humidity, average barometric pressure, low and high temperature, whether or not it rained), and thedependent variable might be a feature of tomorrow's weather (e.g., rain). In a stock market prediction task, theindependent variables might be   representing the values of the value ofa particular stock (the "state variable") at successive time steps, and the dependent variable might be y=x(tn +k), representing the value of the stock at some time in the future. (In these examples there is only onedependent variable y for each vector of independent variables  ; a more general form of the problem wouldallow a vector of dependent variables for each vector of independent variables.)Packard used a GA to search through the space of sets of conditions on the independent variables for thosesets of conditions that give good predictions for the dependent variable. For example, in the stock marketprediction task, an individual in the GA population might be a set of conditions such asChapter 2: Genetic Algorithms in Problem Solving42where "^" is the logical operator "AND" This individual represents all the sets of three days in which thegiven conditions were met (possibly the empty set if the conditions are never met). Such a condition set Cthus specifies a particular subset of the data points (here, the set of all 3-day periods). Packard's goal was touse a GA to search for condition sets that are good predictors of something—in other words, to search forcondition sets that specify subsets of data points whose dependent-variable values are close to being uniform.In the stock market example, if the GA foundFigure 2.11: Plot of a time series from Mackey-Glass equation with Ä = 150. Time is plotted on thehorizontal axis; x(t)s> is plotted on the vertical axis. (Reprinted from Martin Casdagli and Stephen Eubank,eds., Nonlinear Modeling and Forecasting ; © 1992 Addison-Wesley Publishing Company, Inc. Reprinted bypermission of the publisher.)a condition set such that all the days satisfying that set were followed by days on which the price of Xeroxstock rose to approximately $30, then we might be confident to predict that, if those conditions were satisfiedtoday, Xerox stock will go up.The fitness of each individual C is calculated by running all the data points ( y) in the training set through Cand, for each  that satisfies C, collecting the corresponding y. After this has been done, a measurement ismade of the uniformity of the resulting values of y. If the y values are all close to a particular value Å, then Cis a candidate for a good predictor for y—that is, one can hope that a new  that satisfies C will alsocorrespond to a y value close to Å. On the other hand, if the y values are very different from one another, then satisfying C does not seem to predict anything about the corresponding y value.As an illustration of this approach, I will describe the work done by Thomas Meyer and Norman Packard(1992) on finding "regions of predictability" in time series generated by the Mackey-Glass equation, a chaoticdynamical system created as a model for blood flow (Mackey and Glass 1977):Here x(t) is the state variable, t is time in seconds, and a, b, c, and Ä are constants. A time series from thissystem (with Ä set to 150) is plotted in figure 2.11.To form the data set, Meyer and Packard did the following: For each data point i, the independent variables are 50 consecutive values of x(t) (one per second):The dependent variable for data point i, yi, is the state variable t' time steps in the future: yi = xI50 + t'. Eachdata point   is formed by iterating the Mackey-Glass equation with a different initial condition,where an initial condition consists of values for Meyer and Packard used the following as a fitness function:Chapter 2: Genetic Algorithms in Problem Solving43where Ã is the standard deviation of the set of y's for data points satisfying C, Ã0 is the standard deviation ofthe distribution of y's over the entire data set, NC is the number of data points satisfying condition C, and ± is aconstant. The first term of the fitness function measures the amount of information in the distribution of y's forpoints satisfying C, and the second term is a penalty term for poor statistics—if the number of pointssatisfying C is small, then the first term is less reliable, so C should have lower fitness. The constant ± can beadjusted for each particular application.Meyer and Packard used the following version of the GA:1. Initialize the population with a random set of C's.2. Calculate the fitness of each C.3. Rank the population by fitness.4. Discard some fraction of the lower-fitness individuals and replace them by new C's obtained byapplying crossover and mutation to the remaining C's.5. Go to step 2.(Their selection method was, like that used in the cellular-automata project described above, similar to the"(¼ + »)" method of evolution strategies.) Meyer and Packard used a form of crossover known in the GAliterature as "uniform crossover" (Syswerda 1989). This operator takes two Cs and exchanges approximatelyhalf the "genes" (conditions). That is, at each gene position in parent A and parent B, a random decision ismade whether that gene should go into offspring A or offspring B. An example follows:}Here offspring A has two genes from parent A and one gene from parent B. Offspring B has one gene fromparent A and three genes from parent B.In addition to crossover, four different mutation operators were used:Add a new condition:Chapter 2: Genetic Algorithms in Problem Solving44Delete a condition:Broaden or shrink a range:Shift a range up or down:The results of running the GA using these data from the Ä = 150 time series with t' = 150 are illustrated inFigure 2.12 and Figure 2.13. Figure 2.12 gives the four highest-fitness condition sets found by the GA, andfigure 2.13 shows the four results of those condition sets. Each of the four plots in figure 2.13 shows thetrajectories corresponding to data points   that satisfied the condition set. The leftmost white region is theinitial 50 time steps during which the data were taken. The vertical lines in that region represent the variousconditions on given in the condition set. For example, in plot a the leftmost vertical line represents a condition on x20 (thisset of trajectories is plotted starting at time step 20), and the rightmost vertical line in that region represents acondition on x 49. The shaded region represents the period of time between time steps 50 and 200, and therightmost vertical line marks time step 200 (the point at which the yi observation was made). Notice that ineach of these plots the values of yi fall into a very narrow range, which means that the GA was successful infinding subsets of the data for which it is possible to make highly accurate predictions. (Other results alongthe same lines are reported in Meyer 1992.)These results are very striking, but some questions immediately arise. First and most important, do thediscovered conditions yield correct predictions for data points outside the training set (i.e., the set of datapoints used to calculate fitness), or do they merely describe chance statistical fluctuations in the data that werelearned by the GA? Meyer and Packard performed a number of "out of sample" tests with data points outsidethe training set that satisfied the evolved condition sets and found that the results were robust—the yi valuesfor these data points also tended to be in the narrow range (Thomas Meyer, personal communication).Figure 2.12: The four highest-fitness condition sets found by the GA for the Mackey-Glass system withÄ=150. (Adapted from Meyer and Packard 1992.)Chapter 2: Genetic Algorithms in Problem Solving45Exactly how is the GA solving the problem? What are the schemas that are being processed? What is the roleof crossover in finding a good solution? Uniform crossover of the type used here has very different propertiesthan single-point crossover, and its use makes it harder to figure out what schemas are being recombined.Meyer (personal communication) found that turning crossover off and relying solely on the four mutationoperators did not make a big difference in the GA's performance; as in the case of genetic programming, thisraises the question of whether the GA is the best method for this task. An interesting extension of this workwould be to perform control experiments comparing the performance of the GA with that of other searchmethods such as hill climbing.To what extent are the results restricted by the fact that only certain conditions are allowed (i.e., conditionsthat are conjunctions of ranges on independent variables)? Packard (1990) proposed a more general form forconditions that also allows disjunctions (('s); an example might bewhere we are given two nonoverlapping choices for the conditions on x6. A further generalization proposed byPackard would be to allow disjunctions between sets of conditions.To what extent will this method succeed on other types of prediction tasks? Packard (1990) proposes applyingthis method to tasks such as weather prediction, financial market prediction, speech recognition, and visualpattern recognition. Interestingly, in 1991 Packard left the Physics Department at the University of Illinois tohelp form a company to predict financial markets (Prediction Company, in Santa Fe, New Mexico). As IFigure 2.13: Results of the four highest-fitness condition sets found by the GA. (See figure 2.12.) Each plotshows trajectories of data points that satisfied that condition set. The leftmost white region is the initial 50time steps during which data were taken. The vertical lines in that region represent the various conditions on given in the condition set. The vertical line on the right-hand side represents the time at which theprediction is to be made. Note how the trajectories narrow at that region, indicating that the GA has foundconditions for good predictability. (Reprinted from Martin Casdagli and Stephen Eubank (eds.), NonlinearModeling and Forecasting; © 1992 Addison-Wesley Publishing Company, Inc. Reprinted by permission ofthe publisher.)write this (mid 1995), the company has not yet gone public with their results, but stay tuned.Chapter 2: Genetic Algorithms in Problem Solving46Predicting Protein StructureOne of the most promising and rapidly growing areas of GA application is data analysis and prediction inmolecular biology. GAs have been used for, among other things, interpreting nuclear magnetic resonance datato determine the structure of DNA (Lucasius and Kateman 1989), finding the correct ordering for anunordered group of DNA fragments (Parsons, Forrest, and Burks, in press), and predicting protein structure.Here I will describe one particular project in which a GA was used to predict the structure of a protein.Proteins are the fundamental functional building blocks of all biological cells. The main purpose of DNA in acell is to encode instructions for building up proteins out of amino acids; the proteins in turn carry out most ofthe structural and metabolic functions of the cell. A protein is made up of a sequence of amino acidsconnected by peptide bonds. The length of the sequence varies from protein to protein but is typically on theorder of 100 amino acids. Owing to electrostatic and other physical forces, the sequence "folds up" to aparticular three-dimensional structure. It is this three-dimensional structure that primarily determines theprotein's function. The three-dimensional structure of a Crambin protein (a plant-seed protein consisting of46 amino acids) is illustrated in figure 2.14. The three-dimensional structure of a protein is determined by theparticular sequence of its amino acids, but it is not currently known precisely how a given sequence leads to agiven structure. In fact, being able to predict a protein's structure from its amino acid sequence is one of themost important unsolved problems of molecular biology and biophysics. Not only would a successfulprediction algorithm be a tremendous advance in the understanding of the biochemical mechanisms ofproteins, but, since such an algorithm could conceivably be used to design proteins to carry out specificfunctions, it would have profound, far-reaching effects on biotechnology and the treatment of disease.Recently there has been considerable effort toward developing methods such as GAs and neural networks forautomatically predicting protein structures (see, for example, Hunter, Searls, and Shavlik 1993). The relativelysimple GA prediction project of Steffen Schulze-Kremer (1992) illustrates one way in which GAs can beused on this task; it also illustrates some potential pitfalls.Schulze-Kremer took the amino acid sequence of the Crambin protein and used a GA to search in the spaceof possible structures for one that would fit well with Crambin's amino acid sequence. The moststraight-forward way to describe the structure of a protein is to list the three-dimensional coordinates of eachamino acid, or even each atom. In principle, a GA could use such a representation, evolving vectors ofcoordinates to find one that resulted in a plausible structure. But, because of a number of difficulties with thatrepresentation (e.g., the usual crossover and mutation operators would be too likely to create physicallyimpossible structures), Schulze-Kremer instead described protein structures using "torsion angles"—roughly,the angles made by the peptide bonds connecting amino acids and the angles made by bonds in an aminoacid's "side chain." (See Dickerson and Geis 1969 for an overview of how three-dimensional protein structureis measured.) Schulze-Kremer used 10 torsion angles to describe each of the N (46 in the case of Crambin)amino acids in the sequence for a given protein. This collection ofN sets of 10 torsion angles completelydefines the three-dimensional structure of the protein. A chromosome, representing a candidate structure withN amino acids, thus contains N sets of ten real numbers. This representation is illustrated in figure 2.15.Chapter 2: Genetic Algorithms in Problem Solving47Figure 2.14: A representation of the three-dimensional structure of a Crambin protein. (From the "PDB at aGlance" page at the World Wide Web URL http://www.nih.gov/molecular_modeling/pdb_at_a_glance.)The next step is to define a fitness function over the space of chromosomes. The goal is to find a structure thathas low potential energy for the given sequence of amino acids. This goal is based on the assumption that asequence of amino acids will fold to a minimal-energy state, where energy is a function of physical andchemical properties of the individual amino acids and their spatial interactions (e.g., electrostatic pairinteractions between atoms in two spatially adjacent amino acids). If a complete description of the relevantforces were known and solvable, then in principle the minimum-energy structure could be calculated.However, in practice this problem is intractable, and biologists instead develop approximate models todescribe the potential energy of a structure. These models are essentially intelligent guesses as to what themost relevant forces will be. Schulze-Kremer's initial experiments used a highly simplified model in whichthe potential energy of a structure was assumed to be a function of only the torsion angles, electrostatic pairinteractions between atoms, and van der Waals pair interactions between atoms (Schulze-Kremer 1992). Thegoal was for the GA to find a structure (defined in terms of torsion angles) that minimizedFigure 2.15: An illustration of the representation for protein structure used in Schulze-Kremer's experiments.Each of the N amino acids in the sequence is represented by 10 torsion angles: Æ È É and x 1. (SeeSchulze-Kremer 1992 for details of what these angles represent.) A chromosome is a list of these N sets of 10angles. Crossover points are chosen only at amino acid boundaries.Chapter 2: Genetic Algorithms in Problem Solving48this simplified potential-energy function for the amino acid sequence of Crambin.In Schulze-Kremer's GA, crossover was either two-point (i.e., performed at two points along thechromosome rather than at one point) or uniform (i.e., rather than taking contiguous segments from eachparent to form the offspring, each "gene" is chosen from one or the other parent, with a 50% probability foreach parent). Here a "gene" consisted of a group of 10 torsion angles; crossover points were chosen only atamino acid boundaries. Two mutation operators designed to work on real numbers rather than on bits wereused: the first replaced a randomly chosen torsion angle with a new value randomly chosen from the 10 mostfrequently occurring angle values for that particular bond, and the second incremented or decremented arandomly chosen torsion angle by a small amount.The GA started on a randomly generated initial population of ten structures and ran for 1000 generations. Ateach generation the fitness was calculated (here, high fitness means low potential energy), the population wassorted by fitness, and a number of the highest-fitness individuals were selected to be parents for the nextgeneration (this is, again, a form of rank selection). Offspring were created via crossover and mutation. Ascheme was used in which the probabilities of the different mutation and crossover operators increased ordecreased over the course of the run. In designing this scheme, Schulze-Kremer relied on his intuitions aboutwhich operators were likely to be most useful at which stages of the run.The GA's search produced a number of structures with quite low potential energy—in fact, much lower thanthat of the actual structure for Crambin! Unfortunately, however, none of the generated individuals wasstructurally similar to Crambin. The snag was that it was too easy for the GA to find low-energy structuresunder the simplified potential energy function; that is, the fitness function was not sufficiently constrained toforce the GA to find the actual target structure. The fact that Schulze-Kremer's initial experiments were notvery successful demonstrates how important it is to get the fitness function right—here, by getting thepotential-energy model right (a difficult biophysical problem), or at least getting a good enoughapproximation to lead the GA in the right direction.Schulze-Kremer's experiments are a first step in the process of "getting it right." I predict that fairly soon GAsand other machine learning methods will help biologists make real breakthroughs in protein folding and inother areas of molecular biology. I'll even venture to predict that this type of application will be much moreprofitable (both scientifically and financially) than using GAs to predict financial markets.2.3 EVOLVING NEURAL NETWORKSNeural networks are biologically motivated approaches to machine learning, inspired by ideas fromneuroscience. Recently some efforts have been made to use genetic algorithms to evolve aspects of neuralnetworks.In its simplest "feedforward" form (figure 2.16), a neural network is a collection of connected activatable units("neurons") in which the connections are weighted, usually with real-valued weights. The network ispresented with an activation pattern on its input units, such a set of numbers representing features of an imageto be classified (e.g., the pixels in an image of a handwritten letter of the alphabet). Activation spreads in aforward direction from the input units through one or more layers of middle ("hidden") units to the outputunits over the weighted connections. Typically, the activation coming into a unit from other units is multipliedby the weights on the links over which it spreads, and then is added together with other incoming activation.The result is typically thresholded (i.e., the unit "turns on" if the resulting activation is above that unit'sthreshold). This process is meant to roughly mimic the way activation spreads through networks of neurons inChapter 2: Genetic Algorithms in Problem Solving49the brain. In a feedforward network, activation spreads only in a forward direction, from the input layerthrough the hidden layers to the output layer. Many people have also experimented with "recurrent" networks,in which there are feedback connections as well as feedforward connections between layers.Figure 2.16: A schematic diagram of a simple feedforward neural network and the backpropagation processby which weight values are adjusted.After activation has spread through a feedforward network, the resulting activation pattern on the output unitsencodes the network's "answer" to the input (e.g., a classification of the input pattern as the letter A). In mostapplications, the network learns a correct mapping between input and output patterns via a learning algorithm.Typically the weights are initially set to small random values. Then a set of training inputs is presentedsequentially to the network. In the back-propagation learning procedure (Rumelhart, Hinton, and Williams1986), after each input has propagated through the network and an output has been produced, a "teacher"compares the activation value at each output unit with the correct values, and the weights in the network areadjusted in order to reduce the difference between the network's output and the correct output. Each iterationof this procedure is called a "training cycle," and a complete pass of training cycles through the set of traininginputs is called a "training epoch." (Typically many training epochs are needed for a network to learn tosuccessfully classify a given set of training inputs.) This type of procedure is known as "supervised learning,"since a teacher supervises the learning by providing correct output values to guide the learning process. In"unsupervised learning" there is no teacher, and the learning system must learn on its own using less detailed(and sometimes less reliable) environmental feedback on its performance. (For overviews of neural networksand their applications, see Rumelhart et al. 1986, McClelland et al. 1986, and Hertz, Krogh, and Palmer1991.)There are many ways to apply GAs to neural networks. Some aspects that can be evolved are the weights in afixed network, the network architecture (i.e., the number of units and their interconnections can change), andthe learning rule used by the network. Here I will describe four different projects, each of which uses a geneticalgorithm to evolve one of these aspects. (Two approaches to evolving network architecture will bedescribed.) (For a collection of papers on various combinations of genetic algorithms and neural networks, seeWhitley and Schaffer 1992.)Evolving Weights in a Fixed NetworkDavid Montana and Lawrence Davis (1989) took the first approach—evolving the weights in a fixed network.That is, Montana and Davis were using the GA instead of back-propagation as a way of finding a good set ofweights for a fixed set of connections. Several problems associated with the back-propagation algorithm (e.g.,Chapter 2: Genetic Algorithms in Problem Solving50the tendency to get stuck at local optima in weight space, or the unavailability of a "teacher" to superviselearning in some tasks) often make it desirable to find alternative weighttraining schemes.Montana and Davis were interested in using neural networks to classify underwater sonic "lofargrams"(similar to spectrograms) into two classes: "interesting" and "not interesting." The overall goal was to "detectand reason about interesting signals in the midst of the wide variety of acoustic noise and interference whichexist in the ocean." The networks were to be trained from a database containing lofargrams and classificationsmade by experts as to whether or not a given lofargram is "interesting." Each network had four input units,representing four parameters used by an expert system that performed the same classification. Each networkhad one output unit and two layers of hidden units (the first with seven units and the second with ten units).The networks were fully connected feedforward networks—that is, each unit was connected to every unit inthe next higher layer. In total there were 108 weighted connections between units. In addition, there were 18weighted connections between the noninput units and a "threshold unit" whose outgoing links implementedthe thresholding for each of the non-input units, for a total of 126 weights to evolve.The GA was used as follows. Each chromosome was a list (or "vector") of 126 weights. Figure 2.17 shows(for a much smaller network) how the encoding was done: the weights were read off the network in a fixedorder (from left to right and from top to bottom) and placed in a list. Notice that each "gene" in thechromosome is a real number rather than a bit. To calculate the fitness of a given chromosome, the weights inthe chromosome were assigned to the links in the corresponding network, the network was run on the trainingset (here 236 examples from the database of lofargrams), and the sum of the squares of the errors (collectedover all the training cycles) was returned. Here, an "error" was the difference between the desired outputactivation value and the actual output activation value. Low error meant high fitness.Figure 2.17: Illustration of Montana and Davis's encoding of network weights into a list that serves as achromosome for the GA. The units in the network are numbered for later reference. The real-valued numberson the links are the weights.Figure 2.18: Illustration of Montana and Davis's mutation method. Here the weights on incoming links to unit5 are mutated.Chapter 2: Genetic Algorithms in Problem Solving51An initial population of 50 weight vectors was chosen randomly, with each weight being between ‘.0 and +1.0. Montana and Davis tried a number of different genetic operators in various experiments. The mutationand crossover operators they used for their comparison of the GA with back-propagation are illustrated inFigure 2.18 and Figure 2.19. The mutation operator selects n non-input units and, for each incoming link tothose units, adds a random value between ‘.0 and + 1.0 to the weight on the link. The crossover operatortakes two parent weight vectors and, for each non-input unit in the offspring vector, selects one of the parentsat random and copies the weights on the incoming links from that parent to the offspring. Notice that only oneoffspring is created.The performance of a GA using these operators was compared with the performance of a back-propagationalgorithm. The GA had a population of 50 weight vectors, and a rank-selection method was used. The GAwas allowed to run for 200 generations (i.e., 10,000 network evaluations). The back-propagation algorithmwas allowed to run for 5000 iterations, where one iteration is a complete epoch (a complete pass through thetraining data). Montana and Davis reasoned that two network evaluationsFigure 2.19: Illustration of Montana and Davis's crossover method. The offspring is created as follows: foreach non-input unit, a parent is chosen at random and the weights on the incoming links to that unit arecopied from the chosen parent. In the child network shown here, the incoming links to unit 4 come fromparent 1 and the incoming links to units 5 and 6 come from parent 2.under the GA are equivalent to one back-propagation iteration, since back-propagation on a given trainingexample consists of two parts—the forward propagation of activation (and the calculation of errors at theoutput units) and the backward error propagation (and adjusting of the weights). The GA performs only thefirst part. Since the second part requires more computation, two GA evaluations takes less than half thecomputation of a single back-propagation iteration.The results of the comparison are displayed in figure 2.20. Here one back-propagation iteration is plotted forevery two GA evaluations. The x axis gives the number of iterations, and the y axis gives the best evaluation(lowest sum of squares of errors) found by that time. It can be seen that the GA significantly outperformsback-propagation on this task, obtaining better weight vectors more quickly.Chapter 2: Genetic Algorithms in Problem Solving52This experiment shows that in some situations the GA is a better training method for networks than simpleback-propagation. This does not mean that the GA will outperform back-propagation in all cases. It is alsopossible that enhancements of back-propagation might help it overcome some of the problems that preventedit from performing as well as the GA in this experiment. Schaffer, Whitley, and Eshelman (1992) point outFigure 2.20: Montana and Davis's results comparing the performance of the GA with back-propagation. Thefigure plots the best evaluation (lower is better) found by a given iteration. Solid line: genetic algorithm.Broken line: back-propagation. (Reprinted from Proceedings of the International Joint Conference onArtficial Intelligence; © 1989 Morgan Kaufmann Publishers, Inc. Reprinted by permission of the publisher.)that the GA has not been found to outperform the best weight-adjustment methods (e.g., "quickprop") onsupervised learning tasks, but they predict that the GA will be most useful in finding weights in tasks whereback-propagation and its relatives cannot be used, such as in unsupervised learning tasks, in which the errorat each output unit is not available to the learning system, or in situations in which only sparse reinforcementis available. This is often the case for "neurocontrol" tasks, in which neural networks are used to controlcomplicated systems such as robots navigating in unfamiliar environments.Evolving Network ArchitecturesMontana and Davis's GA evolved the weights in a fixed network. As in most neural network applications, thearchitecture of the network—the number of units and their interconnections—is decided ahead of time by theprogrammer by guesswork, often aided by some heuristics (e.g., "more hidden units are required for moredifficult problems") and by trial and error. Neural network researchers know all too well that the particulararchitecture chosen can determine the success or failure of the application, so they would like very much to beable to automatically optimize the procedure of designing an architecture for a particular application. Manybelieve that GAs are well suited for this task. There have been several efforts along these lines, most of whichfall into one of two categories: direct encoding and grammatical encoding. Under direct encoding, a networkarchitecture is directly encoded into a GA chromosome. Under grammatical encoding, the GA does not evolvenetwork architectures:Chapter 2: Genetic Algorithms in Problem Solving53Figure 2.21: An illustration of Miller, Todd, and Hegde's representation scheme. Each entry in the matrixrepresents the type of connection on the link between the "from unit" (column) and the "to unit" (row). Therows of the matrix are strung together to make the bit-string encoding of the network, given at the bottom ofthe figure. The resulting network is shown at the right. (Adapted from Miller, Todd, and Hegde 1989.)rather, it evolves grammars that can be used to develop network architectures.Direct EncodingThe method of direct encoding is illustrated in work done by Geoffrey Miller, Peter Todd, and Shailesh Hegde(1989), who restricted their initial project to feedforward networks with a fixed number of units for which theGA was to evolve the connection topology. As is shown in figure 2.21, the connection topology wasrepresented by an N x N matrix (5 x 5 in figure 2.21) in which each entry encodes the type of connection fromthe "from unit" to the "to unit." The entries in the connectivity matrix were either "0" (meaning no connection)or "L" (meaning a "learnable" connection—i.e., one for which the weight can be changed through learning).Figure 2.21 also shows how the connectivity matrix was transformed into a chromosome for the GA ("O"corresponds to 0 and "L" to 1) and how the bit string was decoded into a network. Connections that werespecified to be learnable were initialized with small random weights. Since Miller, Todd, and Hegde restrictedthese networks to be feedforward, any connections to input units or feedback connections specified in thechromosome were ignored.Miller, Todd, and Hegde used a simple fitness-proportionate selection method and mutation (bits in the stringwere flipped with some low probability). Their crossover operator randomly chose a row index and swappedthe corresponding rows between the two parents to create two offspring. The intuition behind that operatorwas similar to that behind Montana and Davis's crossover operator—each row represented all the incomingconnections to a single unit, and this set was thought to be a functional building block of the network. Thefitness of a chromosome was calculated in the same way as in Montana and Davis's project: for a givenproblem, the network was trained on a training set for a certain number of epochs, using back-propagation tomodify the weights. The fitness of the chromosome was the sum of the squares of the errors on the training setat the last epoch. Again, low error translated to high fitness.Miller, Todd, and Hegde tried their GA on three tasks:XOR: The single output unit should turn on (i.e., its activation should be above a set threshold) if theexclusive-or of the initial values (1 = on and 0 = off) of the two input units is 1.Four Quadrant: The real-valued activations (between 0.0 and 1.0) of the two input units represent thecoordinates of a point in a unit square. All inputs representing points in the lower left and upper rightquadrants of the square should produce an activation of 0.0 on the single output unit, and all other pointsshould produce an output activation of 1.0.Encoder/Decoder (Pattern Copying): The output units (equal in number to the input units) should copy theChapter 2: Genetic Algorithms in Problem Solving54initial pattern on the input units. This would be trivial, except that the number of hidden units is smaller thanthe number of input units, so some encoding and decoding must be done.These are all relatively easy problems for multi-layer neural networks to learn to solve underback-propagation. The networks had different numbers of units for different tasks (ranging from 5 units forthe XOR task to 20 units for the encoder/decoder task); the goal was to see if the GA could discover a goodconnection topology for each task. For each run the population size was 50, the crossover rate was 0.6, and themutation rate was 0.005. In all three tasks, the GA was easily able to find networks that readily learned to mapinputs to outputs over the training set with little error. However, the three tasks were too easy to be a rigoroustest of this method—it remains to be seen if this method can scale up to more complex tasks that require muchlarger networks with many more interconnections. I chose the project of Miller, Todd, and Hegde to illustratethis approach because of its simplicity. For several examples of more sophisticated approaches to evolvingnetwork architectures using direct encoding, see Whitley and Schaffer 1992.Grammatical EncodingThe method of grammatical encoding can be illustrated by the work of Hiroaki Kitano (1990), who points outthat direct-encoding approachs become increasingly difficult to use as the size of the desired networkincreases. As the network's size grows, the size of the required chromosome increases quickly, which leads toproblems both in performance (how high a fitness can be obtained) and in efficiency (how long it takes toobtain high fitness). In addition, since direct-encoding methods explicitly represent each connection in thenetwork, repeated or nested structures cannot be represented efficiently, even though these are common forsome problems.The solution pursued by Kitano and others is to encode networks as grammars; the GA evolves the grammars,but the fitness is tested only after a "development" step in which a network develops from the grammar. Thatis, the "genotype" is a grammar, and the "phenotype" is a network derived from that grammar.A grammar is a set of rules that can be applied to produce a set of structures (e.g., sentences in a naturallanguage, programs in a computer language, neural network architectures). A simple example is the followinggrammar:Here S is the start symbol and a nonterminal, a and b are terminals, and µ is the empty-string terminal.(S ’ µmeans that S can be replaced by the empty string.) To construct a structure from this grammar, start with S,and replace it by one of the allowed replacements given by the righthand sides (e.g., S ’ aSb). Now take theresulting structure and replace any nonterminal (here S) by one of its allowed replacements (e.g., aSb ’aaSbb). Continue in this way until no nonterminals are left (e.g., aaSbb ’ aabb, using S ’ µ). It can easily beshown that the set of structures that can be produced by this grammar are exactly the strings anbn consisting ofthe same number of as and bs with all the as on the left and all the bs on the right.Kitano applied this general idea to the development of neural networks using a type of grammar called a"graph-generation grammar," a simple example of which is given in figure 2.22a Here the right-hand side ofeach rule is a 2 × 2 matrix rather than a one-dimensional string. Capital letters are nonterminals, andlower-case letters are terminals. Each lower-case letter from a through p represents one of the 16 possible 2 ×2 arrays of ones and zeros. In contrast to the grammar foranbn given above, each nonterminal in this particulargrammar has exactly one right-hand side, so there is only one structure that can be formed from thisgrammar: the 8 x 8 matrix shown in figure 2.22b This matrix can be interpreted as a connection matrix for aChapter 2: Genetic Algorithms in Problem Solving55neural network: a 1 in row i and column j,i`j means that unit i is present in the network and a 1 in row i andcolumn i; i means that there is a connection from uniti to unit j. (In Kitano's experiments, connections to orfrom nonexistent units and recurrent connections were ignored.) The result is the network shown in figure2.22c, which, with appropriate weights, computes the Boolean function XOR.Kitano's goal was to have a GA evolve such grammars. Figure 2.23 illustrates a chromosome encoding thegrammar given in figure 2.22a The chromosome is divided up into separate rules, each of which consists offive loci. The first locus is the left-hand side of the rule; the secondFigure 2.22: Illustration of the use of Kitano's "graph generation grammar" to produce a network to solve theXOR problem. (a) Grammatical rules, (b) A connection matrix is produced from the grammar. (c) Theresulting network. (Adapted from Kitano 1990.)Figure 2.23: Illustration of a chromosome encoding a grammar.through fifth loci are the four symbols in the matrix on the right-hand side of the rule. The possible alleles ateach locus are the symbols A–Z and a–p. The first locus of the chromosome is fixed to be the start symbol, S;at least one rule taking S into a 2 × 2 matrix is necessary to get started in building a network from a grammar.All other symbols are chosen at random. A network is built applying the grammar rules encoded in thechromosome for a predetermined number of iterations. (The rules that take a–p to the 16 2 × 2 matrices ofzeros and ones are fixed and are not represented in the chromosome.) In the simple version used by Kitano, ifa nonterminal (e.g., A) appears on the left-hand side in two or more different rules, only the first such rule isincluded in the grammar (Hiroaki Kitano, personal communication).The fitness of a grammar was calculated by constructing a network from the grammar, usingback-propagation with a set of training inputs to train the resulting network to perform a simple task, andthen, after training, measuring the sum of the squares of the errors made by the network on either the trainingset or a separate test set. (This is similar to the fitness measure used by Montana and Davis and by Miller,Chapter 2: Genetic Algorithms in Problem Solving56Todd, and Hegde.) The GA used fitness-proportionate selection, multi-point crossover (crossover wasperformed at one or more points along the chromosome), and mutation. A mutation consisted of replacing onesymbol in the chromosome with a randomly chosen symbol from the A–Z and a–p alphabets. Kitano usedwhat he called "adaptive mutation": the probability of mutation of an offspring depended on the Hammingdistance (number of mismatches) between the two parents. High distance resulted in low mutation, and viceversa. In this way, the GA tended to respond to loss of diversity in the population by selectively raising themutation rate.Kitano (1990) performed a series of experiments on evolving networks for simple "encoder/decoder"problems to compare the grammatical and direct encoding approaches. He found that, on these relativelysimple problems, the performance of a GA using the grammatical encoding method consistently surpassedthat of a GA using the direct encoding method, both in the correctness of the resulting neural networks and inthe speed with which they were found by the GA. An example of Kitano's results is given in figure 2.24,which plots the error rate of the best network in the population (averaged over 20 runs) versus generation. Inthe grammatical encoding runs, the GA found networks with lower error rate, and found the best networksmore quickly, than in the direct encoding runs. Kitano also discovered that the performance of the GA scaledmuch better with network size when grammatical encoding was used—performance decreased very quicklywith network size when direct encoding was used, but stayed much more constant with grammatical encoding.What accounts for the grammatical encoding method's apparent superiority? Kitano argues that thegrammatical encoding method can easily create "regular," repeated patterns of connectivity, and that this is aresult of the repeated patterns that naturally come from repeatedly applying grammatical rules. We wouldexpect grammatical encoding approaches to perform well on problems requiring this kind of regularity.Grammatical encoding also has the advantage of requiring shorter chromosomes, since the GA works on theinstructions for building the network (the grammar) rather than on the network structure itself. For complexnetworks, the latter could be huge and intractable for any search algorithm.Although these attributes might lend an advantage in general to the grammatical encoding method, it is notclear that they accounted for the grammatical encoding method's superiority in the experiments reported byKitano (1990). The encoder/decoder problem is one of the simplestFigure 2.24: Results from Kitano's experiment comparing the direct and grammatical encoding methods. Totalsum squared (TSS) error for the average best individual (over 20 runs) is plotted against generation. (LowTSS is desired.) (Reprinted from Kitano 1990 by permission of the publisher. © 1990 Complex systems.)problems for neural networks; moreover, it is interesting only if the number of hidden units is smaller than thenumber of input units. This was enforced in Kitano's experiments with direct encoding but not in hisChapter 2: Genetic Algorithms in Problem Solving57experiments with grammatical encoding. It is possible that the advantage of grammatical encoding in theseexperiments was simply due to the GA's finding network topologies that make the problem trivial; thecomparison is thus unfair, since this route was not available to the particular direct encoding approach beingcompared.Kitano's idea of evolving grammars is intriguing, and his informal arguments are plausible reasons to believethat the grammatical encoding method (or extensions of it) will work well on the kinds of problems on whichcomplex neural networks could be needed. However, the particular experiments used to support the argumentsare not convincing, since the problems may have been too simple. An extension of Kitano's initial work, inwhich the evolution of network architecture and the setting of weights are integrated, is reported in Kitano1994. More ambitious approaches to grammatical encoding have been tried by Gruau (1992) and Belew(1993).Evolving a Learning RuleDavid Chalmers (1990) took the idea of applying genetic algorithms to neural networks in a differentdirection: he used GAs to evolve a good learning rule for neural networks. Chalmers limited his initial studyto fully connected feedforward networks with input and output layers only, no hidden layers. In general alearning rule is used during the training procedure for modifying network weights in response to the network'sperformance on the training data. At each training cycle, one training pair is given to the network, which thenproduces an output. At this point the learning rule is invoked to modify weights. A learning rule for asingle-layer, fully connected feedforward network might use the following local information for a giventraining cycle to modify the weight on the link from input unit i to output unit j:ai: the activation of input unit ioj: the activation of output unit jtj: the training signal (i.e., correct activation, provided by a teacher) on output unit jwij: the current weight on the link from i to j.The change to make in weight wij, ”wij, is a function of these values:The chromosomes in the GA population encoded such functions.Chalmers made the assumption that the learning rule should be a linear function of these variables and all theirpairwise products. That is, the general form of the learning rule wasThe km (1dmd10) are constant coefficients, and k0 is a scale parameter that affects how much the weights canchange on any one cycle. (k0 is called the "learning rate.") Chalmers's assumption about the form of thelearning rule came in part from the fact that a known good learning rule for such networks—the"Widrow-Hoff" or "delta" rule—has the formChapter 2: Genetic Algorithms in Problem Solving58(Rumelhart et al. 1986), where n is a constant representing the learning rate. One goal of Chalmers's work wasto see if the GA could evolve a rule that performs as well as the delta rule.The task of the GA was to evolve values for the km's. The chromosome encoding for the set of km's isillustrated in figure 2.25. The scale parameter k0 is encoded as five bits, with the zeroth bit encoding the sign(1 encoding + and 0 encoding  and the first through fourth bits encoding an integer n: k 0 = 0 if n = 0;otherwise |k 0 |=2 n–9. Thus k0 can take on the values 0, ±1/256, ±1/128,…, ±32, ±64. The other coefficients kmare encoded by three bits each, with the zeroth bit encoding the sign and theFigure 2.25: Illustration of the method for encoding the kms in Chalmers's system.first and second bits encoding an integer n. For i = 1…10, km = 0 if n = 0; otherwise |km | = 2n–1.It is known that single-layer networks can learn only those classes of input-output mappings that are "linearlyseparable" (Rumelhart et al. 1986). As an "environment" for the evolving learning rules, Chalmers used 30different linearly separable mappings to be learned via the learning rules. The mappings always had a singleoutput unit and between two and seven input units.The fitness of each chromosome (learning rule) was determined as follows. A subset of 20 mappings wasselected from the full set of 30 mappings. For each mapping, 12 training examples were selected. For each ofthese mappings, a network was created with the appropriate number of input units for the given mapping(each network had one output unit). The network's weights were initialized randomly. The network was runon the training set for some number of epochs (typically 10), using the learning rule specified by thechromosome. The performance of the learning rule on a given mapping was a function of the network's erroron the training set, with low error meaning high performance. The overall fitness of the learning rule was aChapter 2: Genetic Algorithms in Problem Solving59function of the average error of the 20 networks over the chosen subset of 20 mappings—low average errortranslated to high fitness. This fitness was then transformed to be a percentage, where a high percentage meanthigh fitness.Using this fitness measure, the GA was run on a population of 40 learning rules, with two-point crossoverand standard mutation. The crossover rate was 0.8 and the mutation rate was 0.01. Typically, over 1000generations, the fitness of the best learning rules in the population rose from between 40% and 60% in theinitial generation (indicating no significant learning ability) to between 80% and 98%, with a mean (overseveral runs) of about 92%. The fitness of the delta rule is around 98%, and on one out of a total of ten runsthe GA discovered this rule. On three of the ten runs, the GA discovered slight variations of this rule withlower fitness.These results show that, given a somewhat constrained representation, the GA was able to evolve a successfullearning rule for simple single-layer networks. The extent to which this method can find learning rules formore complex networks (including networks with hidden units) remains an open question, but these resultsare a first step in that direction. Chalmers suggested that it is unlikely that evolutionary methods will discoverlearning methods that are more powerful than back-propagation, but he speculated that the GA might be apowerful method for discovering learning rules for unsupervised learning paradigms (e.g., reinforcementlearning) or for new classes of network architectures (e.g., recurrent networks).Chalmers also performed a study of the generality of the evolved learning rules. He tested each of the bestevolved rules on the ten mappings that had not been used in the fitness calculation for that rule (the "test set").The mean fitness of the best rules on the original mappings was 92%, and Chalmers found that the meanfitness of these rules on the test set was 91.9%. In short, the evolved rules were quite general.Chalmers then looked at the question of how diverse the environment has to be to produce general rules. Herepeated the original experiment, varying the number of mappings in each original environment between 1and 20. A rule's evolutionary fitness is the fitness obtained by testing a rule on its original environment. Arule's test fitness is the fitness obtained by testing a rule on ten additional tasks not in the originalenvironment. Chalmers then measured these two quantities as a function of the number of tasks in the originalenvironment. The results are shown in figure 2.26. The two curves are the mean evolutionary fitness and themean test fitness for rules that were tested in an environment with the given number of tasks. This plot showsthat while the evolutionary fitness stays roughly constant for different numbers of environmental tasks, thetest fitness increases sharply with the number of tasks, leveling off somewhere between 10 and 20 tasks. Theconclusion is that the evolution of a general learning rule requires a diverse environment of tasks. (In this caseof simple single-layer networks, the necessary degree of diversity is fairly small.)THOUGHT EXERCISES1. Using the function set {AND, OR, NOT} and the terminal set {s–1, s0, s+1}, construct a parse tree (orLisp expression) that encodes the r = 1 majority-rule CA, where si denotes the state of theneighborhood site i sites away from the central cell (with  indicating distance to the left and +indicating distance to the right). AND and OR each take two arguments, and NOT takes oneargument.Chapter 2: Genetic Algorithms in Problem Solving60Figure 2.26: Results of Chalmers's experiments testing the effect of diversity of environment ongeneralization ability. The plot gives the evolutionary fitness (squares) and test fitness (diamonds) asa function of the number of tasks in the environment. (Reprinted from D. S. Touretzky et al. (eds.),Proceedings of the 1990 Connectionist Models Summer School Reprinted by permission of thepublisher. © 1990 Morgan Kaufmann.)2. Assume that "MUTATE-TREE(TREE)" is a function that replaces a subtree in TREE by a randomlygenerated subtree. Using this function, write pseudo code for a steepest-ascent hill climbingalgorithm that searches the space of GP parse trees, starting from a randomly chosen parse tree. Dothe same for random-mutation hill climbing.3. Write a formula for the number of CA rules of radius r.4. Follow the same procedure as in figure 2.23 to construct the network given by the grammar displayedin figure 2.27.Figure 2.27: Grammar for thought exercise 45. Design a grammar that will produce the network architecture given in figure 2.28.Figure 2.28: Network for thought exercise 5.Chapter 2: Genetic Algorithms in Problem Solving61COMPUTER EXERCISES1. Implement a genetic programming algorithm and use it to solve the "6-multiplexer" problem (Koza1992). In this problem there are six Boolean-valued terminals, {a0, a1, d0,d1, d2, d3}, and fourfunctions, {AND, OR, NOT, IF}. The first three functions are the usual logical operators, taking two,two, and one argument respectively, and the IF function takes three arguments. (IF X Y Z) evaluatesits first argument X. If X is true, the second argument Y is evaluated; otherwise the third argument Zis evaluated. The problem is to find a program that will return the value of the d terminal that isaddressed by the two a terminals. E.g., if a0 = 0 and a1 = 1, the address is 01 and the answer is thevalue of d1. Likewise, if a0 = 1 and a1 = 1, the address is 11 and the answer is the value of d3.Experiment with different initial conditions, crossover rates, and population sizes. (Start with apopulation size of 300.) The fitness of a program should be the fraction of correct answers over all 26possible fitness cases (i.e., values of the six terminals).2. Perform the same experiment as in computer exercise 1, but add some "distractors" to the functionand terminal sets—extra functions and terminals not necessary for the solution. How does this affectthe performance of GP on this problem?3. Perform the same experiment as in computer exercise 1, but for each fitness calculation use a randomsample of 10 of the 26 possible fitness cases rather than the entire set (use a new random sample foreach fitness calculation). How does this affect the performance of GP on this problem?4. Implement a random search procedure to search for parse trees for the 6-multiplexer problem: at eachtime step, generate a new random parse tree (with the maximum tree size fixed ahead of time) andcalculate its fitness. Compare the rate at which the best fitness found so far (plotted every 300 timesteps—equivalent to one GP generation in computer exercise 1) increases with that under GP.5. Implement a random-mutation hill-climbing procedure to search for parse trees for the 6-multiplexerproblem (see thought exercise 2). Compare its performance with that of GP and the random searchmethod of computer exercise 4.6. Modify the fitness function used in computer exercise 1 to reward programs for small size as well asfor correct performance. Test this new fitness function using your GP procedure. Can GP find correctbut smaller programs by this method?7. *Repeat the experiments of Crutchfield, Mitchell, Das, and Hraber on evolving r = 3 CAs to solve the problem. (This will also require writing a program to simulate cellular automata.)8. *Chapter 2: Genetic Algorithms in Problem Solving62Compare the results of the experiment in computer exercise 7 with that of using random-mutation hillclimbing to search for CA lookup tables to solve   problem. (See Mitchell, Crutchfield, andHraber 1994a for their comparison.)9. *Perform the same experiment as in computer exercise 7, but use GP on parse-tree representations ofCAs (see thought exercise 1). (This will require writing a program to translate between parse treerepresentations and CA lookup tables that you can give to your CA simulator.) Compare the results ofyour experiments with the results you obtained in computer exercise 7 using lookup-table encodings.10. *Figure 2.29 gives a 19-unit neural network architecture for the "encoder/decoder" problem. Theproblem is to find a set of weights so that the network will perform the mapping given in table2.2—that is, for each given input activation pattern, the network should copy the pattern onto itsoutput units. Since there are fewer hidden units than input and output units, the network must learn toencode and then decode the input via the hidden units. Each hidden unit j and each output unit j has athreshold Ãj. If the incoming activation is greater than or equal to Ãj, the activation of the unit is set to1; otherwise it is set to 0. At the first time step, the input units are activated according to the inputactivation pattern (e.g., 10000000). Then activation spreads from the input units to the hiddenFigure 2.29: Network for computer exercise 10. The arrows indicate that each input node is connected to eachhidden node, and each hidden node is connected to each output node.Table 2.2: Table for computer exercise 10.Input Pattern Output Pattern10000000 1000000001000000 0100000000100000 0010000000010000 0001000000001000 0000100000000100 0000010000000010 0000001000000001 00000001unit i and wi, j is the weight on the link from unit i to unit j. After the hidden units have been activated, they inChapter 2: Genetic Algorithms in Problem Solving63turn activate the output units via the same procedure. Use Montana and Davis's method to evolve weights wi, j(0 d wi, j d 1) and thresholds Ãj (0dÃjd1) to solve this problem. Put the wi,j values on the same chromosome.(The dj values are ignored by the input nodes, which are always set to 0 or 1.) The fitness of a chromosome isthe average sum of the squares of the errors (differences between the output and input patterns at eachposition) over the entire training set. How well does the GA succeed? For the very ambitious reader: Comparethe performance of the GA with that of back-propagation (Rumelhart, Hinton, and Williams 1986a) in thesame way that Montana and Davis did. (This exercise is intended for those already familiar with neuralnetworks.)Chapter 2: Genetic Algorithms in Problem Solving64Chapter 3: Genetic Algorithms in Scientific ModelsOverviewGenetic algorithms have been for the most part techniques applied by computer scientists and engineers tosolve practical problems. However, John Holland's original work on the subject was meant not only todevelop adaptive computer systems for problem solving but also to shed light, via computer models, on themechanisms of natural evolution.The idea of using computer models to study evolution is still relatively new and is not widely accepted in theevolutionary biology community. Traditionally, biologists have used several approaches to understandingevolution, including the following:Examining the fossil record to determine how evolution has proceeded over geological time.Examining existing biological systems in their natural habitats in order to understand the evolutionary forcesat work in the process of adaptation. This includes both understanding the role of genetic mechanisms (such asgeographical effects on mating) and understanding the function of various physical and behavioralcharacteristics of organisms so as to infer the selective forces responsible for the evolution of theseadaptations.Performing laboratory experiments in which evolution over many generations in a population of relativelysimple organisms is studied and controlled. Many such experiments involve fruit flies (Drosophila) becausetheir life span and their reproductive cycle are short enough that experimenters can observe natural selectionover many generations in a reasonable amount of time.Studying evolution at the molecular level by looking at how DNA and RNA change over time under particulargenetic mechanisms, or by determining how different evolutionarily related species compare at the level ofDNA so as to reconstruct "phylogenies" (evolutionary family histories of related species).Developing mathematical models of evolution in the form of equations (representing properties of genotypesand phenotypes and their evolution) that can be solved (analytically or numerically) or approximated.These are the types of methods that have produced the bulk of our current understanding of natural evolution.However, such methods have a number of inherent limitations. The observed fossil record is almost certainlyincomplete, and what is there is often hard to interpret; in many cases what is surmised from fossils isintelligent guesswork. It is hard, if not impossible, to do controlled experiments on biological systems innature, and evolutionary time scales are most often far too long for scientists to directly observe howbiological systems change. Evolution in systems such as Drosophila can be observed to a limited extent, butmany of the important questions in evolution (How does speciation take place? How did multicellularorganisms come into being? Why did sex evolve?) cannot be answered by merely studying evolution inDrosophila. The molecular level is often ambiguous—for example, it is not clear what it is that individualpieces of DNA encode, or how they work together to produce phenotypic traits, or even which pieces do theencoding and which are "junk DNA" (noncoding regions of the chromosome). Finally, to be solvable,mathematical models of evolution must be simplified greatly, and it is not obvious that the simple modelsprovide insight into real evolution.The invention of computers has permitted a new approach to studying evolution and other natural systems:simulation. A computer program can simulate the evolution of populations of organisms over millions of65simulated generations, and such simulations can potentially be used to test theories about the biggest openquestions in evolution. Simulation experiments can do what traditional methods typically cannot: experimentscan be controlled, they can be repeated to see how the modification of certain parameters changes thebehavior of the simulation, and they can be run for many simulated generations. Such computer simulationsare said to be "microanalytic" or "agent based." They differ from the more standard use of computers inevolutionary theory to solve mathematical models (typically systems of differential equations) that captureonly the global dynamics of an evolving system. Instead, they simulate each component of the evolvingsystem and its local interactions; the global dynamics emerges from these simulated local dynamics. This"microanalytic" strategy is the hallmark of artificial life models.Computer simulations have many limitations as models of real-world phenomena. Most often, they mustdrastically simplify reality in order to be computationally tractable and for the results to be understandable. Aswith the even simpler purely mathematical models, it is not clear that the results will apply to more realisticsystems. On the other hand, more realistic models take a long time to simulate, and they suffer from the sameproblem we often face in direct studies of nature: they produce huge amounts of data that are often very hardto interpret.Such questions dog every kind of scientific model, computational or otherwise, and to date most biologistshave not been convinced that computer simulations can teach them much. However, with the increasingpower (and decreasing cost) of computers, and given the clear limitations of simple analytically solvablemodels of evolution, more researchers are looking seriously at what simulation can uncover. Geneticalgorithms are one obvious method for microanalytic simulation of evolutionary systems. Their use in thisarena is also growing as a result of the rising interest among computer scientists in building computationalmodels of biological processes. Here I describe several computer modeling efforts, undertaken mainly bycomputer scientists, and aimed at answering questions such as: How can learning during a lifetime affect theevolution of a species? What is the evolutionary effect of sexual selection? What is the relative density ofdifferent species over time in a given ecosystem? How are evolution and adaptation to be measured in anobserved system?3.1 MODELING INTERACTIONS BETWEEN LEARNING ANDEVOLUTIONMany people have drawn analogies between learning and evolution as two adaptive processes, one takingplace during the lifetime of an organism and the other taking place over the evolutionary history of life onEarth. To what extent do these processes interact? In particular, can learning that occurs over the course of anindividual's lifetime guide the evolution of that individual's species to any extent? These are major questionsin evolutionary psychology. Genetic algorithms, often in combination with neural networks, have been used toaddress these questions. Here I describe two systems designed to model interactions between learning andevolution, and in particular the "Baldwin effect."The Baldwin EffectThe well-known "Lamarckian hypothesis" states that traits acquired during the lifetime of an organism can betransmitted genetically to the organism's offspring. Lamarck's hypothesis is generally interpreted as referringto acquired physical traits (such as physical defects due to environmental toxins), but something learnedduring an organism's lifetime also can be thought of as a type of acquired trait. Thus, a Lamarckian viewmight hold that learned knowledge can guide evolution directly by being passed on genetically to the nextChapter 3: Genetic Algorithms in Scientific Models66generation. However, because of overwhelming evidence against it, the Lamarckian hypothesis has beenrejected by virtually all biologists. It is very hard to imagine a direct mechanism for "reverse transcription" ofacquired traits into a genetic code.Does this mean that learning can have no effect on evolution? In spite of the rejection of Lamarckianism, theperhaps surprising answer seems to be that learning (or, more generally, phenotypic plasticity) can indeedhave significant effects on evolution, though in less direct ways than Lamarck suggested. One proposal for anon-Lamarckian mechanism was made by J.M. Baldwin (1896), who pointed out that if learning helpssurvival then the organisms best able to learn will have the most offspring, thus increasing the frequency ofthe genes responsible for learning. And if the environment remains relatively fixed, so that the best things tolearn remain constant, this can lead, via selection, to a genetic encoding of a trait that originally had to belearned. (Note that Baldwin's proposal was published long before the detailed mechanisms of geneticinheritance were known.) For example, an organism that has the capacity to learn that a particular plant ispoisonous will be more likely to survive (by learning not to eat the plant) than organisms that are unable tolearn this information, and thus will be more likely to produce offspring that also have this learning capacity.Evolutionary variation will have a chance to work on this line of offspring, allowing for the possibility that thetrait—avoiding the poisonous plant—will be discovered genetically rather than learned anew each generation.Having the desired behavior encoded genetically would give an organism a selective advantage overorganisms that were merely able to learn the desired behavior during their lifetimes, because learning abehavior is generally a less reliable process than developing a genetically encoded behavior; too manyunexpected things could get in the way of learning during an organism's lifetime. Moreover, geneticallyencoded information can be available immediately after birth, whereas learning takes time and sometimesrequires potentially fatal trial and error.In short, the capacity to acquire a certain desired trait allows the learning organism to survive preferentially,thus giving genetic variation the possibility of independently discovering the desired trait. Without suchlearning, the likelihood of survival—and thus the opportunity for genetic discovery—decreases. In thisindirect way, learning can guide evolution, even if what is learned cannot be directly transmitted genetically.Baldwin called this mechanism "organic selection," but it was later dubbed the "Baldwin effect" (Simpson1953), and that name has stuck. Similar mechanisms were simultaneously proposed by Lloyd Morgan (1896)and Osborn (1896).The evolutionary biologist G. G. Simpson, in his exegesis of Baldwin's work (Simpson 1953), pointed out thatit is not clear how the necessary correlation between phenotypic plasticity and genetic variation can takeplace. By correlation I mean that genetic variations happen to occur that produce the same adaptation that waspreviously learned. This kind of correlation would be easy if genetic variation were "directed" toward someparticular outcome rather than random. But the randomness of genetic variation is a central principle ofmodern evolutionary theory, and there is no evidence that variation can be directed by acquired phenotypictraits (indeed, such direction would be a Lamarckian effect). It seems that Baldwin was assuming that, giventhe laws of probability, correlation between phenotypic adaptations and random genetic variation will happen,especially if the phenotypic adaptations keep the lineage alive long enough for these variations to occur.Simpson agreed that this was possible in principle and that it probably has happened, but he did not believethat there was any evidence of its being an important force in evolution.Almost 50 years after Baldwin and his contemporaries, Waddington (1942) proposed a similar but moreplausible and specific mechanism that has been called "genetic assimilation." Waddington reasoned thatcertain sweeping environmental changes require phenotypic adaptations that are not necessary in a normalenvironment. If organisms are subjected to such environmental changes, they can sometimes adapt duringtheir lifetimes because of their inherent plasticity, thereby acquiring new physical or behavioral traits. If thegenes for these traits are already in the population, although not expressed or frequent in normalChapter 3: Genetic Algorithms in Scientific Models67environments, they can fairly quickly be expressed in the changed environments, especially if the acquired(learned) phenotypic adaptations have kept the species from dying off. (A gene is said to be "expressed" if thetrait it encodes actually appears in the phenotype. Typically, many genes in an organism's chromosomes arenot expressed.)The previously acquired traits can thus become genetically expressed, and these genes will spread in thepopulation. Waddington demonstrated that this had indeed happened in several experiments on fruit flies.Simpson's argument applies here as well: even though genetic assimilation can happen, that does not meanthat it necessarily happens often or is an important force in evolution. Some in the biology and evolutionarycomputation communities hope that computer simulations can now offer ways to gauge the frequency andimportance of such effects.A Simple Model of the Baldwin EffectGenetic assimilation is well known in the evolutionary biology community. Its predecessor, the Baldwineffect, is less well known, though it has recently been picked up by evolutionary computationalists because ofan interesting experiment performed by Geoffrey Hinton and Steven Nowlan (1987). Hinton and Nowlanemployed a GA in a computer model of the Baldwin effect. Their goal was to demonstrate this effectempirically and to measure its magnitude, using a simplified model. An extremely simple neural-networklearning algorithm modeled learning, and the GA played the role of evolution, evolving a population of neuralnetworks with varying learning capabilities. In the model, each individual is a neural network with 20potential connections. A connection can have one of three values: "present," "absent," and "learnable." Theseare specified by "1," "0," and "?," respectively, where each ? connection can be set during learning to either 1or 0. There is only one correct setting for the connections (i.e., only one correct configuration of ones andzeros), and no other setting confers any fitness on an individual. The problem to be solved isFigure 3.1: Illustration of the fitness landscape for Hinton and Nowlan's search problem. All genotypes havefitness 0 except for the one "correct" genotype, at which there is a fitness spike. (Adapted from Hinton andNowlan 1987.)to find this single correct set of connections. This will not be possible for those networks that have incorrectfixed connections (e.g., a 1 where there should be a 0), but those networks that have correct settings in allplaces except where there are question marks have the capacity to learn the correct settings.Hinton and Nowlan used the simplest possible "learning" method: random guessing. On each learning trial, anetwork simply guesses 1 or 0 at random for each of its learnable connections. (The problem as stated haslittle to do with the usual notions of neural-network learning; Hinton and Nowlan presented this problem interms of neural networks so as to keep in mind the possibility of extending the example to more standardlearning tasks and methods.)This is, of course, a "needle in a haystack" search problem, since there is only one correct setting in a space of220 possibilities. The fitness landscape for this problem is illustrated in figure 3.1—the single spike representsChapter 3: Genetic Algorithms in Scientific Models68the single correct connection setting. Introducing the ability to learn indirectly smooths out the landscape, asshown in figure 3.2. Here the spike is smoothed out into a "zone of increased fitness" that includes individualswith some connections set correctly and the rest set to question marks. Once an individual is in this zone,learning makes it possible to get to the peak.The indirect smoothing of the fitness landscape was demonstrated by Hinton and Nowlan's simulation, inwhich each network was represented by a string of length 20 consisting of the ones, zeros, and the questionmarks making up the settings on the network's connections. The initial population consisted of 1000individuals generated at random but withFigure 3.2: With the possibility of learning, the fitness landscape for Hinton and Nowlan's search problem issmoother, with a zone of increased fitness containing individuals able to learn the correct connection settings.(Adapted from Hinton and Nowlan 1987.)each individual having on average 25% zeros, 25% ones, and 50% question marks. At each generation, eachindividual was given 1000 learning trials. On each learning trial, the individual tried a random combination ofsettings for the question marks. The fitness was an inverse function of the number of trials needed to find thecorrect solution:where n is the number of trials (out of the allotted 1000) remaining after the correct solution has been found.An individual that already had all its connections set correctly was assigned the highest possible fitness (20),and an individual that never found the correct solution was assigned the lowest possible fitness (1). Hence, atradeoff existed between efficiency and plasticity: having many question marks meant that, on average, manyguesses were needed to arrive at the correct answer, but the more connections that were fixed, the more likelyit was that one or more of them was fixed incorrectly, meaning that there was no possibility of finding thecorrect answer.Hinton and Nowlan's GA was similar to the simple GA described in chapter 1. An individual was selected tobe a parent with probability proportional to its fitness, and could be selected more than once. The individualsin the next generation were created by single-point crossovers between pairs of parents. No mutationoccurred. An individual's chromosome was, of course, not affected by the learning that took place during itslifetime—parents passed on their original alleles to their offspring.Hinton and Nowlan ran the GA for 50 generations. A plot of the mean fitness of the population versusgeneration for one run on each of threeChapter 3: Genetic Algorithms in Scientific Models69Figure 3.3: Mean fitness versus generations for one run of the GA on each of three population sizes. The solidline gives the results for population size 1000, the size used in Hinton and Nowlan's experiments; the opencircles the results for population size 250; the solid circles for population size 4000. These plots are from areplication by Belew and are reprinted from Belew 1990 by permission of the publisher. © 1990 ComplexSystems.