abstract representations employed can be used to reduce the subsequent surprise element of the program's behavior. This application of stepwise abstraction hinges on the obvious point that we always know a lot more about what we have, in fact, designed after we've done it, than we do about what we are designing whilst engaged in the process. The popular myth that exercise of a disciplined process of design and development allows us to maintain a full understanding of the eventual outcome—i.e. a program—does  Page 160not bear close scrutiny. For any non-trivial system, further under standing can and does come through observation of behavior, and by a process of stepwise abstraction this extra understanding can be ploughed back into the documentation.However useful you might concede that stepwise abstraction could be, you might also point out that innumerable other seeming impossibilities have stronger claims on our interest and energy. Thus a time-machine would have enormously more potential, and it appears to be no more difficult—they are both impossible. The abstraction process is essentially the problem of decompiling, extracting form from function, and we know that's intractable, so why waste time wishing for the moon? Let's take a closer look at the fundamental process.The problem of decompilingCompilers are common, and decompilers are somewhere between scarce and nonexistent—it depends on the constraints employed. In the earlier discussion of the function-to-form problem (Chapter 4) we looked at how we could lessen the severity of this task by judicious choice and careful use of the 'right' representation—i.e. it is a great help if form reflects function in our programmed systems. We also mentioned how the difficulty (if not impossibility) of decompiling machine code tums out to be such an obstacle that the software industry exploits it to prevent unauthorized tampering with their products.This is not encouraging information, but fortunately there are a number of reasons why the decompiling operations demanded by our SDLCs can be made to fall into the realm of tractable problems.• We are not required to decompile from machine code. We can choose our implementation language wisely.• We are not required to decompile from bare code. We should have full documentation available, including details of the system design procedure as outlined earlier.• We are not required to decompile a complete software system. We typically have some of the necessary abstractions to hand (either as a result of earlier decompilations, or as part of the original documentation), and the demand is for a relatively small-scale decom-pilation exercise.  Page 161• We are not required to decompile a monolithic black box. We can look into the software and see how it is doing what we observe it to be doing in terms of the implementation language. In fact, we can do more than this. We can get the software system to show us how it is doing what it is doing. We can generate traces of the execution sequences responsible for specific behaviors (see, for example, work in 'Program Visualization' introduced by Myers, 1988, and more specifically 'self-explanation' in expert systems described in the next chapter). So we have some explicit structural information to work from, and even though it will not be precisely (or perhaps anything like) the representational form that we might wish to abstract, the explicit presence of a structure underlying the observed functionality can only lessen the severity of the decompiling task.So, without wishing to pretend that decompiling is a simple and straightforward task, I do claim that the nature of the decompiling demanded need not be as daunting a prospect as it first appears. Nevertheless, I do maintain that it is a crucial component of the necessary discipline of ESD (as well as of quite conventional SDLCs), and one that has been largely ignored and neglected because of the belief that its use is indicative of bad practice—i.e. messing with the implementation when attention should be focused on the specification. To my mind, some degree of decompilation is unavoidable when designing and developing complex software systems. It is needed as soon as we are surprised by the behavior of our system. And to claim that the occurrence of such surprises is itself indicative of dubious practices is like claiming that mathematical proofs have no merit—after all, they're just logical inferences from the axioms, if we missed seeing that particular inference sequence then we were clearly not paying close enough attention because it was there all the time.I agree with the widely accepted sentiment that system development should focus on the specification and not the implementation, but I think that the degree to which we can actually do this has been grossly oversold. Such homilies ignore the real problem of complex systems, which is that the window on system adequacy is system performance. Whether we like it or not, the initial spur to system modification is often a behavioral problem, a problem with the implementation, and this must be transformed into implications for the specification before we can shift the focus to where we all agree it should be.  Page 162Hence decompilation is (or should be) an essential component of SDLCs; it is not a misconceived notion to be shunned by every right-minded software developer. A need to decompile is not indicative of a slovenly approach and thus only to be satisfied in secret behind closed doors. But decompilation is a difficult process, although not quite as difficult as it's widely believed to be. The conclusion is that considerably more time, effort and expertise than it customarily attracts should be focused on this particular subproblem.Controlled modificationHaving exposed (but by no means solved) the problem of working back from implementation to specification, we can now see how this fits into a scheme :for evolutionary system development that pushes against the tide of the second law of program evolution (see Chapter 4 for the details of this law). Chapter 4 was also the place where I introduced the notion of controlled modification.Controlled modification is the process of generating system modifications by tackling each specific problem at its source (i.e. in the specification, or in the detailed design, etc.) and then redesigning and reimple-menting the changes into the system code. In addition, the 'style' of the modifications introduced should be constrained by the principles of structured growth (see next subsection).Figure 6.4 illustrates the process of controlled modification. There are several points to note. First, you will notice that there are a couple of arrows shooting information in to support the abstraction process. On the one hand, we have the system developer's knowledge of what the software system is meant to achieve, in general, and what specific problem is behind the current need to effect a modification of the system. And, in addition, we would hope for the system to be able to assist the developer by making both original design knowledge and the results of previous abstraction sequences conveniently available.Secondly, the move from one version of the program to the next is accomplished, on most occasions, by a (possibly substantial) detour through various abstractions from the programmed system. What level of  Page 163Figure 6.4 "Controlled modification as part of the RUDE cycle"abstraction, and what type of abstraction are needed to support controlled modification are questions that can be answered only with respect to specific problems with the programmed system. It is probably quite clear that the level of abstraction needed is the level at which the problem at hand first appears, but the notion of what type of abstraction probably causes some wonderment in the software engineer who has been raised on a specific SDLC and thus sees the range of possible abstractions as very limited—i.e. there may be a requirements document, a specification, a general design and a detailed design perhaps. But, in reality, the options are much broader than this (and hence a major reason why we must practice stepwise abstraction even if we dutifully follow a structured design methodology). A sequence of abstractions may be available, but it may  Page 164Figure 6.5 “A structure-chart representation"  Page 165not be the best (or even a good) sequence to support a clear understanding of the system from the point of view demanded by a specific inadequacy in the system's behavior.For example, work on an AI system to model human learning behaviors led us to use two very different abstract representations of the programmed system. (Full results of this project can be found in Johnson, Partridge & Lopez, 1983.) In Figure 6.5 we have a conventional structure-chart representation of the overall system. It displays the basic modularity of the system design. The system modeled the behavior of an animal in a environment, and in particular the responses that the animal generated to each new environmental input stimulus. The actual response was a function of the current input stimulus, the animal's current location, and its previous history of interaction. A last pertinent point is that the animal's cognitive processing (i.e. how it related input stimulus to response) was implemented as a neural network—i.e. a large network of primitive processing elements.In Figure 6.6 we have an abstract representation of the stimulus-response processing mechanism that we found, through trial and error, to be the best abstract representation to support controlled modification. The AI in the system was concentrated in the network's processing of the input stimuli and the generation of a subsequent response. It was thus in the organization and detailed processing mechanisms of the network nodes (e.g. activity accumulation and transfer functions) that most exploratory system development was concentrated. Such networks are notoriously lacking in conceptual transparency because of the distributed nature of the operations that they perform. This is, of course, not good news when this is precisely the part of the system wherein we needed to develop a deep understanding of what was going on, and why.The loop structure that comprises Figure 6.6, together with an interface for program visualization which used a dynamic graphical display, was found to be the most effective aid for understanding the system behavior in terms of the network structures.What is the relationship between these two abstractions? They are not different level representations of much the same information, neither are they representations of different information. They are best considered, I believe, as different perspectives on a single system. They each emphasize different aspects and neglect others, and this is entirely appropriate as they were developed to support different sorts of understanding of the system. The structure chart is fine for communicating an overall picture of the system organization and for working out the general design in a  Page 166Fig 6.6 “A dynamic-loop representation"  Page 167quite conventional manner. And it would of course support the understanding of problems that arose concerning say, the local and global scopes of specific implementation structures.The 'loop diagram' in Figure 6.6 is focused on the cyclic stimulus-response behavior of the network nodes; it covers nodes 1.5.1.4 and 1.5 of the representation in Figure 6.5. It was used to support reasoning about system behavior with respect to changes in the network structure and processing mechanisms. Figure 6.5 is the conventional software skeleton, and Figure 6.6 focuses attention onto a concentration of AI muscle. Multiple representations of the system should assist the developer in his or her efforts both to gain a fuller understanding of the system and to modify the current version of the system in harmony with the principles of structure upon which it is constructed (see 'structured growth' later).The construction and maintenance of the design history of a software system would be an onerous task (conventional one-time system documentation is far less demanding but often not done precisely because it is time-consuming and unappealing to the technologist). So, as we saw in the context of design records (discussed above), the prevailing view is that much of this burdensome task must be automatic, or else it will just never get done at all.What I shall suggest here is that certain abstractions can be automatically generated provided the programmer observes certain rules in his or her programming activity.Modem programming languages, in supporting both data structure abstraction and control abstraction bring this automatic-abstraction goal nearer. ADA (Barnes, 1983), for example, offers extensive syntactic and semantic support for the rigorous separation of the abstractions, which constitute the algorithm and abstract data structures, from the necessary (but cluttersome) implementation details. Careful system development in accordance with a set of principles and guidelines for the distribution of coded structures would allow the application of 'abstraction algorithms' which can, on a purely syntactic basis, generate the algorithmic and data structure abstractions embodied in the working system code.To repeat an earlier example (Partridge, 1986, pp. 190-192): A 'package' in ADA contains an explicit syntactic demarcation of specification and implementation. In addition, this syntactic differentiation is supported by a semantic one—i.e. other system modules are prevented from accessing what the programmer has decided are implementation details. Such a semantically-supported syntactic distinction provides the basis for an abstraction algorithm which can generate 'specifications' from the executable code.  Page 168The ADA package is an information-hiding mechanism; it has two parts: a public or specification part, and a private or implementation part. The specification part constitutes the abstract specification of the package: it specifies all that a user can know about, and hence use in, the package. The implementation part contains (as you might guess) an implementation of the abstract specification; it is not accessible to the package user.The programmer can thus, in effect, delineate all code as either specification or implementation. But as you should recall from our earlier discussions, these terms are not absolutes. They are relative terms (quite apart from the fact that there are many different ways to represent a given level, say, of specification). So, in practice, the situation is likely to be more complicated. A series of levels of abstract representation demands a more sophisticated decision procedure (and abstraction algorithm) than the binary choice required for two absolute categories.Notice that the detailed distribution procedure employed will be determined by both the abstraction algorithm available and the type of abstract representation required by the system developer. A more satisfactory situation would be to have a general code distribution strategy and a selection of different abstraction algorithms for subsequent use—but this introduces severe complications which I do not want to address. The major point is that the variety of potentially useful abstractions demands some flexibility somewhere within this overall approach to automatic abstraction. It is, for example, not obvious that the best structure to abstract for supporting controlled modification of the AI muscles in the system is also the best abstract representation for supporting conventional software maintenance.On the following page, the rather crude knowledge base is coded as an ADA package. As constructed it comprises a collection of rules, and mechanisms for adding, accessing, and removing rules. The semantics of the ADA language ensure that use of this knowledge base is limited to the three mechanisms listed as procedure and function specifications at the beginning of the package. All code from the reserved word PRIVATE to the end of tile package is inaccessible to users of the package.Clearly, construction of an algorithm that abstracts the package specification is a viable proposition. With this code, and some minor linguistic transformations (in deference to that subset of humanity that has been wise enough to avoid becoming fluent in such inhuman languages) we might automatically obtain the following abstract representation:  Page 169The knowledge base is composed of rules. A rule is composed of a condition part and an action part. The following operations on the knowledge base are possible:1. rules can be added to the knowledge base;2. the next rule can be obtained;3. rules can be deleted.PACKAGE knowledge-base ISTYPE rule IS PRIVATE;PROCEDURE put rule (condition, action: string);FUNCTION next rule RETURN;PROCEDURE remove-rule (item: rule);PRIVATETYPE rule ISRECORDcondition: string;action: string;END RECORD;END knowledge-base;PACKAGE BODY knowledge-base ISTYPE ruleset IS array (1.. max) OF rule;knowledge: ruleset;nextspace: integer:= 1;...PROCEDURE put rule (condition, action: string) IS BEGINknowledge(nextspace):=(condition, action);nextspace:=nextspace + 1;END;...END knowledge-base;  Page 170In closing this ADA example, let us note that such a strategy of structure building in concert with a specific abstraction algorithm is, of course, not limited only to the coded representation of the system. It could be applied at any level in system design; it is just that with comprehensive, well-defined constraints on what is acceptable as, say, ADA code, this would appear to be the easiest level of application of automatic abstraction technology.This sort of automatic abstraction facility is implemented within the support environment of the commercially available object-oriented language, Eiffel (see Meyer, 1989, and described briefly in Chapter 9). A documentation tool, called short, can produce a summary version of a class showing the external interface, i.e. the exported features only and, in the case of routines, only the header and the entry and exit conditions, if stated. Meyer (1989, p. 13) tells us that ''the manual for the Basic Eiffel Library is an example of Eiffel documentation produced almost entirely from output generated by short." He also points out that such documentation is obtained almost for free, and it is guaranteed to be consistent with the actual software as it is extracted from it.The case for alternative (loosely equivalent) representations in software production has actually be made much earlier in this book. In Chapter 1 we viewed several alternative software design strategies (e.g. structured and object-oriented) which were presented as genuine alternatives whose relative power depended critically on the nature of the problem being tackled. In the current context we might say that the conceptual transparency (a major component of the 'power' of a representation) of a representational scheme is dependent on the nature of the problem at hand, both the intrinsic nature of the problem itself and the nature of the task that we are addressing with respect to the particular problem.In Figures 6.5 and 6.6 we have seen examples of different alternative representations for a given computational problem. Each representation best supports a different task with respect to the particular problem—Figure 6.5 provides a traditional view of system organization and modular structure which is a useful initial perspective for general reorganization or as a precursor to focusing on specific modules, Figure 6.6 was found to be a much more powerful representation for reasoning about the input-process-output cycle, the central functionality of the system with respect to the neural network that the system contained.We can now return to the object-oriented paradigm in order to see the power of this representational scheme (it is, of course, much more than this but for the moment we'll take this limited view of OOP). In his positive exposition of both OOA and OOD, Yourdon (1990) considers the  Page 171question of what systems and applications will most effectively use the object-oriented paradigm. He notes: "A software manager at IBM recently commented to me that attempts to use standard structured techniques to build applications in an OS/2 Extended Edition Presentation Manager (PM) environment had been dismal failures; the only successes they could point to were applications developed with object-oriented techniques" p. 260. This sort of application is a typical graphical user interface and is characterized by pull-down menus, multiple windows, icons, and mouse-driven commands. In such an application (as Yourdon says) OOD excels and the traditional structured approaches typically fail.As a direct comparison with the representations in Figures 6.5 and 6.6, Figure 6.7 is an object diagram of the same system, i.e. an animal controlled by a neural network structure which reacts to, and acts upon an environment.Figure 6.7 Animal-environment system object diagram  Page 172Objects are represented by an irregularly shaped, closed contour; relationships between objects, which means that they can send messages to one another, are illustrated with connecting lines; the specific messages that can be passed are named and associated with an arrow that points to the object being acted upon (the particular arrow icon used is taken from Booch, 1991, and denotes that all the messages in this system are synchronized); and the numeric labels associated with each message name specify the simple sequential synchronization required in this system.Clearly, a whole new notation is needed in order to represent more ambitious systems, as indeed it is always needed for any new representational strategy whether it is introducing a radically new paradigm or just a new diagramming scheme within an old one. Within the object-oriented paradigm, and given its focus on message passing between objects, extensive notational impedimenta is developed for specifying and illustrating the time-ordering of events—i.e. simple sequential synchronization (as in Figure 6.7), asynchronous message passing, or more elaborate schemes such as conditional flow of control. A complete representation will be composed of several diagrams each displaying different aspects of the system. A timing diagram, for example, may be employed to show the dynamic interactions among various objects in an object diagram which is used to show the existence of objects and their relationships in the logical design of the system.As this is not meant to be a primer on the object-oriented approach to software design, I'll go no further into notational detail. The points to conclude with here are simple: representation is important, very important, when we are trying to gain a conceptual grasp of non-trivial software, and there is no one best representation. Different representations best support different software-related activities. There may be just one specification and one design sequence for a given piece of software, but there is a variety of alternative representations that are likely to prove invaluable with respect to the software product. The software maintainer, developer or whatever should not be blinkered too much by the actual precursor representations for they will not necessarily best serve his or her current purposes.Structured growthHaving, I hope, convinced you that there is much more to be abstracted from a program than the original design structures and specification, one further problem area needs to be highlighted. Assuming that we can effec-  Page 173tively decompile observed behavioral inadequacies into an understanding of what is wrong with the structure of the system, how do we set about modifying the structure in such a way that overall system clarity is maintained (or even enhanced) and system complexity is kept at a minimum—which may be two ways of saying much the same thing?Conventional software engineering, spurred on by the software crisis, has, you might think, already solved this problem. Structured programming is the common name for the motley collection of rules and looser guidelines that the responsible system developer uses in order to maximize the clarity of his or her programmed artefacts. So why not adopt, wholesale, this farrago of strictures and our problem is solved? But it is not.Structured programming, as you ought to expect from a notion that was developed in the context of linear specify-design-implement SDLCs, was developed to be used for a one-shot, complete implementation of a detailed design and specification. This is rather different from the need to implement selected elements of a design in the context of a fairly comprehensive, existing implementation.So, although it would be foolish to thus toss the wisdom of several decades out of the window, it would be equally foolish to expect it to solve the current problem. What we now face, as Sandewall (1978, but probably more accessible in Barstow, Shrobe & Sandewall, 1984) first termed it, is structured growth—although I am using it in not exactly the same way as he first introduced it, I admit.Sandewall characterizes structured growth as follows: "an initial program with a pure and simple structure is written, tested, and then allowed to grow by increasing the ambition of its modules... the growth can occur both "horizontally", through the addition of more facilities, and "vertically'' through a deepening of existing facilities and making them more powerful in some sense" (p. 63, in Barstow, Shrobe and Sandewall, 1984).The major difference between the two usages of this term is what drives the modification: I write of behavioral inadequacies, and he writes of the program developers desire to enhance the system. It may be that these two driving forces are often not very different. Anyway, the point of structured growth is not why you want to modify the system, but how are you going to do it as a minimal structure-destroying act—to take the most negative viewpoint.One of the key ideas behind the effective use of techniques of structured growth is that you are only introducing modifications to a stable, well-structured and well-tested basic system (the conventional software  Page 174skeleton described in Chapter 4). If indeed it tums out that a major reorganization of the system framework is called for then we are out of the realm of structured growth and into more conventional structured programming—i.e. redesigning the system from first principles.But given that structured growth is not taken to include major upheavals of the system, then Sandewall has several support-system features to suggest:1. Keep track of "open statements" or "expansion points". When users type in a piece of code which they know has to be refined or rewritten later (e.g. a crude first heuristic, or a marker for where a heuristic will eventually be needed), it should be possible to tell this to the system and ask later where the (perhaps local) expansion points are. As a specific example, see the "deferred" structures in the Eiffel language (Meyer, 1989); a detailed example is given in Chapter 9.2. "Distinguish different types of edit operations. Given an initial approximation to a program, some changes will be made to correct bugs, and some to extend the program. It would be desirable to treat these two kinds of changes separately, so that several layers of refinement or growth could be maintained in parallel. It is a common observation that early versions of a program are useful introductions to later full-grown versions. Similarly, the value of (idealized) early-refinement steps as documentation has been emphasized in Berry(1975)." (Sandewall in Barstow, Shrobe and Sandewall, 1984, p. 64)The first of Sandewall's suggestions is a structured-growth specific enhancement of the widely accepted bookkeeping (or supersecretary) role that sophisticated software development support systems should exhibit. The second feature should strike a chord with the earlier discussion on sequences of abstractions and of storing design knowledge. Clearly, the ready availability of abstract representations of earlier (and by implication more readily comprehensible as they are uncluttered with subsequent refinements) versions of the system can be merged with the general goal of maintaining on-line useful representations of the design history of the project. And, although recognizing the need for such functionality in software support environments is one thing, whereas delivering this functionality is altogether another (but see Trenouth's "version trees" in Chapter 9), such recognition is required before there is any hope of serious attention being given to the problems it involves.  Page 175In sum: in this chapter I have provided indications (little more than that I believe) of what might be the major component problems to be solved along the path to a discipline of ESD. None of them are easy problems, but none of them appear to be impossible either. And, as I've also indicated, many of them are now being addressed seriously by competent groups of software researchers. Much of what I have touched on in this chapter is still not widely accepted as necessary in mainstream software engineering—other problems such as automatic verification tools, for example, are thought to be more pressing problems—but there is sufficient activity outside of this mainstream for considerable energy and expertise to be focused on these Cinderella issues.  Page 177CHAPTER 7Machine Learning: Much Promise, Many ProblemsIt might strike you as a bit odd to see this chapter on machine learning pop-up in a book concerned with the practical application of AI. Well, your first impression would be appropriate if this chapter reviewed the theories and mechanisms that are generally taken to constitute the Al subfield of machine learning, but it will not. My goal in this chapter is to convince you that machine learning mechanisms will be a necessary part of many Al systems, and that this necessity brings with it additional aggravation for those of us that wish to engineer AI software. I do this not because I think that engineering artificially-intelligent software is a well understood and boring procedure which is in need of some injection of excitement—far from it. I do this because the potential power of machine learning mechanisms is something that we should be well aware of even though it introduces further problems on top of those that we already face. It may be wise, if not essential, to have the full picture in mind even though work on just one part of it gives us quite enough problems to be going on with.Self-adaptive softwareMention of the possibility of self-adaptive software should, for all the old-timers in the software world, conjure up the spectre of self-modifying  Page 178code. A favored strategy, in the days when programming was largely the pastime of grappling with a mass (or better, morass) of machine code instructions in order to save a few bytes of memory or to shave a few milliseconds off the running time of a program, was to devise ingenious ways of reusing memory locations and blocks of instructions by overwriting certain critical instructions with new ones as the program executed. Thus the code that actually constitutes the program will vary as the program runs through an execution sequence. And, moreover, the details of how it actually varies are dependent upon the details of a given execution sequence.Debugging is not one of the favored pursuits of most software engineers. They are fundamentally creative people, system designers and builders. Looking for errors in such creations (either one's own or worse someone else's) is an exercise that few can gather great enthusiasm to pursue. Add to this inherent lack of appeal the further negative incentive of machine code, especially the early ones with (mis)mnemonic labels and glorious perceptual opacity, and it is not surprising that the queues of qualified applicants to maintain software systems have never been long. And on top of all this, in the presence of self-modifying code, the maintenance person could not even while away the hours staring at the listing hoping for inspiration in the sure knowledge that all the relevant information is in front of him, or her. For the listing, in effect, changes as the program executes. In this thoroughly dismal scenario the only serious options are to abandon the task or to collect strategically chosen snapshots of the program code and data. Octal core dumps were one favored representation of this information (favored by the machines that is, and in those days it was the machines that were the scarce and expensive resource). The advent of high-level languages (among other things) has done much to improve this bleak picture, but another significant improvement was introduced early on; it was a programming principle to abide by if you wanted to stand a good chance of debugging your creative efforts. The principle was:Never write self-modifying code.So to persons well aware of this principle and the very good reasons for its existence, a call for self-adaptive software systems is likely to be greeted with much the same enthusiasm as ham sandwiches at a Jewish wedding. I hope to convince you that such a response would be uncalled for. But first, why do we need to contemplate the undoubted extra problems that self-adaptive software systems will bring with them?  Page 179The promise of increased software powerWay back in Chapter 1 I presented four aspects of how we might approach the problem of increasing software power—and each aspect involved AI. As it happens, not only does each of these four aspects involve AI, but they also each imply a need for machine learning in their more impressive manifestations—i.e. a need for self-adaptive software.The central notion here is that many significant enhancements of software power require a move from static, context-free systems to dynamic context-sensitive systems. Intelligence is not a context-free phenomenon, and AI can not be either. "Machine learning," as Roger Schank has succinctly put it, "is the quintessential AI issue." This does not mean that all AI software must involve mechanisms for machine learning. But it does mean that our options in the absence of robust and reliable mechanisms for machine learning will be severely limited.The need for self-adaptive software derives from several sources: there is a need for software that is reactive to changing circumstances, and at the more mundane level there is a need for mechanisms to lessen the difficulty of the task of incrementally upgrading knowledge bases (as argued by Michalski, Carbonell and Mitchell, 1983, in their volume entitled Machine Learning).The threat of increased software problemsGiven that there are some not unreasonable reasons why we might want self-adaptive software systems, what does this new need entail? In a word: problems.To begin with, there is the old problem of the state of a system changing over time—i.e, the program that you are asked to debug or enhance is defined by the specification plus its usage history. Now, if the old-timers were smart enough to realize that this type of software system is primarily a short cut to disaster, do we need to re-embark on this learning experience?I think not; we must clearly not rush in, but times have changed, and, I shall argue, they have changed sufficiently for the possibility of self-adaptive software to be a viable proposition, provided we operate with caution. It is the differences more than the similarities between old-style machine-code programming and modem software system design that are most obvious—the two processes bear little resemblance except at some very basic level. So what are the modem innovations that suggest to me  Page 180that self-adaptive software need not carry with it the curse of self-modi-fying code?• Systems are designed and developed more systematically using appropriate abstract representations which reduce the effective complexity by several orders of magnitude.• With the development of elaborate machine-manipulable data structures program complexity can be traded out of the algorithm and into the data structures; as a result substantial software adaptivity can be gained by merely changing data values and keeping the actual algorithm unchanged.• Software can now be developed within a sophisticated support environment, which can remove a myriad of trivial considerations from the concern of the programmer, and this again significantly reduces the effective complexity of the overall task.• Principles of structured system design capture the hard-won wisdom of the intervening years, and when adhered to make a well-structured system much more conceptually manageable than the equivalent monolithic list of machine code instructions.• Within the comprehensive, multi-levelled and highly-structured framework implied by the first three points, we can constrain and limit the scope of any self-adaptive mechanisms employed.This last point is particularly important. I am not saying that ill-consid-ered and over-ambitious machine learning mechanisms are now fair game for the zealous software engineer. I am saying that within the constraints of a well-engineered software system there is scope for the careful deployment of self-adaptive mechanisms that will enhance the power of the software without necessarily wrecking its maintainability, and I shall provide you a couple of examples in the final section of this chapter.In particular, some types of machine learning are more inherently controllable than others, and judicious encapsulation of these mechanisms within a software system will keep the overall system intellectually manageable—especially if appropriate software management tools are developed and added to the support environment at the same time as the particular mechanisms of machine learning.Machine learning does not have to be self-modifying code, and it should not be—at least, not in the undisciplined way that was customary in the bad old days. So what are the options for mechanisms to implement self-adaptive software?  Page 181The state of the art in machine learningMachine learning (ML), the umbrella term for most mechanisms of self-adaptation in computer programs, is very broad with surprising depths here and there. I do not propose to subject you to a comprehensive survey, for apart from being inordinately long, it would for the most part address strategies that show little promise of being viable practical mechanisms in the near future. So I shall be highly selective, and for those readers who doubt the appropriateness of my selection (or would just like to see the big picture) I can indicate a number of comprehensive sources of information. Most general AI books are not very good on this topic, perhaps because it has blossomed so much in recent years and they have yet to catch up. But putting modesty aside, you'll find no more comprehensive coverage than the 100-page chapter on ML in Partridge (1991) A New Guide to AI. Up-to-date details on individual projects can be found in the periodic edited collections entitled Machine Learning, published by Morgan Kaufmann (vols I, II and III published), and in the journal Machine Learning, published by Kluwer.The ML mechanisms with some promise of near-term practical utility can be divided into the classical ones—such as inductive generalization—and the network learning models (the connectionistic ones—to use the common but universally disliked general label)—such as back propagation of an error signal.In both categories, mechanisms are designed to exploit the accumulated experience of the system within a given operating environment; the major differences are the degree to which experience can be automatically exploited, and the nature of the system modifications employed in this exploitation process. Let's take the connectionistic (or parallel distributed processes—hence PDP) first.A PDP system is typically a network of primitive processing elements that receive 'activity' values from other elements that are directly connected into them, accumulate the received activity values according to some specified function, and then pass on through further links to other elements some function of the activity which they have accumulated. A further, and crucial feature, of these models is that the activity transfer operations occur in parallel. The obvious analogy for those readers who are grappling with this description is with the brain as a network of neurons each receiving and passing on electrical pulses. Finally, each of the links between network elements has an associated 'weight' which is usually involved in the activity-transfer function through the link. So by changing these link weights activity flow paths can be effectively opened up and closed down. Hence link-weight adjustment is the typical learn-  Page 182ing mechanism, which is, of course, totally different from learning in classical models (as you will soon see if this is not immediately obvious to you). But this sort of information can be classed as merely implementation detail (a widely misused phrase but I think that I'm on relatively safe ground this time) and thus not of prime importance to the software designer; it is the general learning strategies that are of major concern.Unfortunately, the general strategy for ML in PDP systems is one of error-signal feedback (and individual mechanisms dictate how to adjust link weights to reduce the observed error). This means that the capacity for self-adaptation is restricted to the training of given networks to exhibit some well-defined functionality, and not the process of learning to respond appropriately to specific, but changing, environmental idiosyncrasies. PDP models can be trained (and with formal guarantees of convergence, although the training itself may be a very lengthy process) to exhibit some 'correct' behavior, but this is not really the type of self-adap-tation that we are looking for, although it undoubtedly has some useful practical applications (e.g. in pattern recognition applications, see WIS-ARD, Aleksander, 1983). We'll return briefly to this topic at the end of the chapter in order to view the tantalizing possibility of 'black-box' software.So PDP models, although currently generating much excitement and hope, fall a long way short of offering us the mechanisms of self-adaption that we might use to build powerful Al systems of the type discussed earlier. This leaves us with only the classical models, but there are some bright prospects here.ML in the classical mould (i.e. sequential reasoning in a 'space' of alternative and successive possibilities) has had some three decades to proliferate and promulgate the resultant diversity of alternative strategies. There is thus a wide selection of mechanisms to consider: learning from analogy, rote learning, advice taking, learning from examples, explana-tion-based learning, apprentice learning, etc. But while there is a wealth of research projects emanating from this AI subfield, very few of the current mechanisms can boast the robustness and reliability that practical software systems demand. Hence the set of actual mechanisms to consider is quite small; it is basically inductive learning schemes (i.e. the automatic generation of new information by abstracting generalities from a set of instances of some phenomenon). In addition, I shall mention an up and coming contender from the area of deductive learning schemes—explanation-based learning (EBL)—which shows some promise of practical utility and has a great advantage in its deductive nature. For induction suffers from the inherent weakness than it cannot be guaranteed  Page 183correct. Except in totally artificial domains, induction, working as it does from the particular to the general, is always liable to generate errors. No matter how many white swans you see, you will never be sure an inductive generalization that all swans are white is correct. Induction is a powerful mechanism (and we humans exploit it to the full) but it is one that comes with no guarantees. Nevertheless, it is the basic mechanism that has proved useful in practical software systems.The general strategy for self-adaptation based on inductive generalization is for a system to amass a collection of instances of, say, a specific user's interaction with the system, and then to generate from this set of instances a general model of the preferred style of interaction of this user for use when he or she next appears. To the user of such a system, it should appear that the system has tailored its style of interaction to his or her personal idiosyncrasies. And furthermore, if the system continued to collect instances of the interactions with this user, subsequent inductive generalizations could be used to either fine tune this specific example of human-computer interaction or to track the changing habits of this user. Such a software system would clearly be well on the road to the greater software power that we discussed in the opening chapter of this book.An even better mechanism would take each new instance of interaction with this user and examine it to see if it implied any change in the general strategy of interaction being used for this user. The former, what we might call 'batch-type' inductive generalization (called non-incre-mental generalization in ML), is mechanistically easier but behaviorally less satisfactory, and the latter is incremental inductive generalization, which can adjust the generalized information after each new instance of interaction—i.e, it can offer just the functionality that we require.What represents an instance, and what are we aiming for with a generalization? Well, an instance is determined by the specific problem being addressed. Thus, in my example of a system customizing its user interface to accommodate a specific user's idiosyncrasies, an instance (or a training example in EBL terminology) is a particular example of an interaction with this user. But, although this may clarify things a little, it does lead us on to a number of salient subsidiary problems: What specific aspects of an actual interaction sequence constitute an instance to learn from? What does the system need to learn from the specific instances? What are the goals of this self-adaptivity? Much of the ML literature seems to assume that the world is neatly divided into 'instances', and moreover that these instances are not only clearly demarcated but are also conveniently labeled to tell us what general behavior they are instances of—reality, even the limited reality of software systems, is just not  Page 184structured in this handy way. Answers to the above questions can then be viewed as imposing the necessary structure and an unstructured reality.The first question raises considerations of what to abstract from the individual interactive sessions, i.e. what sort of abstract representation will the inductive generalization algorithm be operating on? Surely, we will not want to record minor typing errors that the user makes? Or will we? In trying to answer this question we are led on to a consideration of the others. If one of the goals of the overall self-adaptivity of the interface is to adapt to user deficiencies, then we might well want to record all of the users slips and mistakes in the hope that the system is able to produce a general response strategy to this particular user that is tolerant of such minor flaws in communication. There are many (very many) different structures that can be abstracted from any given session of actual behavior, and so one of the major initial decisions is which aspects of an interactive exchange will be recorded and taken to comprise an instance of behavior for the inductive generalization process.Even earlier issues to be addressed are the goals of the self-adaptivi-ty within the software system. It's all very well to decide that a user-responsive interface to a piece of software is desirable, but then there are many consequent issues of the exact form that responsiveness is to take. And the final decisions here will rest on notions of exactly how and to what purpose the software system is to be used: the self-adaptivity might be seen as a means to train humans to use the underlying system as efficiently and as effectively as possible, or it might be viewed as a tolerant interface that will accept (whenever possible) the efforts of casual and sloppy (by the pedantic standards of most software systems) users and deal with them as best it can. So there are clearly many tricky decisions to be made even when it has been decided that self-adaptivity would be beneficial and that inductive generalization is the route to take.The other major question broached earlier, but as yet unaddressed, is what form of general information can we hope that our inductive generalization mechanism will produce? Currently, there are just two answers in systems that have found practical application: decision trees and IF-THEN-type rules. And this focusing down takes us on to the current limitations of inductive generalization algorithms as mechanisms for practical self-adaptive software.My example of automatically customizing a user interface to specific users is beyond the current state of the art in machine learning: it is unclear how best to 'model' individual users (i.e. we probably need a representation structure that is more complex than either decision trees or IF-THEN rules), and there is a similar mystery about what 'dimensions' of  Page 185the human-computer interaction to model and how they might themselves interact. But this sort of self-adaptive mechanism can be, and has been, successfully applied on less ambitious tasks. I can characterize the tractable tasks as declarative, data-representation tasks rather than procedural, process-representation tasks (as a user's interaction would be most naturally represented).So, mechanisms of inductive generalization are viable for practical software systems when the problem is one than can be realized (at a useful level) as a fixed processing algorithm operating on a dynamic data structure such as a decision tree or set of, say, production rules. It is conceivable that effective dynamic user models can be implemented in this fashion but so far we have failed to find the right combination of processing algorithm and data structure—only time will tell, perhaps.The practical applications of this technology have so far concentrated on expert decision-making processes; this is clearly a procedural task but one that can sometimes be implemented effectively in terms of a fixed processing algorithm (say, a mechanism of logical inference) and a dynamic data structure (say, a set of IF-THEN rules). The self-adaptivity can then be realized by inducing new IF-THEN rules as a result of processing specific instances of the decision-making behavior.One pioneer of this sort of mechanism is Michie (see Michie, 1982, for example) who was motivated by the difficulty of extracting general rules from human experts when building expert systems. Michie observed that human experts are often quite poor at articulating, in general terms, how they do whatever it is that they are expert at doing. But, almost by definition, they are good at providing specific instances of their decision-making expertise. So why not collect a set of such instances and then inductively generate a representation of the general decision-making expertise that is expected to be implicit in the set of instances? And indeed Michie constructed a system to do this, and even produced a commercial software version called ''Expertease" for use on PCs.The core of Michie's system was an algorithm for inductive generalization developed some years earlier by Quinlan (1979) and called ID3. According to Cendrowska (1989) the limitations on the use of this algorithm derive from three requirements that the training set must meet:• The instances of a given decision are described in terms of values for a fixed number of attributes. "The set of attributes must be adequate... The set of attributes is inadequate if there are two or more correct instances which have the same values for the attributes but are of different classes" or are from different decisions.  Page 186• The general classes or decisions must be specifiable in terms of the attribute descriptions. This seems obvious, but consider the problems of classifying system users and how to characterize, in terms of a set of attributes, examples of system usage in a way that is sufficient for the desired results.• The classes used must be mutually exclusive—i.e, each example that the system encounters must be classifiable in only one class.Additionally, there is the problem of the completeness. A complete training set contains all possible instances of the general class. Clearly even the possibility of completeness only comes after we have made a lot of decisions about the attribute set and the set of discrete values that are allowable for each attribute. Thus our user model may use, say, average response time as a characteristic attribute but we must further constrain the model by only allowing a discrete set of values for this attribute, e.g. <<0.5 seconds, 0.5 to 0.75 seconds, and >0.75 seconds.Once we have set up such a framework then we can entertain the notion of completeness. But if we have a complete training set then our induction algorithm would be no more than a data compression device—i.e. it would reduce a large set of instances to examples of just a few general categories. And although this information compression may be very handy on occasion, it is not the sort of functionality that we are looking towards induction to support. We want the system to generate the general classes with an incomplete set of examples. So how good are induction algorithms; at this exercise?Cendrowska (1990), who has developed PRISM (a system for inductively generating expert system rules) as an alternative to ID3, presents empirical data on the predictive power of these two induction schemes. She used data from two different problems:1. The classification of board configurations in the King-Knight-King-Rook chess end-game (a problem that ID3 was extensively exercised on): the problem is to find a rule set which will determine for each configuration of the four pieces, whether Knight's side is lost in two-ply in a black-to-move situation.2. The contact-lens decision procedure: the problem is to find a rule set that will decide, on the basis of a number of characteristics of a person, whether that person is suitable for contact lens wear, and if so, which type of lens they should be fitted with.  Page 187In order to determine the relationship between size of a training set and the predictive power of a rule set induced from it, a fixed number of instances were selected at random from the complete data set... a set of rules was induced from these instances using PRISM [or ID3], and then the induced rules were tested on the full set of instances... This was repeated one hundred times each for training sets containing 5%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80% and 90% of the complete data, and the results averaged for each size of training set. (p. 135)Figure 7.1 illustrates the predictive power of both PRISM and ID3 when applied to the contact lens decision problem. With respect to correct classifications, there is little to choose between the two systems: both start at about 75% correct classification with only 5% of the training set to generalize from, and both quickly improve to about 90% correct when the induction is based on 40% of the training set. But with respect to incorrect classifications PRISM looks to be superior to ID3. Figure 7.1 "The predictive power of induction algorithms" from Cendrowska, 1990, Figure 9.2, p. 1381926-0187a.jpg  Page 188The results with the chess end-game problem are very similar, except that both systems start at nearly 90% correct (with only 5% of the training set) and attain almost 99% correct classifications when the induction is based on 40% of the training set.But for our current purposes, we need only note that these schemes do seem to exhibit the required predictive functionality, if only we can constrain our problems to fit into the training set requirements without destroying the essence of the problem. And, in this regard, we must be aware of yet another constraint that I have so far failed to mention: the training examples must be free of 'noise'—i.e. there should be no errors in the examples presented to the system. This requirement is, of course, impossible to meet with real-world data.Quinlan has tackled several of these important problems that separate ideal training sets from the real-world phenomena of interest; in particular, he has explored the problems of 'noise' (e.g. Quinlan, 1986), and of the need to manage the uncertainties that the real-world always brings with it (e.g. Quinlan, 1987, but described later and illustrated in Figure 7.4).A final mechanism to bring to your attention with regard to the state of the art in classical machine learning and the possibilities for practical application, is that of explanation-based learning (EBL). This is a deductive scheme that uses a single training example in conjunction with domain knowledge to generate an operational version of an already known goal concept. In short, EBL attempts to transform a non-useful generalization into a useful (i.e. operational, in the jargon of EBL) one when presented with one specific example of this general behavior and sufficient knowledge to be able to link the example and the general concept—actually, in classical EBL, it has to be able to prove that the specific example is indeed a specific example of the goal concept. It is this proof that is viewed as an explanation for why the supposed example is indeed an example of the goal concept, hence the name 'explanation-based'.EBL is an exciting new idea, but it is, of course, not without its problems as well as its promise. The topic is both too involved and too contentious for us to go into the issues and do justice to them in this book. The interested reader should ferret out and consult either the two 'original' papers, Mitchell, Keller and Kedar-Cabelli (1986), and De Jong and Mooney (1986), or read Chapter 9 in Partridge (1991).Even my sparse introduction to EBL should give rise to some knowledgeable puzzlement even for the reader who is totally au fait with the differences between inductive and deductive inferences. The former we  Page 189have been discussing at length; inductive inference mechanisms suffer from a lack of guarantees but hey do permit a system to 'go beyond' the known information which is, of course, one of the major features that we require of our intelligent software. Deductive inferences, on the other hand, are as solid as logic itself—they come with a lifetime warranty, as it were—but the cost is that they add no new information to the system. So where's the gain? The benefits to be derived from a mechanism of deductive inference are several-fold. An explicit and practically useful statement is a very different thing to an implicit and non-useful (i.e. non-operational in EBL jargon) representation of exactly the same information (if it were not then the science of mathematics would be eviscerated). In addition, appropriate rearrangements of the available information can lead to better and faster solutions to problems. So there can be a lot of potential in a purely deductive mechanism but it will never have the unbounded promise of an inductive leap, but neither will it have capacity for blunder.One particularly relevant possibility for EBL, and one that will connect us to the subsequent chapter, is its potential application in expert systems technology. Expert systems (as we shall explore in the next chapter) is a subfield of computer technology, and one that is a beacon of light for those struggling with the possibilities for AI in practical software; it is also provides a point of focus that brings a few of the nebulous problems into sharp focus. One specific problem is that of the brittleness of AI systems: they perform excellently on some occasions and fail dismally on others, with no obvious indication why—i.e, seemingly very similar situations can elicit both modes of behavior. Part of the essence of good practical software is that is does not exhibit this sort of brittleness. It should be robust and reliable.In the context of expert systems this unsatisfactory bimodal behavior—i.e, expert quality performance or total failure—can be viewed as a result of the fact that current expert systems are founded on purely superficial, syntactic rules. For example, a car-engine fault diagnosis system might have the rule:IF lights are dim AND engine will not startTHEN the battery is flatUsing such a rule (in conjunction with others, of course) the expert system will be able to correctly diagnose the 'flat battery' fault on many appropriate occasions. This rule codifies a useful heuristic, and heuristics by definition will not always work. This rule can also be viewed as the  Page 190codification of superficial, or syntactic knowledge. It is superficial because it contains no information as to why this heuristic actually works on many occasions. The rule allows the system to conclude the truth of 'the battery is flat' whenever it can determine the truth of 'lights are dim' and 'engine will not start'. This is a blind pattern-matching operation, and it works much of the time simply because it encodes quite a good heuristic. But when the battery has been stolen, this system will still conclude that it is flat—it doesn't know when the rule is appropriate and when it's invalidated by special circumstances (like the absence of a battery). In the jargon of expert systems: they are brittle because they have no deep knowledge—i.e, no knowledge of why certain heuristics work most of the time and hence in what circumstances they are inappropriate. There is thus currently much interest in building deep knowledge into expert systems, and perhaps automatically deriving the shallow heuristic rules from it (which would also address the problem that these shallow heuristic rules are surprisingly difficult to extract from the human experts).Now, you can appreciate (I hope) the potential for representational change that EBL offers us. All the information necessary to make expert decisions is (presumably) present in a comprehensive deep-knowledge representation of the problem domain—e.g, for car engine diagnosis the deep knowledge would codify information that the battery supplies charge that illuminates the lights and turns the engine (via other components) when the ignition key is turned. But human expertise is typically both quick and slick: human experts give high-quality responses most of the time by focusing on just a few of the most likely possibilities virtually immediately. It seems unlikely that this sort of behavior can come from an exhaustive evaluation of all possibilities—i.e, it seems that human experts use good heuristics most of the time. The point being that they can, and do, go beyond (or beneath) these superficial rules when necessary. To finally get to this point: the superficial rules can be considered as an operational version of the deep knowledge. So we may be able to apply EBL techniques to derive the superficial, heuristic rules from a representation of deep knowledge. Worden (1989) provides an instructive pictorial representation of this alluring possibility (see Figure 7.2). But there are still many outstanding problems: What is deep knowledge? How do we represent it? How do we deal with the fact that we can't always prove that a training example is indeed an example? and so on.And to close this section we can take a quick look at a specific example from a rather different branch of machine learning that holds promise  Page 191Figure 7.2 "A possible use of EBL in expert systems technology", from Worden, 1989, Figure 1, p. 1461926-0191a.jpgfor the engineering of practical AI software. I refer once more to the blossoming AI subfield of connectionism, or parallel distributed processing (PDP),or neural networks. Work in this AI subarea is characterized by the use of networks of primitive processing elements that receive, accumulate and pass on 'activity' values and the activity-transfers occur in parallel. There are many radically different species of connectionistic models (again Partridge, 1990, reviews them, and Hinton, 1989, provides  Page 192a comprehensive technical overview of the range of learning mechanisms being explored).From our current perspective there are a couple of important points to make:1. Some species of network are trainable with formal guarantees that the trained system will converge to some desired functionality.2. The internal structure of these models, and their mode of operation, makes them conceptually opaque and hence not readily amenable to explicit modification by a human software engineer.One example will serve to give you a flavor of this new brand of AI. A system called NETtalk (Sejnowski and Rosenberg, 1986) was built and trained to pronounce English. They used a three-layer network as illustrated in Figure 7.3.Figure 7.3 The network for NETtalk1926-0192a.jpg  Page 193Each of the seven input units was able to code a single letter, or a space, or one of a few punctuation marks. Thus a text sequence of seven elements (usually letters and spaces) can be presented to this network as an input. The coding of the input results in activity values being sent to each of the 80 units in the "hidden layer"; the amount of activity sent is controlled by the "weights" associated with each link. These hidden units accumulate the incoming activity values and then pass a weight-depen-dent amount on to the output units. Each output unit represents a pronunciation feature, and thus by thresholding the activity they all get the network generates a representation of the pronunciation of the middle letter of the 7 elements input (the three letters either side provide some context for the pronunciation of the middle letter). The pronunciation generated can be compared with the correct pronunciation (in terms of the features represented by the output units) and the link weights throughout the network are adjusted so that next time the letter occurs in the same context the output pronunciation will be improved.