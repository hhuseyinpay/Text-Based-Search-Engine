Technology is the making, usage, and knowledge of tools, machines, techniques, crafts, systems or methods of organization in order to solve a problem or perform a specific function. It can also refer to the collection of such tools and machinery. The word technology comes from Greek ?????????? (technología); from ????? (téchne), meaning "art, skill, craft", and -????? (-logía), meaning "study of-".[1] The term can either be applied generally or to specific areas: examples include construction technology, medical technology, and information technology.

Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments. The human species' use of technology began with the conversion of natural resources into simple tools. The prehistorical discovery of the ability to control fire increased the available sources of food and the invention of the wheel helped humans in travelling in and controlling their environment. Recent technological developments, including the printing press, the telephone, and the Internet, have lessened physical barriers to communication and allowed humans to interact freely on a global scale. However, not all technology has been used for peaceful purposes; the development of weapons of ever-increasing destructive power has progressed throughout history, from clubs to nuclear weapons.

Technology has affected society and its surroundings in a number of ways. In many societies, technology has helped develop more advanced economies (including today's global economy) and has allowed the rise of a leisure class. Many technological processes produce unwanted by-products, known as pollution, and deplete natural resources, to the detriment of the Earth and its environment. Various implementations of technology influence the values of a society and new technology often raises new ethical questions. Examples include the rise of the notion of efficiency in terms of human productivity, a term originally applied only to machines, and the challenge of traditional norms.

Philosophical debates have arisen over the present and future use of technology in society, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar movements criticise the pervasiveness of technology in the modern world, opining that it harms the environment and alienates people; proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition. Indeed, until recently, it was believed that the development of technology was restricted only to human beings, but recent scientific studies indicate that other primates and certain dolphin communities have developed simple tools and learned to pass their knowledge to other generations.
The use of the term technology has changed significantly over the last 200 years. Before the 20th century, the term was uncommon in English, and usually referred to the description or study of the useful arts.[2] The term was often connected to technical education, as in the Massachusetts Institute of Technology (chartered in 1861).[3] "Technology" rose to prominence in the 20th century in connection with the second industrial revolution. The meanings of technology changed in the early 20th century when American social scientists, beginning with Thorstein Veblen, translated ideas from the German concept of Technik into "technology." In German and other European languages, a distinction exists between Technik and Technologie that is absent in English, as both terms are usually translated as "technology." By the 1930s, "technology" referred not to the study of the industrial arts, but to the industrial arts themselves.[4] In 1937, the American sociologist Read Bain wrote that "technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them."[5] Bain's definition remains common among scholars today, especially social scientists. But equally prominent is the definition of technology as applied science, especially among scientists and engineers, although most social scientists who study technology reject this definition.[6] More recently, scholars have borrowed from European philosophers of "technique" to extend the meaning of technology to various forms of instrumental reason, as in Foucault's work on technologies of the self ("techniques de soi").

Dictionaries and scholars have offered a variety of definitions. The Merriam-Webster dictionary offers a definition of the term: "the practical application of knowledge especially in a particular area" and "a capability given by the practical application of knowledge".[1] Ursula Franklin, in her 1989 "Real World of Technology" lecture, gave another definition of the concept; it is "practice, the way we do things around here".[7] The term is often used to imply a specific field of technology, or to refer to high technology or just consumer electronics, rather than technology as a whole.[8] Bernard Stiegler, in Technics and Time, 1, defines technology in two ways: as "the pursuit of life by means other than life", and as "organized inorganic matter."[9]

Technology can be most broadly defined as the entities, both material and immaterial, created by the application of mental and physical effort in order to achieve some value. In this usage, technology refers to tools and machines that may be used to solve real-world problems. It is a far-reaching term that may include simple tools, such as a crowbar or wooden spoon, or more complex machines, such as a space station or particle accelerator. Tools and machines need not be material; virtual technology, such as computer software and business methods, fall under this definition of technology.[10]

The word "technology" can also be used to refer to a collection of techniques. In this context, it is the current state of humanity's knowledge of how to combine resources to produce desired products, to solve problems, fulfill needs, or satisfy wants; it includes technical methods, skills, processes, techniques, tools and raw materials. When combined with another term, such as "medical technology" or "space technology", it refers to the state of the respective field's knowledge and tools. "State-of-the-art technology" refers to the high technology available to humanity in any field.

Technology can be viewed as an activity that forms or changes culture.[11] Additionally, technology is the application of math, science, and the arts for the benefit of life as it is known. A modern example is the rise of communication technology, which has lessened barriers to human interaction and, as a result, has helped spawn new subcultures; the rise of cyberculture has, at its basis, the development of the Internet and the computer.[12] Not all technology enhances culture in a creative way; technology can also help facilitate political oppression and war via tools such as guns. As a cultural activity, technology predates both science and engineering, each of which formalize some aspects of technological endeavor.
Science, engineering and technology

The distinction between science, engineering and technology is not always clear. Science is the reasoned investigation or study of phenomena, aimed at discovering enduring principles among elements of the phenomenal world by employing formal techniques such as the scientific method.[13] Technologies are not usually exclusively products of science, because they have to satisfy requirements such as utility, usability and safety.

Engineering is the goal-oriented process of designing and making tools and systems to exploit natural phenomena for practical human means, often (but not always) using results and techniques from science. The development of technology may draw upon many fields of knowledge, including scientific, engineering, mathematical, linguistic, and historical knowledge, to achieve some practical result.

Technology is often a consequence of science and engineering — although technology as a human activity precedes the two fields. For example, science might study the flow of electrons in electrical conductors, by using already-existing tools and knowledge. This new-found knowledge may then be used by engineers to create new tools and machines, such as semiconductors, computers, and other forms of advanced technology. In this sense, scientists and engineers may both be considered technologists; the three fields are often considered as one for the purposes of research and reference.[14]

The exact relations between science and technology in particular have been debated by scientists, historians, and policymakers in the late 20th century, in part because the debate can inform the funding of basic and applied science. In the immediate wake of World War II, for example, in the United States it was widely considered that technology was simply "applied science" and that to fund basic science was to reap technological results in due time. An articulation of this philosophy could be found explicitly in Vannevar Bush's treatise on postwar science policy, Science—The Endless Frontier: "New products, new industries, and more jobs require continuous additions to knowledge of the laws of nature... This essential new knowledge can be obtained only through basic scientific research." In the late-1960s, however, this view came under direct attack, leading towards initiatives to fund science for specific tasks (initiatives resisted by the scientific community). The issue remains contentious—though most analysts resist the model that technology simply is a result of scientific research.
Paleolithic (2.5 million – 10,000 BC)
A primitive chopper

The use of tools by early humans was partly a process of discovery, partly of evolution. Early humans evolved from a species of foraging hominids which were already bipedal,[17] with a brain mass approximately one third that of modern humans.[18] Tool use remained relatively unchanged for most of early human history, but approximately 50,000 years ago, a complex set of behaviors and tool use emerged, believed by many archaeologists to be connected to the emergence of fully modern language.
Human ancestors have been using stone and other tools since long before the emergence of Homo sapiens approximately 200,000 years ago.[20] The earliest methods of stone tool making, known as the Oldowan "industry", date back to at least 2.3 million years ago,[21] with the earliest direct evidence of tool usage found in Ethiopia within the Great Rift Valley, dating back to 2.5 million years ago.[22] This era of stone tool use is called the Paleolithic, or "Old stone age", and spans all of human history up to the development of agriculture approximately 12,000 years ago.

To make a stone tool, a "core" of hard stone with specific flaking properties (such as flint) was struck with a hammerstone. This flaking produced a sharp edge on the core stone as well as on the flakes, either of which could be used as tools, primarily in the form of choppers or scrapers.[23] These tools greatly aided the early humans in their hunter-gatherer lifestyle to perform a variety of tasks including butchering carcasses (and breaking bones to get at the marrow); chopping wood; cracking open nuts; skinning an animal for its hide; and even forming other tools out of softer materials such as bone and wood.[24]

The earliest stone tools were crude, being little more than a fractured rock. In the Acheulian era, beginning approximately 1.65 million years ago, methods of working these stone into specific shapes, such as hand axes emerged. The Middle Paleolithic, approximately 300,000 years ago, saw the introduction of the prepared-core technique, where multiple blades could be rapidly formed from a single core stone.[23] The Upper Paleolithic, beginning approximately 40,000 years ago, saw the introduction of pressure flaking, where a wood, bone, or antler punch could be used to shape a stone very finely.[25]
Fire

The discovery and utilization of fire, a simple energy source with many profound uses, was a turning point in the technological evolution of humankind.[26] The exact date of its discovery is not known; evidence of burnt animal bones at the Cradle of Humankind suggests that the domestication of fire occurred before 1,000,000 BC;[27] scholarly consensus indicates that Homo erectus had controlled fire by between 500,000 BC and 400,000 BC.[28][29] Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.[30]
Clothing and shelter

Other technological advances made during the Paleolithic era were clothing and shelter; the adoption of both technologies cannot be dated exactly, but they were a key to humanity's progress. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380,000 BC, humans were constructing temporary wood huts.[31][32] Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate out of Africa by 200,000 BC and into other continents, such as Eurasia.[33]
Neolithic through Classical Antiquity (10,000BC – 300AD)
Man's technological ascent began in earnest in what is known as the Neolithic period ("New stone age"). The invention of polished stone axes was a major advance because it allowed forest clearance on a large scale to create farms. The discovery of agriculture allowed for the feeding of larger populations, and the transition to a sedentist lifestyle increased the number of children that could be simultaneously raised, as young children no longer needed to be carried, as was the case with the nomadic lifestyle. Additionally, children could contribute labor to the raising of crops more readily than they could to the hunter-gatherer lifestyle.[34][35]

With this increase in population and availability of labor came an increase in labor specialization.[36] What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures, the specialization of labor, trade and war amongst adjacent cultures, and the need for collective action to overcome environmental challenges, such as the building of dikes and reservoirs, are all thought to have played a role.[37]
Metal tools

Continuing improvements led to the furnace and bellows and provided the ability to smelt and forge native metals (naturally occurring in relatively pure form).[38] Gold, copper, silver, and lead, were such early metals. The advantages of copper tools over stone, bone, and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 8000 BC).[39] Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4000 BC). The first uses of iron alloys such as steel dates to around 1400 BC.
Energy and Transport
Meanwhile, humans were learning to harness other forms of energy. The earliest known use of wind power is the sailboat.[40] The earliest record of a ship under sail is shown on an Egyptian pot dating back to 3200 BC.[41] From prehistoric times, Egyptians probably used the power of the Nile annual floods to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and 'catch' basins. Similarly, the early peoples of Mesopotamia, the Sumerians, learned to use the Tigris and Euphrates rivers for much the same purposes. But more extensive use of wind and water (and even human) power required another invention.

According to archaeologists, the wheel was invented around 4000 B.C. probably independently and nearly-simultaneously in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture) and Central Europe. Estimates on when this may have occurred range from 5500 to 3000 B.C., with most experts putting it closer to 4000 B.C. The oldest artifacts with drawings that depict wheeled carts date from about 3000 B.C.; however, the wheel may have been in use for millennia before these drawings were made. There is also evidence from the same period of time that wheels were used for the production of pottery. (Note that the original potter's wheel was probably not a wheel, but rather an irregularly shaped slab of flat wood with a small hollowed or pierced area near the center and mounted on a peg driven into the earth. It would have been rotated by repeated tugs by the potter or his assistant.) More recently, the oldest-known wooden wheel in the world was found in the Ljubljana marshes of Slovenia.[42]

The invention of the wheel revolutionized activities as disparate as transportation, war, and the production of pottery (for which it may have been first used). It didn't take long to discover that wheeled wagons could be used to carry heavy loads and fast (rotary) potters' wheels enabled early mass production of pottery. But it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources.
Medieval and Modern history (300 AD —)
Main articles: Medieval technology, Renaissance technology, Industrial Revolution, Second industrial revolution, and Productivity improving technologies (historical)

Innovations continued through the Middle Ages with new innovations such as silk, the horse collar and horseshoes in the first few hundred years after the fall of the Roman Empire. Medieval technology saw the use of simple machines (such as the lever, the screw, and the pulley) being combined to form more complicated tools, such as the wheelbarrow, windmills and clocks. The Renaissance brought forth many of these innovations, including the printing press (which facilitated the greater communication of knowledge), and technology became increasingly associated with science, beginning a cycle of mutual advancement. The advancements in technology in this era allowed a more steady supply of food, followed by the wider availability of consumer goods.

Starting in the United Kingdom in the 18th century, the Industrial Revolution was a period of great technological discovery, particularly in the areas of agriculture, manufacturing, mining, metallurgy and transport, driven by the discovery of steam power. Technology later took another step with the harnessing of electricity to create such innovations as the electric motor, light bulb and countless others. Scientific advancement and the discovery of new concepts later allowed for powered flight, and advancements in medicine, chemistry, physics and engineering. The rise in technology has led to the construction of skyscrapers and large cities whose inhabitants rely on automobiles or other powered transit for transportation. Communication was also improved with the invention of the telegraph, telephone, radio and television.

The second half of the 20th century brought a host of new innovations. In physics, the discovery of nuclear fission has led to both nuclear weapons and nuclear power. Computers were also invented and later miniaturized utilizing transistors and integrated circuits. These advancements subsequently led to the creation of the Internet. Humans have also been able to explore space with satellites (later used for telecommunication) and in manned missions going all the way to the moon. In medicine, this era brought innovations such as open-heart surgery and later stem cell therapy along with new medications and treatments. Complex manufacturing and construction techniques and organizations are needed to construct and maintain these new technologies, and entire industries have arisen to support and develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education — their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have been created to support them, including engineering, medicine, and computer science, and other fields have been made more complex, such as construction, transportation and architecture.
Technology and philosophy
Technicism

Generally, technicism is a reliance or confidence in technology as a benefactor of society. Taken to extreme, technicism is the belief that humanity will ultimately be able to control the entirety of existence using technology. In other words, human beings will someday be able to master all problems and possibly even control the future using technology. Some, such as Stephen V. Monsma,[43] connect these ideas to the abdication of religion as a higher moral authority.
Optimism
See also: Extropianism

Optimistic assumptions are made by proponents of ideologies such as transhumanism and singularitarianism, which view technological development as generally having beneficial effects for the society and the human condition. In these ideologies, technological development is morally good. Some critics see these ideologies as examples of scientism and techno-utopianism and fear the notion of human enhancement and technological singularity which they support. Some have described Karl Marx as a techno-optimist.[44]
Skepticism and Critics of Technology
See also: Luddite, Neo-luddism, Anarcho-primitivism, and Bioconservatism

On the somewhat skeptical side are certain philosophers like Herbert Marcuse and John Zerzan, who believe that technological societies are inherently flawed. They suggest that the inevitable result of such a society is to become evermore technological at the cost of freedom and psychological health.

Many, such as the Luddites and prominent philosopher Martin Heidegger, hold serious, although not entirely deterministic reservations, about technology (see "The Question Concerning Technology[45])". According to Heidegger scholars Hubert Dreyfus and Charles Spinosa, "Heidegger does not oppose technology. He hopes to reveal the essence of technology in a way that 'in no way confines us to a stultified compulsion to push on blindly with technology or, what comes to the same thing, to rebel helplessly against it.' Indeed, he promises that 'when we once open ourselves expressly to the essence of technology, we find ourselves unexpectedly taken into a freeing claim.'[46]" What this entails is a more complex relationship to technology than either techno-optimists or techno-pessimists tend to allow.[47]

Some of the most poignant criticisms of technology are found in what are now considered to be dystopian literary classics, for example Aldous Huxley's Brave New World and other writings, Anthony Burgess's A Clockwork Orange, and George Orwell's Nineteen Eighty-Four. And, in Faust by Goethe, Faust's selling his soul to the devil in return for power over the physical world, is also often interpreted as a metaphor for the adoption of industrial technology. More recently, modern works of science fiction, such as those by Philip K. Dick and William Gibson, and films (e.g. Blade Runner, Ghost in the Shell) project highly ambivalent or cautionary attitudes toward technology's impact on human society and identity.

The late cultural critic Neil Postman distinguished tool-using societies from technological societies and, finally, what he called "technopolies," that is, societies that are dominated by the ideology of technological and scientific progress, to the exclusion or harm of other cultural practices, values and world-views.[48]

Darin Barney has written about technology's impact on practices of citizenship and democratic culture, suggesting that technology can be construed as (1) an object of political debate, (2) a means or medium of discussion, and (3) a setting for democratic deliberation and citizenship. As a setting for democratic culture, Barney suggests that technology tends to make ethical questions, including the question of what a good life consists in, nearly impossible, because they already give an answer to the question: a good life is one that includes the use of more and more technology.[49]

Nikolas Kompridis has also written about the dangers of new technology, such as genetic engineering, nanotechnology, synthetic biology and robotics. He warns that these technologies introduce unprecedented new challenges to human beings, including the possibility of the permanent alteration of our biological nature. These concerns are shared by other philosophers, scientists and public intellectuals who have written about similar issues (e.g. Francis Fukuyama, Jürgen Habermas, William Joy, and Michael Sandel).[50]

Another prominent critic of technology is Hubert Dreyfus, who has published books On the Internet and What Computers Still Can't Do.

Another, more infamous anti-technological treatise is Industrial Society and Its Future, written by Theodore Kaczynski (aka The Unabomber) and printed in several major newspapers (and later books) as part of an effort to end his bombing campaign of the techno-industrial infrastructure.
Appropriate technology
See also: Technocriticism and Technorealism

The notion of appropriate technology, however, was developed in the 20th century (e.g., see the work of Jacques Ellul) to describe situations where it was not desirable to use very new technologies or those that required access to some centralized infrastructure or parts or skills imported from elsewhere. The eco-village movement emerged in part due to this concern.
Technology and competitiveness

In 1983 a classified program was initiated in the US intelligence community to reverse the US declining economic and military competitiveness. The program, Project Socrates, used all source intelligence to review competitiveness worldwide for all forms of competition to determine the source of the US decline. What Project Socrates determined was that technology exploitation is the foundation of all competitive advantage and that the source of the US declining competitiveness was the fact that decision-making through the US both in the private and public sectors had switched from decision making that was based on technology exploitation (i.e., technology-based planning) to decision making that was based on money exploitation (i.e., economic-based planning) at the end of World War II.

Technology is properly defined as any application of science to accomplish a function. The science can be leading edge or well established and the function can have high visibility or be significantly more mundane but it is all technology, and its exploitation is the foundation of all competitive advantage.

Technology-based planning is what was used to build the US industrial giants before WWII (e.g., Dow, DuPont, GM) and it what was used to transform the US into a superpower. It was not economic-based planning.

Project Socrates determined that to rebuild US competitiveness, decision making through out the US had to readopt technology-based planning. Project Socrates also determined that countries like China and India had continued executing technology-based (while the US took its detour into economic-based) planning, and as a result had considerable advanced the process and were using it to build themselves into superpowers. To rebuild US competitiveness the US decision-makers needed adopt a form of technology-based planning that was far more advanced than that used by China and India.

Project Socrates determined that technology-based planning makes an evolutionary leap forward every few hundred years and the next evolutionary leap, the Automated Innovation Revolution, was poised to occur. In the Automated Innovation Revolution the process for determining how to acquire and utilize technology for a competitive advantage (which includes R&D) is automated so that it can be executed with unprecedented speed, efficiency and agility.

Project Socrates developed the means for automated innovation so that the US could lead the Automated Innovation Revolution in order to rebuild and maintain the country's economic competitiveness for many generations.
The use of basic technology is also a feature of other animal species apart from humans. These include primates such as chimpanzees, some dolphin communities,[54][55] and crows.[56][57] Considering a more generic perspective of technology as ethology of active environmental conditioning and control, we can also refer to animal examples such as beavers and their dams, or bees and their honeycombs.

The ability to make and use tools was once considered a defining characteristic of the genus Homo.[58] However, the discovery of tool construction among chimpanzees and related primates has discarded the notion of the use of technology as unique to humans. For example, researchers have observed wild chimpanzees utilising tools for foraging: some of the tools used include leaf sponges, termite fishing probes, pestles and levers.[59] West African chimpanzees also use stone hammers and anvils for cracking nuts,[60] as do capuchin monkeys of Boa Vista, Brazil.

Theories of technology
From Wikipedia, the free encyclopedia

There are a number of theories attempting to address technology, which tend to be associated with the disciplines of science and technology studies (STS) and communication studies. Most generally, the theories attempt to address the relationship between technology and society and prompt questions about agency, determinism/autonomy, and teleonomy.
If forced, one might categorize them into social and group theories. Additionally, one might distinguish between descriptive and critical theories. Descriptive theories attempt to address the definition and substance of technology, the ways it has emerged, changed and its relation to the human/social sphere. More substantively it addresses the extent of which technology is autonomous and how much force it has in determining human practice or social structure. Critical theories of technology often take a descriptive theory as their basis and articulate concerns, examining what way the relationship can be changed. The authors mentioned in this article are those that have some concern with technology or media, though they often borrow from one another and of course build upon seminal theorists that preceded them.
Descriptive approaches

    Actor-network theory (ANT) - posits a heterogeneous network of humans and non-humans as equal interrelated actors. It strives for impartiality in the description of human and nonhuman actors and the reintegration of the natural and social worlds. For example, Latour (1992) argues that instead of worrying whether we are anthropomorphizing technology, we should embrace it as inherently anthropomorphic: technology is made by humans, substitutes for the actions of humans, and shapes human action. What is important is the chain and gradients of actors' actions and competences, and the degree to which we choose to have figurative representations. Key concepts include the inscription of beliefs, practices, relations into technology, which is then said to embody them. Key authors include Latour (1997) and Callon (1999).

    Social construction of technology (SCOT) - argues that technology does not determine human action, but that human action shapes technology. Key concepts include:
        interpretive flexibility: "Technological artifacts are culturally constructed and interpreted ... By this we mean not only that there is flexibility in how people think of or interpret artifacts but also that there is flexibility in how artifacts are designed."
        relevant social group: shares a particular set of meanings about an artifact
        closure and stabilization: when the relevant social group has reached a consensus
        wider context: "the sociocultural and political situation of a social group shapes its norms and values, which in turn influence the meaning given to an artifact"

    Key authors include Pinch and Bijker (1992) and Kline.

    Structuration theory - defines structures as rules and resources organized as properties of social systems. The theory employs a recursive notion of actions constrained and enabled by structures which are produced and reproduced by that action. Consequently, in this theory technology is not rendered as an artifact, but instead examines how people, as they interact with a technology in their ongoing practices, enact structures which shape their emergent and situated use of that technology. Key authors include DeSantis and Poole (1990), and Orlikowski (1992).

    Systems theory - considers the historical development of technology and media with an emphasis on inertia and heterogeneity, stressing the connections between the artifact being built and the social, economic, political and cultural factors surrounding it. Key concepts include reverse salients when elements of a system lag in development with respect to others, differentiation, operational closure, and autopoietic autonomy. Key authors include Thomas P. Hughes (1992) and Luhmann (2000).

    Activity theory

[edit] Critical theories

    Values in Design - asks how do we ensure a place for values (alongside technical standards such as speed, efficiency, and reliability) as criteria by which we judge the quality and acceptability of information systems and new media. How do values such as privacy, autonomy, democracy, and social justice become integral to conception, design, and development, not merely retrofitted after completion? Key thinkers include Nissenbaum (2001).

[edit] Other stances

Additionally, many authors have posed technology so as to critique and or emphasize aspects of technology as addressed by the mainline theories. For example, Steve Woolgar (1991) considers technology as text in order to critique the sociology of scientific knowledge as applied to technology and to distinguish between three responses to that notion: the instrumental response (interpretive flexibility), the interpretivist response (environmental/organizational influences), the reflexive response (a double hermeneutic). Pfaffenberger (1992) treats technology as drama to argue that a recursive structuring of technological artifacts and their social structure discursively regulate the technological construction of political power. A technological drama is a discourse of technological "statements" and "counterstatements" within the processes of technological regularization, adjustment, and reconstitution.

An important philosophical approach to technology has been taken by Bernard Stiegler, whose work has been influenced by other philosophers and historians of technology including Gilbert Simondon and André Leroi-Gourhan.
[edit] Group theories

There are also a number of technology related theories that address how (media) technology affects group processes. Broadly, these theories are concerned with the social effects of communication media. Some (e.g., media richness) are concerned with questions of media choice (i.e., when to use what medium effectively). Other theories (social presence, SIDE, media naturalness) are concerned with the consequences of those media choices (i.e., what are the social effects of using particular communication media).

    Social presence theory (Short, et al., 1976) is a seminal theory of the social effects of communication technology. Its main concern is with telephony and telephone conferencing (the research was sponsored by the British Post Office, now British Telecom). It argues that the social impact of a communication medium depend on the social presence it allows communicators to have. Social presence is defined as a property of the medium itself: the degree of acoustic, visual, and physical contact that it allows. The theory assumes that more contact will increase the key components of "presence": greater intimacy, immediacy, warmth and inter-personal rapport. As a consequence of social presence, social influence is expected to increase. In the case of communication technology, the assumption is that more text-based forms of interaction (e-mail, instant messaging) are less social, and therefore less conducive to social influence.

    Media richness theory (Daft & Lengel, 1986) shares some characteristics with social presence theory. It posits that the amount of information communicated differs with respect to a medium's richness. The theory assumes that resolving ambiguity and reducing uncertainty are the main goals of communication. Because communication media differ in the rate of understanding they can achieve in a specific time (with "rich" media carrying more information), they are not all capable of resolving uncertainty and ambiguity well. The more restricted the medium's capacity, the less uncertainty and equivocality it is able to manage. It follows that the richness of the media should be matched to the task so as to prevent over simplification or complication.

    Media naturalness theory (Kock, 2001; 2004) builds on human evolution ideas and has been proposed as an alternative to media richness theory. Media naturalness theory argues that since our Stone Age hominid ancestors have communicated primarily face-to-face, evolutionary pressures have led to the development of a brain that is consequently designed for that form of communication. Other forms of communication are too recent and unlikely to have posed evolutionary pressures that could have shaped our brain in their direction. Using communication media that suppress key elements found in face-to-face communication, as many electronic communication media do, thus ends up posing cognitive obstacles to communication. This is particularly the case in the context of complex tasks (e.g., business process redesign, new product development, online learning), because such tasks seem to require more intense communication over extended periods of time than simple tasks.

    Media synchronicity theory (MST, Dennis & Valacich, 1999]) redirects richness theory towards the synchronicity of the communication.

    The Social Identity model of Deindividuation Effects (SIDE) (Postmes, Spears and Lea 1999; Reicher, Spears and Postmes, 1995; Spears & Lea, 1994) was developed as a response to the idea that anonymity and reduced presence made communication technology socially impoverished (or "deindividuated"). It provided an alternative explanation for these "deindividuation effects" based on theories of social identity (e.g., Turner et al., 1987). The SIDE model distinguishes cognitive and strategic effects of a communication technology. Cognitive effects occur when communication technologies make "salient" particular aspects of personal or social identity. For example, certain technologies such as email may disguise characteristics of the sender that individually differentiate them (i.e., that convey aspects of their personal identity) and as a result more attention may be given to their social identity. The strategic effects are due to the possibilities, afforded by communication technology, to selectively communicate or enact particular aspects of identity, and disguise others. SIDE therefore sees the social and the technological as mutually determining, and the behavior associated with particular communication forms as the product or interaction of the two.

    Time, interaction, and performance (TIP; McGrath, 1991) theory describes work groups as time-based, multi-modal, and multi-functional social systems. Groups interact in one of the modes of inception, problem solving, conflict resolution, and execution. The three functions of a group are production (towards a goal), support (affective) and well-being (norms and roles).

[edit] Analytic theories

Finally, there are theories of technology which are not defined or claimed by a proponent, but are used by authors in describing existing literature, in contrast to their own or as a review of the field.

For example, Markus and Robey (1988) propose a general technology theory consisting of the causal structures of agency (technological, organizational, imperative, emergent), its structure (variance, process), and the level (micro, macro) of analysis.

Orlikowski (1992) notes that previous conceptualizations of technology typically differ over scope (is technology more than hardware?) and role (is it an external objective force, the interpreted human action, or an impact moderated by humans?) and identifies three models:

    technological imperative: focuses on organizational characteristics which can be measured and permits some level of contingency
    strategic choice: focuses on how technology is influenced by the context and strategies of decision-makers and users
    technology as a trigger of structural change: views technology as a social object

DeSanctis and Poole (1994) similarly write of three views of technology's effects:

    decision-making: the view of engineers associated with positivist, rational, systems rationalization, and deterministic approaches
    institutional school: technology is an opportunity for change, focuses on social evolution, social construction of meaning, interaction and historical processes, interpretive flexibility, and an interplay between technology and power
    an integrated perspective (social technology): soft-line determinism, with joint social and technological optimization, structural symbolic interaction theory

Bimber (1998) addresses the determinacy of technology effects by distinguishing between the:

    normative: an autonomous approach where technology is an important influence on history only where societies attached cultural and political meaning to it (e.g., the industrialization of society)
    nomological: a naturalistic approach wherein an inevitable technological order arises based on laws of nature (e.g., steam mill had to follow the hand mill).
    unintended consequences: a fuzzy approach that is demonstrative that technology is contingent (e.g., a car is faster than a horse, but unbeknownst to its original creators become a significant source of pollution)
Emerging technologies
From Wikipedia, the free encyclopedia

In the history of technology, emerging technologies are contemporary advances and innovation in various fields of technology. Various converging technologies have emerged in the technological convergence of different systems evolving towards similar goals. Convergence can refer to previously separate technologies such as voice (and telephony features), data (and productivity applications) and video that now share resources and interact with each other, creating new efficiencies.

Emerging technologies are those technical innovations which represent progressive developments within a field for competitive advantage;[1] converging technologies represent previously distinct fields which are in some way moving towards stronger inter-connection and similar goals. However, the opinion on the degree of impact, status, and economic viability of several emerging and converging technologies vary.
History

Over centuries, innovative methods and new technologies are developed and opened up. Some of these technologies due to theoretical research, others due to commercial research and development.

Technological growth includes incremental developments and disruptive technologies. An example of the former was the gradual roll-out of DVD as a development intended to follow on from the previous optical technology Compact Disc. By contrast, disruptive technologies are those where a new method replaces the previous technology and make it redundant, for example the replacement of horse drawn carriages by automobiles.

Emerging technologies in general denote significant technological developments that broach new territory in some significant way in their field. Examples of currently emerging technologies include information technology, nanotechnology, biotechnology, cognitive science, robotics, and artificial intelligence.[2]
Debate over emerging technologies

Many writers, including computer scientist Bill Joy, have identified clusters of technologies that they consider critical to humanity's future. Joy warns that the technology could be used by elites for good or evil. They could use it as "good shepherds" for the rest of humanity, or decide everyone else is superfluous and push for mass extinction of those made unnecessary by technology.[3] Advocates of the benefits of technological change typically see emerging and converging technologies as offering hope for the betterment of the human condition. However, critics of the risks of technological change, and even some advocates such as transhumanist philosopher Nick Bostrom, warn that some of these technologies could pose dangers, perhaps even contribute to the extinction of humanity itself; i.e., some of them could involve existential risks.[4][5]

Much ethical debate centers on issues of distributive justice in allocating access to beneficial forms of technology. Some thinkers, such as environmental ethicist Bill McKibben, oppose the continuing development of advanced technology partly out of fear that its benefits will be distributed unequally in ways that could worsen the plight of the poor.[6] By contrast, inventor Ray Kurzweil is among techno-utopians who believe that emerging and converging technologies could and will eliminate poverty and abolish suffering.[7]

Some analysts such as Martin Ford, author of The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future,[8] argue that as information technology advances, robots and other forms of automation will ultimately result in significant unemployment as machines and software begin to match and exceed the capability of workers to perform most routine jobs.

As robotics and artificial intelligence develop further, even many skilled jobs may be threatened. Technologies such as machine learning[9] may ultimately allow computers to do many knowledge-based jobs that require significant education. This may result in substantial unemployment at all skill levels, stagnant or falling wages for most workers, and increased concentration of income and wealth as the owners of capital capture an ever larger fraction of the economy. This in turn could lead to depressed consumer spending and economic growth as the bulk of the population lacks sufficient discretionary income to purchase the products and services produced by the economy.[10]

Acronyms

NBIC, an acronym for Nanotechnology, Biotechnology, Information technology and Cognitive science, is currently the most popular term for emerging and converging technologies, and was introduced into public discourse through the publication of Converging Technologies for Improving Human Performance, a report sponsored in part by the U.S. National Science Foundation.[11]

Various other acronyms have been offered for the same concept such as GNR (Genetics, Nanotechnology and Robotics). Journalist Joel Garreau in Radical Evolution: The Promise and Peril of Enhancing Our Minds, Our Bodies — and What It Means to Be Human uses "GRIN", for Genetic, Robotic, Information, and Nano processes,[12] while science journalist Douglas Mulhall in Our Molecular Future: How Nanotechnology, Robotics, Genetics and Artificial Intelligence Will Transform Our World uses "GRAIN", for Genetics, Robotics, Artificial Intelligence, and Nanotechnology.[13] Another acronym coined by the appropriate technology organization ETC Group is "BANG" for "Bits, Atoms, Neurons, Genes".

Bioethics
From Wikipedia, the free encyclopedia

Bioethics is the study of controversial ethics brought about by advances in biology and medicine. Bioethicists are concerned with the ethical questions that arise in the relationships among life sciences, biotechnology, medicine, politics, law, and philosophy.
History
[edit] Terminology

The term Bioethics (Greek bios, life; ethos, behavior) was coined in 1927 by Fritz Jahr, who "anticipated many of the arguments and discussions now current in biological research involving animals" in an article about the "bioethical imperative," as he called it, regarding the scientific use of animals and plants. Lolas, F. (2008). Bioethics and animal research: A personal perspective and a note on the contribution of Fritz Jahr. Fritz Jahr's 1927 concept of bioethics. Kennedy Inst Ethics J, In 1970, the American biochemist Van Rensselaer Potter also used the term with a broader meaning including solidarity towards the biosphere, thus generating a "global ethics," a discipline representing a link between biology, ecology, medicine and human values in order to attain the survival of both human beings and other animal species.[1][2]
[edit] Development of a discipline

Although bioethical issues have been debated since ancient times, and public attention briefly focused on the role of human subjects in biomedical experiments following the revelation of Nazi experiments conducted during World War II, the modern field of bioethics first emerged as an academic discipline in Anglophone societies in the 1960s. Technological advances in such diverse areas as organ transplantation and end-of-life care, including the development of kidney dialysis and respirators, posed novel questions regarding when and how care might be withdrawn. Furthermore, as philosophy in Britain and elsewhere moved away from the influences of logical positivism and emotivism, the development of theories of ethics and their application to practical problems gained in interest. These questions were often discussed by philosophers and religious scholars; in England, there were notable contributions from GEM Anscombe and RM Hare. By the 1970s, bioethical think tanks and academic bioethics programs had emerged. Among the earliest such institutions were the Hastings Center (originally known as The Institute of Society, Ethics and the Life Sciences), founded in 1969 by philosopher Daniel Callahan and psychiatrist Willard Gaylin, and the Kennedy Institute of Ethics, established at Georgetown University in 1971. The publication of Principles of Biomedical Ethics by James F. Childress and Tom Beauchamp—the first American textbook of bioethics—marked a transformative moment in the discipline.

During the subsequent three decades, bioethical issues gained widespread attention through the court cases surrounding the deaths of Karen Ann Quinlan, Nancy Cruzan and Terri Schiavo. The field developed its own cadre of widely-known advocates, such as Al Jonsen at the University of Washington, John C Fletcher at the University of Virginia, Ruth Faden at Johns Hopkins University, and Arthur Caplan at the Center for Bioethics at the University of Pennsylvania. US Presidents have focused attention on bioethics for several decades, for instance by forming the President's Commission on the Study of Ethical Problems in Medicine and Biomedicine and Behavioral Research, which produced the landmark report, "Defining Death" in 1981.[3] President George W. Bush also relied upon a Council on Bioethics in rendering decisions in areas such as the public funding of embryonic stem-cell research
[edit] Purpose and scope

The field of bioethics has addressed a broad swath of human inquiry, ranging from debates over the boundaries of life (e.g. abortion, euthanasia), Surrogacy to the allocation of scarce health care resources (e.g. organ donation, health care rationing) to the right to turn down medical care for religious or cultural reasons. Bioethicists often disagree among themselves over the precise limits of their discipline, debating whether the field should concern itself with the ethical evaluation of all questions involving biology and medicine, or only a subset of these questions. Some bioethicists would narrow ethical evaluation only to the morality of medical treatments or technological innovations, and the timing of medical treatment of humans. Others would broaden the scope of ethical evaluation to include the morality of all actions that might help or harm organisms capable of feeling fear.
[edit] Principles

One of the first areas addressed by modern bioethicists was that of human experimentation. The National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research was initially established in 1974 to identify the basic ethical principles that should underlie the conduct of biomedical and behavioral research involving human subjects. However, the fundamental principles announced in the Belmont Report (1979)--namely, autonomy, beneficence and justice--have influenced the thinking of bioethicists across a wide range of issues. Others have added non-maleficence, human dignity and the sanctity of life to this list of cardinal values.

Another important principle of bioethics is its placement of value on discussion and presentation. Numerous discussion based bioethics groups exist in universities across the United States to champion exactly such goals. Examples include The Ohio State Bioethics Society and the Bioethics Society of Cornell. Professional level versions of these organizations also exist.
[edit] Medical ethics
Main article: Medical ethics

Medical ethics is the study of moral values and judgments as they apply to medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology.

Medical ethics tends to be understood narrowly as an applied professional ethics, whereas bioethics appears to have worked more expansive concerns, touching upon the philosophy of science and issues of biotechnology. Still, the two fields often overlap and the distinction is more a matter of style than professional consensus. Medical ethics shares many principles with other branches of healthcare ethics, such as nursing ethics. A Bioethicist assists the health care and research community in examining moral issues involved in our understanding of life and death, and resolving ethical dilemmas in medicine and science.
[edit] Perspectives and methodology

Bioethicists come from a wide variety of backgrounds and have training in a diverse array of disciplines. The field contains individuals trained in philosophy such as Peter Singer of Princeton University, Daniel Callahan of the Hastings Center, and Daniel Brock of Harvard University, medically-trained clinician ethicists such as Mark Siegler of the University of Chicago and Joseph Fins of Cornell University, lawyers such as Nancy Dubler of Albert Einstein College of Medicine or Jerry Menikoff of the federal Office of Human Research Protections, political economists like Francis Fukuyama, and theologians including James Childress. The field, once dominated by formally trained philosophers, has become increasingly interdisciplinary, with some critics even claiming that the methods of analytic philosophy have had a negative effect on the field's development. Leading journals in the field include The Hastings Center Report, the American Journal of Bioethics, the Journal of Medical Ethics and the Cambridge Quarterly of Healthcare Ethics.

Many religious communities have their own histories of inquiry into bioethical issues and have developed rules and guidelines on how to deal with these issues from within the viewpoint of their respective faiths. The Jewish, Christian and Muslim faiths have each developed a considerable body of literature on these matters. In the case of many non-Western cultures, a strict separation of religion from philosophy does not exist. In many Asian cultures, for example, there is a lively discussion on bioethical issues. Buddhist bioethics, in general, is characterised by a naturalistic outlook that leads to a rationalistic, pragmatic approach. Buddhist bioethicists include Damien Keown. In India, Vandana Shiva is the leading bioethicist speaking from the Hindu tradition. In Africa, and partly also in Latin America, the debate on bioethics frequently focusses on its practical relevance in the context of underdevelopment and geopolitical power relations.