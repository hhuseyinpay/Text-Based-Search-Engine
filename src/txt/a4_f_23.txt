examples of entropy-defying system development lose some of their validity in the context of software development. But, to return to the familiar domain of bridges and buildings: all major engineering is an entropy-decreasing exercise, and bridges and buildings tend to continue to perform adequately over long periods of time despite continual modification. So why not software systems? As we know, they are more complex and have tighter functional constraints. Nevertheless, it ought to be possible. We just have to be more sophisticated about how the successive modifications are introduced. The point is that incremental development towards an adequate system performance is not flying in the face of a fundamental law of the physical world. This means it might be possible; it does not mean that it will be easy.A key to locking out the bad influence of the second law of program evolution is to introduce modifications at the highest level possible in the design structure of the system. This is the conventional wisdom from software engineering. When you have analyzed a problem and understood the fault in the current version, you do not dive into the code and modify it (unless the fault is a trivial code-level fault). What you should do is go back to the design documentation and determine where this fault was first introduced into the system—it may be a basic design fault, it may be a fault in the detailed design, or (as I noted above) it may be simply a coding fault. When you have located the origin of the fault, you  Page 85modify the system to remove it at the source—this might mean modifying a modular design document, or it might mean modifying a detailed structural design document. Having corrected the problem at its origin you must then re-elaborate the changed design to determine the actual code changes that it specifies. So, the program code does, of course, become changed, but only as a result of a systematic redesign of the modified system.Such a scheme of systematic redesign, or controlled modification (as detailed in Chapter 6), would seem to be a necessary basis for a discipline of incremental system development. The reader who has entered properly into the spirit of this text will now be expecting to read about why AI takes us two steps back after this confident step forward towards practical AI software. Surely AI introduces a few extra problems?To begin with, in AI we lay no claim to a complete specification of the original problem. Do we have design documents that can form a basis for systematic redesign? We are going to develop an adequate implementation as a result of successive modifications of a first version. So, do we cobble together something that may have potential and dive into a long-term commitment with the RUDE cycle? Sometimes we do, but this has got to stop. We must design a first version bearing in mind both malleability and exploitation of conventional aspects of the problem. Then conventional design documents will be available, and thus systematic redesign is an approach to modification that we can use.Some more unqualified good news is that decompiling (i.e. inferring form from function) is not impossible in the context of the RUDE cycle. This is primarily because we have to modify not a monolithic function but a highly structured, and hierarchically functional object. We thus have access to form-function correspondences at various levels in the system, as you will see when you get to Chapter 6.How do we finish?Having a procedure for successive modification that does not lead us inexorably to a state of high-entropy software systems is a good thing, but it is by no means a complete answer to our overall problem. We now have some inkling of how to proceed sensibly, but how do we know when to stop? Remember that there is no precise specification (at least not one that describes a tractable problem) to measure the performance of the current version against.  Page 86This is a nagging problem in AI, but not one that has yet been faced squarely. The PhD demo AI systems have a fairly clear stopping point: it's when your money has run out and you've manage to get the system to do some impressive tricks on a few selected examples. For practical AI software, this will not do. Everyone knows this, but it has not occupied too much of anyone's time as there is no point in learning how to recognize the finish line before you have much idea of which direction it's in and how you might approach it.We are now beginning to see a growing concern with essentially this problem; it is typically addressed in the guise of 'validating expert systems'. The relative success of expert systems technology—the first major practical exploitation of AI—has forced this halting problem into general awareness.An expert system has to be validated (and sometimes verified according to the more ambitious practitioners—hence V&V) before it is fit for public use, or any real use, even an in-house application. Validation is a process that determines that a particular version of the software system has attained (or has not attained) an adequate level of performance with a reliability and efficiency that is satisfactory for the proposed application. We shall concentrate on adequate level of performance but we should not lose sight of the fact that a useful practical system must simultaneously satisfy a web of constraints.So, validation is a stopping test. Successful validation is not necessarily the cue for a full stop on the versioning cycle, it is more the sanctioning of a move from experimental to real-world application. The incremental development of the system may well continue within the system's application niche (in fact, it almost has to), for AI software is never finished: it is always possible to evolve a more adequate system from the current version.The reason that incremental development is almost certain to continue within the application niche is that validation cannot be a one-step procedure: it must be a long-term in-use procedure. So we can't expect to validate our AI system completely in an abstract test harness. We need cautious use of the system, and then validation will be attained as we build confidence in the system by assessing its performance repeatedly in the work environment. As is sometimes pointed out, this is exactly the way that we assess a human candidate for expert qualification.There are, at least, two rather different views of the notion of validation with respect to AI software systems. The view presented above  Page 87expresses the idea that validation and verification are two different processes. The latter is usually associated with the formal notions of proof of correctness, leaving the former term to cover the more down-to-earth, workman-like activities of checking that the system satisfies the operational need. This view is summed up neatly as:verification confirms "we have built the system right"validation confirms "we have built the right system"Verification is used to determine that the system is composed of modules correctly implementing the desired functionality. If, for example, you have determined that a sorting procedure is needed, then the implementation of that sort can be verified (in some sense). Thus you can be assured that the sorting function does indeed sort correctly. Validation is concerned with the problem of whether a sorting function applied in this particular context will contribute to an adequate solution to the problem—i.e. is sorting the right thing to be doing?Another view of the relationship between validation and verification is found in the VALID project (Vaudet and Kerdiles, 1989) which aims to produce generic validation tools for knowledge-based systems. They have designed VETA a metalanguage for validation. Figure 4.5 illustrates their validation taxonomy.As you can see, verification is no longer seen as a different sort of checking procedure to validation, but as a component of the validation process—a major subdivision contrasted with evaluation. They describe their two major kinds of validation as follows:By verification is meant 'proving that the KB or the KBS is in accordance with specifications or expectations'.By evaluation is meant 'judging or measuring the KB or the KBS characteristics'. (Vaudet and Kerdiles, 1989, p.4)These two views can be brought together if we call validation in the first scheme evaluation. Then given this renaming, you can see that determining that we have built the right system (i.e. validation in the first scheme) seems to closely fit judging the KBS characteristics (i.e. evaluation in the second scheme—the one illustrated).  Page 88Figure 4.5 "The validation structure used in the VALID project" from Vaudet and Kerdiles, 1989, p.4An important validity-checking technique that has arisen in AI is that of the self-explanation by the system of the reasoning behind its behavior. Using this technique, which we shall see in detail in Chapter 8, the software system actually presents to the observer a rationale—a justification for the validity—of its specific output. This is a technique that has been pioneered in expert systems (hence the examination of it in Chapter 8) because it is particularly easy to implement in conjunction with the basic architecture of an expert system—i.e. a set of facts and rules together with a mechanism of logical inference. Nevertheless, it is an idea that has potential in all types of software systems, but it is yet to be thoroughly exploited outside (or inside for that matter) the domain of expert systems.  Page 89An objection to this approach to software validation is that just because a software system tells us this is how it is computing a result that is no reason to believe it. The point is not that software systems tell lies (mostly they don't), but that any programmer will tell you how easy it is to tailor a system to output totally misleading information. So can we put any credence in the system's explanation of its behavior? Clearly, we must be careful, but, given safeguards about the generality of the underlying mechanism and the independent random checking of some explanations, this technique can be reliable and useful. We can't guarantee that the explanation-generating module is correct and never generates spurious explanations, but then this sort of guarantee is never possible in the real world anyway. How do we verify the verifying system? The answer is: we can't, but we can generate sufficient confidence and use the results in conjunction with other information to minimize reliance on any one source.Interestingly, Michie and Johnston (1984) elevate this feature of AI (often viewed as no more than a minor spin-off from expert systems' technology) to a (perhaps the) fundamental goal. They see the rise of technology as increasing the complexity of the environment which the human race must learn to comprehend and control if it is to continue to develop and prosper. From the software viewpoint, they believe that we must strive to construct programs which are understandable, and this means that the programs must be built "in the image of the human mind" (p. 60). "No longer can they be built with the central aim of maximizing performance and making the best of machine resources. Instead they will have to be... designed to be anthropocentric" (p. 60). So, for Michie and Jonhston, the important question seems to be not whether the algorithm is a correct implementation of the specification but whether an applied software system is transparent to its users—i.e. can the user readily 'see' why and how the system is doing what it is observed to be doing. As examples of this problem, they cite the Three Mile Island nuclear power station disaster as well as numerous other instances of problems caused by the inscrutability of high-technology systems. As a path to the solution of this problem, they offer us AI. "What AI is about is... making machines more fathomable and more under the control of human beings, not less" (p. 214, authors' emphasis). And what they mean is that we need "rule-based systems" for they "are specifically designed to operate with human concepts, both accepting them from the domain specialist and displaying them to the user as explanations" (p. 72). We shall take a look at this technology in Chapter 8, but for the moment just note this novel  Page 90angle on software validation, which is fairly radical although quite widespread among the expert systems' technologists.So far, I have concentrated on the nature of individual elements of a validation strategy, but what about the overall strategy? Testing for correctness has its problems as we noted in the earlier chapters. But a given software system either succeeds on a suite of test data or it doesn't—there's no two ways about it (or perhaps, there's only two ways about it!). Adequacy, as opposed to correctness, is a very different phenomenon. Experts will argue about adequacy of system performance with respect to a suite of test data, and there is no well-defined way to adjudicate.This raises a question more properly considered earlier (perhaps) of how should we best proceed from one inadequate version of the system to the next. If the system is clearly in error then we know that the error is to be removed in order to progress to the next version, but if the system is fairly adequate, what should we do to generate a more adequate version?The scheme that I like to invoke in this awkward situation first came to my attention through the architectural design work of Alexander in his influential little book Notes on the Synthesis of Form (1964). There has been much subsequent research on design methodology, and his simple linear scheme is undoubtedly much too simple. Nevertheless, his basic idea is still valid and can be used to seed systems development in the AI domain.His core idea is to approach adequacy through the elimination of major inadequacies. We don't say 'Is this system adequate?', but 'Does it exhibit any major inadequacies?' If so, it is not adequate. This may seem like a trivial shuffling of the words that leaves the problem much the same. It is not. If you can say that a system contains no major inadequacies, that system is adequate.The implication of this approach to the problem of incremental system development is that the direction of successive modifications of the system should be towards eliminating major behavioral inadequacies of the system. This may be a non-trivial strategy, but it is by no means a solution to the fundamental problem either. In fact, I will leave this topic as a largely unsolved problem, which is what it is in reality, but we shall return to see various approaches to it in the next two chapters.Bad solutions, based on thoughtlessness or a misguided belief that a sufficient input of energy can cancel out a lack of planning, abound, and form the central topic of the final section in this chapter.  Page 91The question of hackingBy 'hacking' I mean the thoughtless, more or less random, changing of system code in the hope that the problems or system inadequacies will go away. My use of the adjective ''thoughtless" needs a little explaining, as hackers might want to object that they sometimes spend long hours hunched over the keyboard thinking out the next fix to try on the system—the changes are far from thoughtlessly introduced. But my use of "thoughtless" refers to the lack of consideration of the system specification and design documents when generating a modification to the system code. You can spend as much time as you like wading back and forth through the program code itself, but you'll seldom see the wood because of all the trees in the way—less metaphorically: an overall view of the system will be obscured by the mass of code details. The resultant, local fixes, will seldom prove to be globally satisfactory in the long run. The preferred alternative is, as we have seen above, a procedure of systematic redesign.Boehm (1988) dignifies this activity with the status of a software development model and calls it the code-and-fix model.This was, as he says, the basic model used in the earliest days of software development; it contained two steps:1. write some code, and2. fix the problems in the code.Boehm sees three primary difficulties with this model:a. system quickly becomes excessively expensive to maintain,b. initial system is such a poor match to user's needs that it is either rejected outright or expensively redeveloped,c. lack of preparation for testing and modification of system means code fixes are expensive.This view of code hacking is clearly a commercial perspective, with its continual reiteration of the high costs associated with the code-and-fix model. Boehm uses this critique to motivate the emergence of the waterfall model, and then he goes on to explain the subsequent spiral model; we shall also look at both of these models in the final section of this chapter.  Page 92But even today, when we can no longer plead ignorance of the real consequences, why is code hacking so prevalent? There are several reasons. The first is the malleability of software systems, as I stressed in the opening chapter of this book. Making unthought-out changes to the code is quick, easy, cheap (in terms of immediate cost) and therefore difficult to resist. Lack of resistance is also due, I think, to the almost inextinguishable optimism of computer programmers, and a refusal to countenance the possibility that any given problem with the system is a fundamental problem in the design rather than a trivial slip in the coding. I'm sure that the psychological factors are complex, and so in order not to trivialize this aspect, I'll say no more, except to record my belief that this is an important factor in the overall problem.Another important reason for the ubiquity of code hacking in software system's development is to be found in the nature of the tools of the trade. Software tools and software development environments tend to make code hacking even quicker, cheaper, easier and thus less resistible than it already is, and AI-software development which, I shall argue, requires sophisticated support systems has an unfortunate tendency to fuel the problem. A state-of-the-art AI-programming support environment can all too easily be a code hacker's paradise. This may then be a problem that is endemic to AI-software system development—we shall see, but it's not at all obvious how to provide the necessary support for a discipline of incremental system development without at the same time also providing most of the goodies on the code hacker's Christmas wish list. Most powerful tools can be misused, but with the detailed specificity available with software support tools it is tempting to think that we may be able to provide power that is not easily abused.Finally, code hacking sometimes works. And it nearly always works for really good hackers working on small systems from which no great longevity is expected. Such work is necessarily the norm in educational situations, hence many software engineers enter the world of commercial software development with the firm (but erroneous) conviction that, irrespective of the principles of good practice, they will be able to hack any system into shape It's viewed as a puzzle, and a challenge to their probleto solving abilities at the keyboard.  Page 93The ancient programming proverb:The sooner you start coding, the longer it takes to finish.(adapted from Ledgard, 1975, p. 10)can be reconditioned for the incremental context:The less you consider the design documents, the less chance that you'll ever finish.orThe more you concentrate on the code, the less likely that you'll ever finish.Traditionally, consulting the design documents requires breaking away from the keyboard and leafing through stacks of diagrams and charts. It is part of this breaking away from the keyboard that is so hard to do when you are concentrating on an awkward problem in the system. As we shall see, part of the move to support an effective process of incremental development involves building and maintaining the system designs within the computer system itself. If this were the case, it might be both easier to persuade system developers to consult the designs as well as easier to enforce the stipulation that all modifications must come through the system designs. Interest in such construction and maintenance of system designs within the computer system is now quite intense, not only as part of the general move towards online everything but also as a definite aid to incremental system development methodologies. We shall look at some of the research in this direction in Chapter 6.Conventional paradigmsThe single, most widely acknowledged (although now somewhat dated) software development paradigm is based upon the waterfall model;it is illustrated in Figure 4.6.  Page 94Figure 4.6 The "waterfall" model of software developmentBoehm (1988), who devised the waterfall model, presents it as a refinement of a stagewise model that emerged from the problems inherent in the code-and-fix model which we have examined above. According to Boehm, the stagewise model stipulated that software be developed in successive stages such that some planning and design preceded coding and that a proper testing and evaluation stage followed it. The waterfall model was a 1970 refinement that provided two primary enhancements to the stagewise model:  Page 95(1) Recognition of the feedback loops between stages, and a guideline to confine the feedback loops to successive stages to minimize the expensive rework involved in feedback across many stages.(2) An initial incorporation of prototyping in the software life cycle, via a "build it twice" step running in parallel with requirements analysis and design.Boehm, 1988, p. 63Fundamental difficulties with the waterfall model "have led to the formulation of alternative process models. A primary source of difficulty with the waterfall model has been its emphasis on fully elaborated documents as completion criteria for early requirements and design phases." (Boehm, 1988, p.63). For some classes of software (e.g. compilers) this may still be the most effective way to proceed, but for others (e.g. interactive end-user applications) it does not work well.Boehm offers us three classes of model that have been proposed as improvements over the waterfall—the evolutionary development model, the transform model, and the spiral model. As the first two are dealt at length in subsequent chapters, we'll just look at the spiral model for the moment which is, in fact, also the one that Boehm focuses on.The spiral model (illustrated in Figure 4.7) reflects the underlying concept that each cycle involves a progression that addresses the same sequence of steps, for each portion of the product and for each of its levels of elaboration... each cycle of the spiral begins with the identification of•  the objectives of the portion of the product being elaborated (performance, functionality, ability to accommodate change, etc.);•  the alternative means of implementing this portion of the product (design A, design B, reuse, buy, etc.); and•  the constraints imposed on the application of the alternatives (cost, schedule, interface, etc.). p. 65  Page 96Figure 4.7 The spiral model of the software processBoehm terms the spiral "a risk-driven approach," which he contrasts with the "primarily document-driven or code-driven process" portrayed in the earlier models (p. 61). Having identified the alternatives relative to the objectives and constraints, the next step is to evaluate them. "Frequently, this process will identify areas of uncertainty that are significant sources of project risk. If so, the next step should involve the formulation of a cost-effective strategy for resolving the sources of risk. This may involve prototyping, simulation, benchmarking" etc.  Page 97Boehm sees the spiral model as accommodating the previous and supposedly alternative (e.g. evolutionary or transforms models) schemes as special cases. For example, if performance risks strongly dominate then the next step may be an evolutionary development one. Whereas, once prototyping efforts have resolved all the performance or user-inter-face risks, the next step may follow the basic waterfall approach. So, using Boehm's terminology, we might view AI software as software within which performance risks predominate, and reach extremes. Thus the question: is there an adequate approximation to this impossible problem? may be rephrased as: is there an achievable performance level which is both adequate and within acceptable constraints? And dependent upon whether this performance risk refers to overall behavior (with many component performance risks) or can be contained within specific, non-dominant aspects of system functionality, we can expect overall system development to be primarily RUDE or more conventional once the necessary RUDE development steps have been employed to resolve the per-formance-risk elements of the problem, respectively. But in either case (and in all intermediate ones) Boehm sees the spiral, risk-driven framework as applicable.Before leaving the spiral model we should see Boehm's answers to four fundamental questions:1. How does the spiral ever get started?2. How do you get off the spiral when it is appropriate to terminate a project early?3. Why does the spiral end so abruptly?4. What happens to software enhancement (or maintenance)?To begin with this model is meant to apply equally well to development or enhancement efforts. The model is started by a hypothesis that a software system is needed in some role. The spiral process then involves a test of this hypothesis, and failure of the test terminates the spiral. The alternative to failure is termination by installation of new or modified software. This leads to tests of operational effectiveness which can then lead to further hypotheses about software improvements and a new (maintenance) spiral is initiated.In his very-readable book on The Principles of Software Engineering Management, Gilb (1988) deals with totally conventional software and the management and organizational problems not the programming ones.  Page 98Of particular interest are his "moves away from the 'Waterfall Model' to the Evolutionary Delivery model" (p. ix). The evolutionary delivery method (evo method) is based on the following principle:Deliver something to a real end-user.Measure the added-value to the user in all critical dimensions.Adjust both design and objectives based on observed realities.(p. 84)As Gilb explains it the evo method is used to build systems that are "real and evolving. Users can modify design, coding detail and even their final objectives" (p.84). He sees this method as rooted in engineering literature and practice: "It is time the software community recognized the potential of the method and exploited it fully" (p.84). He contrasts the evo method with the dominant 'Waterfall Model' in which there is little or no formal feedback and learning because of the necessity to freeze the system requirements and design specification at an unrealistically early stage of knowledge and experience.The evo method is not radically new (IBM Federal Systems Division, for example, has been using it since about 1970), but it is, Gilb claims, "still the least understood of all the methods," and this despite the fact that it is "the most impressive method for ensuring dramatic productivity in software products" (p. 260). He then devotes a whole chapter to various sources of commentary on the idea of evolutionary delivery. It is composed of quotations, from both inside and outside software engineering, and (sometimes) his own commentary on them. Within the quotations from engineering, he gives us from B.V. Koen:Engineering is a risk-taking activity. To control these risks, engineers have many heuristics.1. They make only small changes in what has worked in the past, but they also2. try to arrange matters so that if they are wrong they can retreat, and3. they feed back past results in order to improve future performance.(Gilb, 1988, p. 301)  Page 99This viewpoint is of particular interest because it links back to our Chapter 1 discussion of what exactly is the nature of engineering practice, and what are its implications for development of robust and reliable software products.Gilb also consider Boehm's spiral model within his chapter on the evo method. He comments that the spiral model is not in any sense identical to the evo method but more of a framework within which evolutionary delivery could be chosen as a strategy. Gilb claims that the evo method, together with the tools and principles supplied in his book, constitutes a complete mechanism for making decisions about risk in software development. He does not see the spiral model as adding any necessary ingredient. It's just another perspective, and one that may be particularly useful in the context within which Boehm finds himself—i.e. where there is a need to find a "politically viable way to convert from a waterfall model dominated environment into a more evolutionary environment, without having to make a major formal shift of direction" (p. 270).Quite apart from the possibility of these political considerations, the spiral model is far broader in scope than other models we shall look at. This broad perspective in software development is sorely needed but largely neglected (as you will see in the most of the subsequent chapters). The reason for this seems to be that purely technical issues already provide us with more than enough problems, and that when these technical problems (such as specification-implementation correspondence) are solved we will be ready to raise our sights to consider the broader questions of, for example, the societal niche within which the system operates. It is not clear that such a partitioning of the total problem is a workable one, and although the spiral model is far-ranging, it still leaves many issues unaddressed. We shall return to explore some of these societal issues of software development in the last chapter of this book, but for the moment we must focus again on the purely technical questions.  Page 101CHAPTER 5New Paradigms for Software EngineeringWe are now at the point where we can fully appreciate the inadequacies of conventional software engineering methodology when confronted with AI-ish problems—not to mention its inadequacies tout court. And those of us who have not given up on the possibility of practical AI software will probably concede the need for significant changes in the way that software systems are developed—perhaps changes so radical that, in effect, we are looking for a whole new way to go about building software systems, in a phrase, a new paradigm for engineering AI software.In the last chapter we explored the nature of the incremental scheme that is at root the way that most AI systems are developed. And we saw only too clearly that the RUDE cycle, as basis for the construction of robust, reliable and maintainable software systems, is also rife with problems. On the positive side however, we saw that there is in fact substantial practical activity with evolutionary schemes in the software-systems world: primarily a strategy of evolutionary software delivery which shadows our desired incremental system development paradigm in a non-trivial way. Before I go on, in Chapter 6 to sketch out the possibilities for a discipline of exploratory programming based on the RUDE cycle, it might be wise to invest one chapter in trying to convince you that the problem has not yet been solved. Then I can't be accused of reinventing the cycle (as it were).Activity within the software world has not been totally focused on improving specifications, verification techniques, and testing regimes: there have, of course, been attempts to appreciate 'the big picture' and set  Page 102the software world aright by presenting new paradigms for software-sys-tem development. In this chapter we shall examine some of these proposals. Most are not engendered by an appreciation of the extra problems that AI brings to the software engineer, although a number of them do propose the use of AI technology as a crucial element of their paradigm. The majority of these new models for the software development process are a response to the unsolved difficulties of large-scale, but totally conventional, software engineering—the continued presence of undesirable phenomena, legacies of the software crisis, if not the good old software crisis itself. Figure 5.1''A taxonomy of AI and Software Engineering"1926-0102A.jpg  Page 103In Figure 5.1 we have a taxonomy of AI and software engineering. The node labeled methodology is the centre of interest for us. You will notice that it is directly connected to a number of other nodes. We can interpret the upward links as providing three perspectives on methodology within this broad domain of AI and software engineering overlap. It is the central perspective—"AI mechanisms and techniques in practical software"—that is of central concern to us, but achievement of this goal will require that we also devote some considerable attention to both of the other two perspectives: sophisticated support environments (probably containing AI-ish features) are needed to help us manage the development of practical AI-software, and, although we might reject the basic framework of conventional software engineering, we should reuse and exploit as much as possible of the conventional wisdom.We can begin by looking at the possibility that the programming task is (or should be if done properly) quite well defined: we have a formal specification and the task amounts to transforming it into a different sort of formalism, an operational one, a program. Perhaps this transformation from one formalism to another can be done automatically, thereby removing the human from this essentially mechanical process and probably gaining confidence that the transformation is a correctness-preserving one to boot. Automatic programming is the subdiscipline (traditionally of AI) that takes roughly this line of attack on the problem.Automatic programmingWithin the short span of computer science history, research in automatic programming has had a long run. It was there almost at the outset. With creative problem solving finished once an acceptable specification was developed, it seemed not too unreasonable that a correct operational equivalent could be generated automatically—i.e. that a program could be automatically (logically) deduced from a specification. The temptation to view programming in this theorem-proving manner is yet another legacy of the fact that a program (and in this case the specification as well) is a statement in a formal language and so the power of logic ought to be applicable, etc.If automatic programming were possible then the bulk of the programmer's headaches would vanish and the software crisis would be a thing of the past, a quaint old phenomenon (like say, phlogiston) to be chuckled over as we wait for the automatic-programming system to chum out the latest, guaranteed correct, implementation of our specification for  Page 104a desired piece of software. But, sadly, it does not appear to be possible. In fact, it is more likely that we will be chuckling over the early proposals for automatic programming systems as we take a break from the demands for creative input made by the software development environment that we are piloting through a software design sequence. Rich and Waters (1988) provide a comprehensive review of automatic programming, presenting what they see as both its myths and its prospects. They see the simplistic view (outlined above) as describing an unachievable goal because it is based on false assumptions (such as the one we surveyed earlier, that requirements and hence a specification can be complete). But they also see work in automatic programming as making continual progress and thus yielding many benefits which have resulted in (and will continue to produce) dramatic improvements in programmer productivity and in further reduction of the distance between programmers and end users.Automatic programming, in its extreme manifestation and as the answer to the software engineer's problems, is clearly a pipe dream, but it may have a useful role to play in the future of software engineering. It is an AI topic because, just like straightforward theorem proving, the logical deduction is only a minor part of the problem. There is a practical necessity for heuristics to choose effectively between alternative proof paths, proof strategies, when there are far too many to consider them all, and no well-defined way to choose the best (or even guaranteed adequate choices). There is also a need to represent and efficiently employ domain knowledge. Application domain expertise is required for software development, and this requirement lands us squarely in the middle of another AI sub-area.For the reader who would like to delve further into the current state of the art with respect to formal synthesis of programs (particularly the logic-programming approaches), Kodratoff, Franova, and Partridge (1990) give a critical appraisal in The why and how of program synthesis as well as provide some positive contributions to the problem. Of special interest to our general concern in this book (i.e. engineering AI software) is the authors' introduction of a formal reverse transformation to give a complete cycle—i.e. specification to algorithm and algorithm to specification. Clearly, we might expect such formalized support for the transition from program behavior to specification to be most useful in the context of exploratory, evolutionary programming. And as we shall see below, the new paradigms may be iterative and evolutionary but they all pivot on the notion of specification without ever saying how we draw  Page 105implications for the specification from observed behavioral inadequacies. It is one thing to say that changes should be made to the specification and not the implementation, and quite another to determine what those specification changes should be on the basis of observation of the behavior of an implementation. The failure to address this crucial question is a major reason why the proposed new paradigms fall way short of providing a methodology for the incremental development of AI software.A useful compendium of the current wisdom on this topic is the collection (mis)titled Readings in Artificial Intelligence and Software Engineering, edited by Rich and Waters (1986); it might easily have be entitled Readings in Automatic Programming. In the introduction they make the claim that:The ultimate goal of artificial intelligence applied to software engineering is automatic programming.[!] In the limit, automatic programming would allow a user simply to say what is wanted and have a program produced completely automatically. However, it is unlikely that this sort of performance will be achieved soon. (p. xi, authors' emphasis)But they note subsequently that "By itself, deductive synthesis is not likely to be a solution to the automatic programming problem. However, deductive methods... will very likely be an important component of all future automatic programming systems." (p. 1)This last view is much more in accord with the notions that I shall promote in this chapter. So, where and how will deductive methods fit into a methodology for engineering AI software?Rich and Waters (1986) provide us with a useful framework within which to develop answers to this question. They explain their scheme with reference to Figure 5.2.Working from the bottom up there are projects to develop so-called very high level languages. Coding (i.e. writing the program) is done with structures that are both human- and problem-oriented such that you almost don't realize that you are indeed programming when you cast your problem in terms of the a language provided. In this (somewhat strained, I admit) sense of automatic programming, the necessary techniques have been 'compiled' into the very high-level language, and the programmer is programming without knowing it (and the built-in constraints in the language ensure that a minimum of errors are introduced).  Page 106Figure 5.2 "A hierarchy of agents producing an accounting system" from Rich and Waters (1986) p. xiiThe complementary line of attack is to work top-down with respect to Figure 5.2. The research in this area centres on converting free-form English into formal requirements and also on the representation of knowledge in a requirements document. In this niche, automatic programming delivers a formal (and by implication reliable) requirements document, which is, of course, not a program, but it would, nevertheless, constitute a big advance towards the more lofty peaks of automatic programming dreams.Two more ways to view the automatic programming problem are presented by Rich and Waters as "vertical slices" through Figure 5.2. One sort of vertical slice is obtained when we limit the domain so drastically that a complete automatic programming system can be constructed. Within this subdomain we do already find some practical systems in existence. Use of a special-purpose problem-oriented language and a special-purpose program generator makes completely automatic programming possible, but at the cost of a very narrow domain of applicability. Such systems exist for certain business and scientific applications and are called Application Generators (AG).  Page 107Horowitz, Kemper and Narasimham (1985) survey the AG systems available. They provide evidence to support the contention that there is an urgent need for more productivity in software engineering—productivity increases in the field have been relatively small in the last decade whilst demand has grown alarmingly, and the qualified manpower available has similarly only increased relatively slightly. And, of course, automatic programming could help alleviate the worst impacts of this problem—the end-users would develop their own software systems given a suitable AG. Quite surprisingly, in this narrative packed with overambitious aspirations yet only lightly sprinkled with mentions of practical demonstrations, an impressive range of practical AG systems exist and perform a useful service in the real world of business systems. So here's a rare opportunity for me to provide you with some concrete details; I shall avail myself of it.Horowitz et al.(1985) give Table 5.1 to contrast typical programming languages with their equivalent in AG systems. From this table it is quite easy to see why automatic programming is possible in certain domains and what characterizes those domains.Table 5.1 A comparison of programming languages and AGs [from Horowitz, Kemper, and Narasimhan, 1985, p. 50]Programming Languages Application GeneratorsProcedural; programs define computations step by stepNon-procedural; very high level programs that mainly state the required resultsUsually non-data-intensive Data-intensiveExplicit iteration and loops Implicit iterationVery wide range of application Limited problem domainsDetailed documentation necessary Mostly self-documentingPrototyping is usually slow and error prone Supports fast and correct prototypingDifficult to maintain Easier to maintain  Page 108Automatic programming, in the form of AGs, is a viable technology in domains of data manipulation as opposed to what we might call computation (either numeric or pattern matching operations). This excluded domain is, of course, the major domain of good old-fashioned programming—detailed organizing of the primitive steps to achieve some desired computation. This might seem to limit the scope of AG technology rather severely, and it does, but in the business world especially many of their problems are ones of management, manipulation, retrieval, and presentation of data from large files or data bases. And this is the domain of AG technology. Figure 5.3 is an illustration of the sort of facility that an AG can offer. Figure 5.3 Program fragments in (a) a report generator language, and (b) a conventional programming language from Horowitz, Kemper, and Narasimhan, 1985, p. 43file EMPLOYERSlist NAME SALARYWHERE DEPT = "CSCI"(a)  <open file Employees>for i:=1 to maxNumOfEmpl do     begin       <retrieve ith data record using        some existing retrieval path>       <extract NAME and SALARY fields>         if DEPT[i]="CSCI" then          begin             writln(NAME[i',SALARY[i]          end     end    <close file EMPLOYEES>   (b)       As you can see from Figure 5.3, a report generator can really be an automatic programming system. But as you can also see, it's not really a programming problem, at least not one from the major class of programming problems—it's a data retrieval and presentation (or formatting) problem.  Page 109AGs are seen as a powerful technology within their rather limited niche, and so interest focuses on whether and how this technology may be extended to broaden its scope of applicability. Horowitz et al. (1985) argue that the merger of AGs with conventional programming languages is both desirable and feasible as a route to extending their scope.After that short excursion into the world of AGs, we can return to consider the last approach to the notion of automatic programming. The second slice through our diagram yields the approach in which we find partial automation at many levels. Work with this emphasis has given rise to systems that are termed assistants: The Designer/Verifier's Assistant, the Programmer's Apprentice, the Knowledge-Based Software Assistant, etc. This approach might be termed semi-automatic programming, as some of the programming burden is taken from the software engineer's shoulders—just as much as the current state of the art will permit.In sum, the practicing software engineer is not about to be made redundant by automatic programming systems, but he or she will probably experience one or more of these different manifestations of automatic programming helping to manage the complexity or increase the reliability of the software design and development task. Purely hand-crafted software is rapidly becoming a thing of the past. And as our automatic tools improve we'll see hand-made software, like certain hand-made arte-facts, become the expensive and unreliable alternative. Old-fashioned software systems may possess the charms of individuality and the blemishes that a low-tech cottage industry engenders, but most users would willingly forego these niceties when grappling with a software product, as opposed to admiring the one-off features in an old Persian carpet.Transformational implementationThis is an active, and therefore important, subfield of automatic programming. A transformational implementation scheme is typically based on the notion of achieving program correctness by converting programs in an easily readable, very high-level, wide-spectrum language into an efficient, directly executable form.If the transformations were all automatic and the original statement of the problem (in that highly qualified language) was something more like a user specification than a conventional program, then we should be right back to the notion of fully automatic programming. Interest in this area stems from the realization that although there is little chance of fully automating transformations there is a lot of scope for developing very  Page 110useful, semi-automatic transformations. In other words, there is no requirement to solve the problem completely before anything of use will be forthcoming.Agresti (1986) in his edited collection, New Paradigms for Software Development, distinguishes between the operational paradigm and the transformational paradigm.He illustrates the differences as shown in Figures 5.4 and 5.5.Figure 5.4 "The operational paradigm"  Page 111Figure 5.5 "The transformational paradigm" about here, both from Agresti, 1986, p.8 and p.9Notice that both paradigms focus on the specification of the problem—i.e. they follow the conventional wisdom of SAV/SAT.  Page 112Advocates of the operational paradigm expose the fiction that specification and implementation (the WHAT and the HOW) of a problem can be separated and dealt with sequentially (WHAT before HOW). The reality is that we cannot discuss the WHAT of a system effectively without considerations of design and implementation intruding in our discussions.A second major problem with the conventional wisdom is that it relegates too many important concerns to the later stage of design. According to Agresti (1986, p. 7), the designer needs to consider:•  Problem-oriented issues of decomposing high-level functions into successively lower levels•  Purely design issues such as information-hiding and abstraction•  Implementation issues such as system performance constraints and the feasibility of the design to be implemented in the target hard-ware-software environmentThe operational paradigm employs a different basis for the separation of specification and implementation. In Agresti's (1986, p. 7) words: ''The guiding principle is not to overwhelm system designers by requiring them to assess simultaneously the varied issues above. Instead, the separation is based on problem-oriented versus implementation-oriented concerns. The goal is to produce an operational specification that deals only with problem-oriented issues and to express it in some language or form that enables it to be executed, evaluated, or interpreted to reveal the behavior of the system." The resulting specification, as Agresti also notes, is a kind of prototype.According to Agresti (1986), the transformational paradigm "is an approach to software development that uses automated support to apply a series of transformations that change a specification into a concrete software system." This may not strike the reader as very different from the operational paradigm, but the difference in emphasis comes out when he continues: "The transformational approach addresses the labor intensive-ness of software development by using specialized computer software to transform successive versions of the developing system mechanically."(p. 8)So, the operational paradigm focuses on developing an appropriate specification for your system, and the transformational paradigm then concentrates on how to transform most accurately and efficiently your specification into a usable implementation. But notice that both schemes  Page 113give pride of place to the specification; it is still taken to be the lynch-pin of the software development process.There are two classes of transformation, vertical and lateral (Rich and Waters, 1986, p.XV). Vertical transformations are between different levels of abstraction, as, say, from an abstract specification language to a more concrete operational representation. Lateral transformations are within a level of abstraction and are used to produce an alternative equivalent form, as, say, transforming X**2 to X*X. Lateral transformations can then be used to improve program efficiency.An important spin-off from the transformational paradigm (as we shall see) is that it holds the possibility of recording and storing the essential details of a design and implementation sequence (perhaps a largely human-driven one) for subsequent reuse and replaying of the design and implementation process automatically. Why would you want to keep redesigning and reimplementing the same program? That's the obvious question, and the equally obvious answer is that you wouldn't. But you might well want to keep redesigning and reimplementing variations on the initial specification. And if so, it seems reasonable to expect that substantial portions of the earlier design and implementation sequence could be reused to automate significantly the implementation of successive versions of a system.This notion has its problems. To begin with we have little idea what are the essential details of a design process, nor how to store them such that they are conveniently retrievable when we want to reuse the stored design knowledge. But clearly, if this concept is not totally impossible (and there's no reason to believe that it is) then it may well contribute to a discipline of incremental system development—this chimerical beast that I claim we need for engineering artificially intelligent software. But this is not generally the motivation for exploring the transformational paradigm, as we can see in the following manifestation of a transformational scheme.The "new paradigm" of Balzer, Cheatham, and GreenA representative example of the work on transformational schemes can be found in Software Technology in the 1990's: Using a New Paradigm, by Balzer, Cheatham, and Green (1983). Figure 5.6 illustrates their new paradigm and contrasts it with a more conventional paradigm (SAT with pro-totyping).  Page 114Figure 5.6 “A new paradigm and the conventional paradigm for software development" from Balzer, Cheatham, and Green. 1983  Page 115If you half-close yours eyes and look at the two diagrams in Figure 5.6, you will gain a fairly accurate impression of the major innovation introduced in the new paradigm. It is that a bunch of the iterative loops back to an earlier stage have been shifted left in order to generate the new paradigm from the old—i.e. the bulk of the iterative system development has been moved from operating on the implemented system to become manipulation of the specification. The new paradigm thus clearly embraces the notion of iterative development, but as development focused on the specification and system requirements.This is precisely in accord with the oft-heard lament from the centre party of computer science when its members feel motivated to comment on the difficulties facing the AI programmer, to wit, the fundamental mistake is to work with the implementation when it's the specification that you must concentrate on.The authors are again motivated, not by concerns for AI software, but by the problem of escalating demand for software unmatched by increasing productivity of thoroughly conventional software systems. Hence, they propose more automation of the software development process (and this implies, for them at least, more formalization). The foremost problem, as they see it, is maintainability—both correcting bugs and, more importantly, adding enhancements. From the perspective of AI-software building we might view their goal of tackling the successive-enhance-ments problem as quite similar to our one of evolutionary software development—but there is a critical difference in the 'well definedness' of the two sorts of enhancement, and we should not forget that either. However, for the moment, we can see that progress towards this maintainability goal may well contribute significantly to our overall goal of engineering AI software.Balzer, Cheatham, and Green single out two major flaws in the conventional methodology for software development:1. There is no technology for managing the knowledge-intensive activities that constitute software development processes. These processes, which convert requirements into a specification and then into an implementation, are informal, labor intensive, and largely undocumented. Information behind these processes and the rationale behind each of their steps is crucial for maintenance, but unavailable.  Page 1162. Maintenance is performed on source code, a representation that is cluttered with programming-language idiosyncrasies and is likely to be made even more opaque (with respect to attempted visualization of the underlying problem) by the introduction of optimization tricks and the like.I'm sure that their second flaw actually exists, but the fact that it does so is perhaps more a denunciation of current practice than a criticism of the conventional methodology which (in its better manifestations) makes it quite clear that code-hacking is not the route to system enhancement. But the prime reason why code-hacking is the usual approach to system enhancement is because of the first of the two flaws listed: it is because the record of the prior processes is scanty, when it exists, usually out of date (i.e. it documents some earlier version of the system) and, even more usually, doesn't exist at all in the vicinity of the software. But if it was in the system and updated automatically as new enhancements are introduced, then both of the methodological flaws would have been repaired without recourse to a whole new paradigm. However, as mentioned earlier, we don't know what to store, how to store it, nor how or when to retrieve it, let alone how to make the process automatic (or compulsory) as each new modification to the system is implemented.Deciding what information to store and how to best store it to constitute effective on-line documentation of a piece of software is, as I've said, an open problem (but see one solution in the Eiffel system in Chapter 9). But it's one that we can confidently expect to get to grips with by appropriate study of the problem. The associated subproblem of ensuring that the on-line documentation is entered and updated whenever necessary also needs to be addressed. For in just the same way as you can take a horse to water but can't make it drink, you can supply the software engineer with the tools but you can't make him or her use them—or can you?As it happens we may be in a position to ensure that the horse drinks when it's supposed to drink, as it were; perhaps we can make an offer that can't be refused. By encapsulating the software development process in a comprehensive support environment we gain the power to virtually insist that the software developer follows almost any procedure that we care to specify—a carrot and stick strategy, metaphorically speaking of course. This point will be elaborated in Chapter 9, but I mention it here because Balzer, Cheatham and Green make a similar proposal.They sketch out the possibilities for a knowledge-based software assistant (KBSA);Figure 5.7 depicts their vision.  Page 117Figure 5.7 "A generalized knowledge-based software assistant structure" from Balzer, Cheatham, and Green, 1986The KBSA supports the new paradigm by recording the development activities, performing some of them, analyzing their effects, and aiding their selection. By mediating in the development (using the new paradigm) of a particular piece of software the KBSA can record the human design decisions such that when the specification is modified this recorded history of the design process can be reused to more or less automate the reimplementation. This scenario removes much of the pain from the process of modifying a specification and thereby making reimplementation necessary. This makes specification modification less problematic (at least it would if it were possible), but by my reckoning code hacking is still the more tempting option. I believe that we must, in addition, actively eliminate the code hacking option (and we'll get to that in Chapter 9).  Page 118Operational requirements of KowalskiAnother example of a new approach to engineering software systems stems from the logic-programming movement. I shall use Kowalski's schemes as a representative sample from this active subfield.With the advent of the programming language Prolog, something quite close to logic became a machine-executable notation—i.e. a programming language. This gives logic (I should write, almost gives logic, but I'll stop quibbling as long as you bear in mind that Prolog programming is not programming in logic) a new face—a procedural interpretation in fact. So, a statement in logic can take its usual stance in the world as a declarative statement, which may be either true of false, and is rather like the specification of a problem. But, in addition, as a Prolog program it can be executed—interpreted procedurally as a specification of a sequence of actions to be performed. This raises the exciting possibility that logic can be an executable specification language, or even a representation of requirements, as we shall see.Kowalski gave us the following observation about programs:PROGRAM = DECLARATION + CONTROLThis slogan equation makes the point that there are two rather different features in a typical program, say, a FORTRAN, PASCAL, or COBOL program, and they are intermeshed. The programmer makes decisions about fixed relationships between objects in the computation, e.g. if one item in a list of numbers is greater than the following one then the list is not sorted in ascending order. The programmer also has to make detailed decisions about the flow of control in the program, e.g. how to exchange the places of two items in a list, and how to ensure that the last list item is not compared with a non-existent successor. And to make matters worse both sets of detailed decisions are tightly intermeshed and interrelated—no wonder programming is so difficult.Logic programming, and hence Prolog, is just the right scheme to deal with this confused state of affairs. In logic programming we have a clear separation of declarative structure and control structure, and moreover the control-structure considerations are removed from the programmer's scope of concern—they are packed away in the language processor. So the programming task has been reduced to the simpler task of just specifying the relationships that must hold true if the desired computation is to be correct. I'm sure that this must all sound mysterious and quite dubious to the non-Prolog programmer. A healthy skepticism is to be  Page 119encouraged, but, for once, it's almost unfounded. It is thus time for an illuminating example.Consider the trivial problem of counting the number of items in a list, whose name is L. The conventional programmer, untainted by exposure to either LISP or Prolog, would immediately think of setting up a loop structure together with an accumulator initially set to zero. And then of starting at one end of the list and moving item-by-item through the list adding one to the total in the accumulator on each move until the end of the list is reached. The total number of items in the list will then be given by the final number in the accumulator.This is classical mixed declarative and control-structure programming. The programmer has to worry about the declarative essentials of the problem, e.g. that the discovery of another list item means that one must added to the accumulator. And, at the same time, the programmer must be concerned about setting up a loop structure that starts at the first list item and finishes at the last whether the list has zero, one, or ten thousand items in it.The logic programmer approaches the task totally differently. This person observes that the number of items in any list is one plus the number of items in the same list without its first item. And this is true for all lists except the empty list which contains zero items.After this sort of statement one is very tempted to retort, "That is obviously true but not much help. It's just a less slick re-statement of the problem." If you feel this sort of temptation coming on, resist it vigorously, because we can rewrite that re-statement of the problem directly in Prolog and execute it to compute list lengths. As a Prolog program it would look like this:listlength( , 0).listlength([Head | Tail], Length):-listlength(Tail, Lengthtail),Length is Lengthtail + 1.The cryptic syntax of Prolog is no help, but I can transcribe this Prolog program into English. It consists of two clauses. The first one just states the (seemingly) trivial fact that the length of an empty list is zero. No quarrel with that I hope. The second clause states the similarly obvious fact (although not in such an obvious way, I admit) that the length of a non-empty list is the length of its tail (i.e. the original list without its first item) plus one. This second clause can be written out in a more Prology way: the length of a non-empty list is the value of variable Length if the  Page 120value of Lengthtail is the length of the tail of the original list and the value of Length is equal to the value of Lengthtail plus one.To claim that this last sentence is one from the set of obviously true statements may be stretching credulity a bit, but if you close your eyes and think about it step by step, there will be no doubting is veracity. This is taking us somewhat far afield, but it is worth the journey if we can use it to dispel some of the magic and mystery that often surrounds logic programming.Hopefully, even those readers who suffer from Prolog-phobia have been able to stay with us for the requisite couple of paragraphs and thus now have a fairly firm idea what is being claimed about declarative programming. The above two clauses constitute an executable Prolog program that will compute the length of any list that it is given, and yet we did not have to bother ourselves with setting up a loop, or a counter variable and incrementing it to move from one list item to the next, and how to test for the end of the list, etc. In short, we wrote a program (I'm being overgenerous here, but I'm sure that you could have done it if this was a dynamic, on-line book) to compute list lengths without any of the normally necessary, petty concerns of control structure. But at least one aspect of conventional control structure did intrude.The whole truth is that I did (I'll accept full blame) introduce just one control structure consideration. I plead guilty, but I had no choice. The offending feature is the ordering of the two parts that constitute the latter part of the second clause. The statement that the length of the tail of the original list is the value of Lengthtail must precede the statement that the value of Length is the value of Lengthtail plus one.Why is this? As a declarative statement of logical truth the order has no significance. It's just a fact that if the value of Length is to be the length of a non-empty list then two things have to be true: the tail has to have a length (we called it Lengthtail) and the relationship between the two values has to be that the length of the tail is one less than the length of the original list. But in order to compute specific values with this structure, the computer cannot add one to the value of Lengthtail until it has computed a value for this variable. So we need to ensure that the computer finds the length of the tail of the list before it tries to add one to it in order to obtain the length of the original list. The commonly available (sequential) Prolog, which executes these subclauses in left-to-right order, forces the ordering that I gave upon us. The program would crash if the order was reversed.  Page 121So, there's an example of why Prolog is not quite logic programming, and why programming in Prolog does not quite allow you to forget con-trol-structuring details. There are a few other reasons why the popular equation:programming in Prolog = programming in logicis not a true statement of equality. But just as long as you are aware of this popular misconception, and no longer view Prolog as a total mystery, then you will be in a better position to assess the merit of the software engineering proposals that emanate from the logic-programming world.Having set the scene, I am now in a position to present and discuss the operational requirements approach to software engineering as advocated by Kowalski. In the proposals that I am about to present, he argues that "artificial intelligence technology [i.e. logic 