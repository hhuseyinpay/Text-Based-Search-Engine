4.6 Summary
The main theme of this chapter was how to build big networks by interconnecting
smaller networks. We looked at bridging in the last chapter, but it is a technique
that is mostly used to interconnect a small to moderate number of similar networks.
What bridging does not do well is tackle the two closely related problems of building
very large networks: heterogeneity and scale. The Internet Protocol is the key tool for
dealing with these problems, and it provided most of the examples for this chapter.
IP tackles heterogeneity by defining a simple, common service model for an
internetwork, which is based on the best-effort delivery of IP datagrams. An important
part of the service model is the global addressing scheme, which enables any two nodes
in an internetwork to uniquely identify each other for the purposes of exchanging data.
The IP service model is simple enough to be supported by any known networking
technology, and the ARP mechanism is used to translate global IP addresses into local
link-layer addresses.
A crucial aspect of the operation of an internetwork is the determination of
efficient routes to any destination in the internet. Internet routing algorithms solve
this problem in a distributed fashion; this chapter introduced the two major classes of
algorithms—link-state and distance-vector—along with examples of their application
(RIP and OSPF). We also examined the extensions to IP routing that will support
mobile hosts.
We then saw a succession of scaling problems and the ways that IP deals with
them. The major scaling issues are the efficient use of address space and the growth
of routing tables as the Internet grows. The hierarchical IP address format, with its
network and host parts, gives us one level of hierarchy to manage scale. Subnetting
lets us make more efficient use of network numbers and helps consolidate routing
information; in effect, it adds one more level of hierarchy to the address. Classless
routing (CIDR) lets us introduce more levels of hierarchy and achieve further routing
Open Issue: Deployment of IPv6 353
aggregation. Autonomous systems allow us to partition the routing problem into two
parts, interdomain and intradomain routing, each of which is much smaller than the
total routing problem would be. These mechanisms have enabled today’s Internet to
sustain remarkable growth.
Eventually, all of these mechanisms will be unable to keep up with the Internet’s
growth, and a new address format will be needed. This will require a new IP datagram
format and a new version of the protocol. Originally known as IP Next Generation
(IPng), this new protocol is now known as IPv6, and it provides a 128-bit address with
CIDR-like addressing and routing. While many new capabilities have been claimed for
IPv6, its main advantage remains its ability to support an extremely large number of
addressable devices.
More than 10 years have elapsed
since the shortage of IPv4 address
space became serious enough to warrant
proposals for a new version of
IP. The orginal IPv6 specification is
now more than 7 years old. IPv6-
capable host operating systems are
O P E N I S S U E
Deployment of IPv6
now widely available and the major router vendors offer varying degrees of support
for IPv6 in their products. Yet the deployment of IPv6 in the Internet has not, at the
time of writing, begun in any meaningful way. It is worth wondering when deployment
is likely to begin in earnest, and what will cause it.
One reason why IPv6 has not been needed sooner is because of the extensive use
of NAT (network address translation, described earlier in this chapter). As providers
viewed IPv4 addresses as a scarce resource, they handed out fewer of them to their
customers or charged for the number of addresses used; customers responded by hiding
many of their devices behind a NAT box and a single IPv4 address. For example, it
is likely that most home networks with more than one IP-capable device have some
sort of NAT in the network to conserve addresses. So one factor that might drive IPv6
deployment would be applications that don’t work well with NAT. While client/server
applications work reasonably well when the client’s address is “hidden” behind a NAT
box, peer-to-peer applications fare less well. Examples of applications that would work
better without NAT and would therefore benefit from more liberal address allocation
policies are multiplayer gaming and IP telephony.
Obtaining blocks of IPv4 addresses has been getting more difficult for years, and
this is particularly noticeable in countries outside the United States. As the difficulty
increases, the incentive for providers to start offering IPv6 addresses to their customers
354 4 Internetworking
also rises. At the same time, for existing providers, offering IPv6 is a substantial additional
cost because they don’t get to stop supporting IPv4 when they start to offer IPv6.
This means, for example, that the size of a provider’s routing tables can only increase
initially because they need to carry all the existing IPv4 prefixes plus new IPv6 prefixes.
At the moment, IPv6 deployment is happening almost exclusively in research
networks. A few service providers are starting to offer it (often with some incentive
from national governments). It seems hard to imagine that the Internet can continue to
grow indefinitely without IPv6 seeing some more significant deployments, but it also
seems likely that the overwhelming majority of hosts and networks will be IPv4-only
for the foreseeable future.
F U R T H E R R E A D I N G
Not surprisingly, there have been countless papers written on various aspects of the
Internet. Of these, we recommend two as must reading: The paper by Cerf and Kahn
is the one that originally introduced the TCP/IP architecture and is worth reading just
for its historical perspective; the paper by Bradner and Mankin gives an informative
overview of how the rapidly growing Internet has stressed the scalability of the original
architecture, ultimately resulting in the next-generation IP. The paper by Paxson
describes a study of how routers behave in the Internet. It also happens to be a good
example of how researchers are now studying the dynamic behavior of the Internet.
The final paper discusses multicast, presenting the approach to multicast originally
used on the MBone.
¦ Cerf, V., and R. Kahn. A protocol for packet network intercommunication.
IEEE Transactions on Communications COM-22(5):637–648, May
1974.
¦ Bradner, S., and A. Mankin. The recommendation for the next generation IP
protocol. Request for Comments 1752, January 1995.
¦ Paxson, V. End-to-end routing behavior in the Internet. SIGCOMM ’96,
pages 25–38, August 1996.
¦ Deering, S., and D. Cheriton. Multicast routing in datagram internetworks
and extended LANs. ACM Transactions on Computer Systems 8(2):85–110,
May 1990.
Beyond these papers, Perlman gives an excellent explanation of routing in an
internet, including coverage of both bridges and routers [Per00]. Also, the book by
Lynch and Rose gives general information on the scalability of the Internet [Cha93].
Exercises 355
Some interesting experimental studies of the behavior of Internet routing are presented
in Labovitz et al. [LAAJ00].
Many of the techniques and protocols developed to help the Internet scale are
described in RFCs: Subnetting is described in Mogul and Postel [MP85], CIDR is
described in Fuller et al. [FLYV93], RIP is defined in Hedrick [Hed88] and Malkin
[Mal93], OSPF is defined in Moy [Moy98], and BGP-4 is defined in Rekhter and Li
[RL95]. The OSPF specification, at over 200 pages, is one of the longer RFCs around,
but also contains an unusual wealth of detail about how to implement a protocol. A
collection of RFCs related to IPv6 can be found in Bradner and Mankin [BM95], and
the most recent IPv6 spec is by Deering and Hinden [DH98]. The reasons to avoid IP
fragmentation are examined in Kent and Mogul [KM87], and the path MTU discovery
technique is described in Mogul and Deering [MD90]. Protocol Independent Multicast
(PIM) is described in Deering et al. [DEF+96] and [EFH+98].
There has been a lot of work developing algorithms that can be used by routers to
do fast lookup of IP addresses. (Recall that the problem is that the router needs to match
the longest prefix in the forwarding table.) PATRICIA trees are one of the first algorithms
applied to this problem [Mor68]. More recent work is reported in [DBCP97],
[WVTP97], [LS98], and [SVSW98]. For an overview of how these algorithms can be
used to build a high-speed router, see Partridge et al. [Par98].
MPLS and the related protocols that fed its development are described in Chandranmenon
and Varghese [CV95], Rekhter et al. [RDR+97], and Davie and Rekhter
[DR00]. The latter reference describes many applications of MPLS such as traffic
engineering, fast recovery from network failures, and virtual private networks. [RR99]
provides the specification of MPLS/BGP VPNs, a form of layer 3 VPN that can be provided
over MPLS networks.
Finally, we recommend the following live references:
¦ http://www.ietf.org: the IETF home page, from which you can get RFCs, Internet
drafts, and working group charters
¦ http://playground.sun.com/pub/ipng/html/ipng-main.html:current state of IPv6
E X E R C I S E S
1 What aspect of IP addresses makes it necessary to have one address per network
interface, rather than just one per host? In light of your answer, why does IP
tolerate point-to-point interfaces that have nonunique addresses or no addresses?
2 Why does the Offset field in the IP header measure the offset in 8-byte units? (Hint:
Recall that the Offset field is 13 bits long.)
356 4 Internetworking
3 Some signalling errors can cause entire ranges of bits in a packet to be overwritten
by all 0s or all 1s. Suppose all the bits in the packet including the Internet checksum
are overwritten. Could a packet with all 0s or all 1s be a legal IPv4 packet? Will
the Internet checksum catch that error? Why or why not?
4 Suppose a TCP message that contains 2048 bytes of data and 20 bytes of TCP
header is passed to IP for delivery across two networks of the Internet (i.e., from
the source host to a router to the destination host). The first network uses 14-byte
headers and has an MTU of 1024 bytes; the second uses 8-byte headers with an
MTU of 512 bytes. Each network’s MTU gives the size of the largest IP datagram
that can be carried in a link-layer frame. Give the sizes and offsets of the sequence
of fragments delivered to the network layer at the destination host. Assume all IP
headers are 20 bytes.
5 PathMTUis the smallestMTUof any link on the current path (route) between two
hosts. Assume we could discover the path MTU of the path used in the previous
exercise, and that we use this value as the MTU for all the path segments. Give
the sizes and offsets of the sequence of fragments delivered to the network layer
at the destination host.
6 Suppose an IP packet is fragmented into 10 fragments, each with a 1% (independent)
probability of loss. To a reasonable approximation, this means there
is a 10% chance of losing the whole packet due to loss of a fragment. What
is the probability of net loss of the whole packet if the packet is transmitted
twice,
(a) assuming all fragments received must have been part of the same transmission?
(b) assuming any given fragment may have been part of either transmission?
(c) Explain how use of the Ident field might be applicable here.
7 Suppose the fragments of Figure 4.5(b) all pass through another router onto a
link with an MTU of 380 bytes, not counting the link header. Show the fragments
produced. If the packet were originally fragmented for this MTU, how many
fragments would be produced?
8 What is the maximum bandwidth at which an IP host can send 576-byte packets
without having the Ident field wrap around within 60 seconds? Suppose IP’s maximum
segment lifetime (MSL) is 60 seconds; that is, delayed packets can arrive
up to 60 seconds late but no later. What might happen if this bandwidth were
exceeded?
Exercises 357
9 ATM AAL3/4 uses fields Btag/Etag, BASize/Len, Type, SEQ, MID, Length, and
CRC-10 to implement fragmentation into cells. IPv4 uses Ident, Offset, and the M
bit in Flags, among others. What is the IP analog, if any, for each AAL3/4 field?
Does each IP field listed here have an AAL3/4 analog? How well do these fields
correspond?
10 Why do you think IPv4 has fragment reassembly done at the endpoint, rather than
at the next router? Why do you think IPv6 abandoned fragmentation entirely?
Hint: Think about the differences between IP-layer fragmentation and link-layer
fragmentation.
11 Having ARP table entries time out after 10–15 minutes is an attempt at a reasonable
compromise. Describe the problems that can occur if the timeout value is too
small or too large.
12 IP currently uses 32-bit addresses. If we could redesign IP to use the 6-byte MAC
address instead of the 32-bit address, would we be able to eliminate the need for
ARP? Explain why or why not.
13 Suppose hosts A and B have been assigned the same IP address on the same Ethernet,
on which ARP is used. B starts up after A. What will happen to A’s existing
connections? Explain how “self-ARP” (querying the network on startup for one’s
own IP address) might help with this problem.
14 Suppose an IP implementation adheres literally to the following algorithm on
receipt of a packet, P, destined for IP address D:
if (Ethernet address for D is in ARP cache)
send P
else
send out an ARP query for D
put P into a queue until the response comes back
(a) If the IP layer receives a burst of packets destined for D, how might this
algorithm waste resources unnecessarily?
(b) Sketch an improved version.
(c) Suppose we simply drop P, after sending out a query, when cache lookup
fails. How would this behave? (Some early ARP implementations allegedly did
this.)
358 4 Internetworking
2
3
6
2
8 1
D
A
F
E
B
C
Figure 4.48 Network for Exercises 15, 17, and 20.
A B E
D C F
2 1
2 3
5 2 3
Figure 4.49 Network for Exercise 16.
15 For the network given in Figure 4.48, give global distance-vector tables like those
of Tables 4.5 and 4.8 when
(a) each node knows only the distances to its immediate neighbors.
(b) each node has reported the information it had in the preceding step to its
immediate neighbors.
(c) step (b) happens a second time.
16 For the network given in Figure 4.49, give global distance-vector tables like those
of Tables 4.5 and 4.8 when
(a) each node knows only the distances to its immediate neighbors.
(b) each node has reported the information it had in the preceding step to its
immediate neighbors.
(c) step (b) happens a second time.
17 For the network given in Figure 4.48, show how the link-state algorithm builds
the routing table for node D.
Exercises 359
A F
Node Cost NextHop Node Cost NextHop
B 1 B A 3 E
C 2 B B 2 C
D 1 D C 1 C
E 2 B D 2 E
F 3 D E 1 E
Table 4.12 Forwarding tables for Exercise 18.
A F
Node Cost NextHop Node Cost NextHop
B 1 B A 2 C
C 1 C B 3 C
D 2 B C 1 C
E 3 C D 2 C
F 2 C E 1 E
Table 4.13 Forwarding tables for Exercise 19.
18 Suppose we have the forwarding tables shown in Table 4.12 for nodes A and F,
in a network where all links have cost 1. Give a diagram of the smallest network
consistent with these tables.
19 Suppose we have the forwarding tables shown in Table 4.13 for nodes A and F,
in a network where all links have cost 1. Give a diagram of the smallest network
consistent with these tables.
20 For the network in Figure 4.48, suppose the forwarding tables are all established
as in Exercise 15 and then the C–E link fails. Give
(a) the tables of A, B, D, and F after C and E have reported the news.
(b) the tables of A and D after their next mutual exchange.
(c) the table of C after A exchanges with it.
360 4 Internetworking
SubnetNumber SubnetMask NextHop
128.96.39.0 255.255.255.128 Interface 0
128.96.39.128 255.255.255.128 Interface 1
128.96.40.0 255.255.255.128 R2
192.4.153.0 255.255.255.192 R3
Default R4
Table 4.14 Routing table for Exercise 21.
SubnetNumber SubnetMask NextHop
128.96.170.0 255.255.254.0 Interface 0
128.96.168.0 255.255.254.0 Interface 1
128.96.166.0 255.255.254.0 R2
128.96.164.0 255.255.252.0 R3
Default R4
Table 4.15 Routing table for Exercise 22.
21 Suppose a router has built up the routing table shown in Table 4.14. The router
can deliver packets directly over interfaces 0 and 1, or it can forward packets to
routers R2, R3, or R4. Describe what the router does with a packet addressed to
each of the following destinations:
(a) 128.96.39.10
(b) 128.96.40.12
(c) 128.96.40.151
(d) 192.4.153.17
(e) 192.4.153.90
22 Suppose a router has built up the routing table shown in Table 4.15. The router
can deliver packets directly over interfaces 0 and 1, or it can forward packets to
routers R2, R3, or R4. Assume the router does the longest prefix match. Describe
Exercises 361
E A B
Figure 4.50 Simple network for Exercise 23.
what the router does with a packet addressed to each of the following destinations:
(a) 128.96.171.92
(b) 128.96.167.151
(c) 128.96.163.151
(d) 128.96.169.192
(e) 128.96.165.121
23 Consider the simple network in Figure 4.50, in which A and B exchange distancevector
routing information. All links have cost 1. Suppose the A–E link fails.
(a) Give a sequence of routing table updates that leads to a routing loop between
A and B.
(b) Estimate the probability of the scenario in (a), assuming A and B send out
routing updates at random times, each at the same average rate.
(c) Estimate the probability of a loop forming if A broadcasts an updated report
within 1 second of discovering the A–E failure, and B broadcasts every 60
seconds uniformly.
24 Consider the situation involving the creation of a routing loop in the network
of Figure 4.15 when the A–E link goes down. List all sequences of table updates
among A, B, and C, pertaining to destination E, that lead to the loop. Assume that
table updates are done one at a time, that the split horizon technique is observed
by all participants, and that A sends its initial report of E’s unreachability to B
before C. You may ignore updates that don’t result in changes.
25 Suppose a set of routers all use the split horizon technique; we consider here under
what circumstances it makes a difference if they use poison reverse in addition.
(a) Show that poison reverse makes no difference in the evolution of the routing
loop in the two examples described in Section 4.2.2, given that the hosts
involved use split horizon.
(b) Suppose split horizon routers A and B somehow reach a state in which they
forward traffic for a given destination X toward each other. Describe how this
situation will evolve with and without the use of poison reverse.
362 4 Internetworking
E A B
D
and E A B
Figure 4.51 Networks for Exercise 26.
A
C
B F G
Figure 4.52 Network for Exercise 27.
(c) Give a sequence of events that leads A and B to a looped state as in (b), even
if poison reverse is used. Hint: Suppose B and A connect through a very slow
link. They each reach X through a third node, C, and simultaneously advertise
their routes to each other.
26 Hold-down is another distance-vector loop-avoidance technique, whereby hosts
ignore updates for a period of time until link failure news has had a chance to
propagate. Consider the networks in Figure 4.51, where all links have cost 1,
except E–D with cost 10. Suppose that the E–A link breaks and B reports its loopforming
E route to A immediately afterward (this is the false route, via A). Specify
the details of a hold-down interpretation, and use this to describe the evolution
of the routing loop in both networks. To what extent can hold down prevent the
loop in the EAB network without delaying the discovery of the alternative route
in the EABD network?
27 Consider the network in Figure 4.52, using link-state routing. Suppose the B–F
link fails, and the following then occur in sequence:
(a) Node H is added to the right side with a connection to G.
(b) Node D is added to the left side with a connection to C.
(c) A new link D–A is added.
The failed B–F link is now restored. Describe what link-state packets will flood
back and forth. Assume that the initial sequence number at all nodes is 1, and
Exercises 363
A B C
D E
5
5 4
2 2 2 1
Figure 4.53 Network for Exercise 28.
A D E
B C
6
5 3
1 3 1 1
Figure 4.54 Network for Exercise 29.
A
B
C
Figure 4.55 Network for Exercise 30.
that no packets time out, and that both ends of a link use the same sequence
number in their LSP for that link, greater than any sequence number either used
before.
28 Give the steps as in Table 4.9 in the forward search algorithm as it builds the
routing database for node A in the network shown in Figure 4.53.
29 Give the steps as in Table 4.9 in the forward search algorithm as it builds the
routing database for node A in the network shown in Figure 4.54.
30 Suppose that nodes in the network shown in Figure 4.55 participate in link-state
routing, and C receives contradictory LSPs: One from A arrives claiming the A–B
link is down, but one from B arrives claiming the A–B link is up.
(a) How could this happen?
(b) What should C do? What can C expect?
Do not assume that LSPs contain any synchronized timestamp.
364 4 Internetworking
A
1 2
3
B
4
Provider Q
Provider P
Provider R
Figure 4.56 Network for Exercise 31.
31 Consider the network shown in Figure 4.56, in which horizontal lines represent
transit providers and numbered vertical lines are interprovider links.
(a) How many routes to P could provider Q’s BGP speakers receive?
(b) Suppose Q and P adopt the policy that outbound traffic is routed to the closest
link to the destination’s provider, thus minimizing their own cost. What paths
will traffic from host A to host B and from host B to host A take?
(c) What could Q do to have the B-›A traffic use the closer link 1?
(d) What could Q do to have the B-›A traffic pass through R?
32 Give an example of an arrangement of routers grouped into autonomous systems
so that the path with the fewest hops from a point A to another point B crosses
the same AS twice. Explain what BGP would do with this situation.
33 Let A be the number of autonomous systems on the Internet, and let D (for
diameter) be the maximum AS path length.
(a) Give a connectivity model for which Dis of order log Aand another for which
D is of order ?A.
(b) Assuming each AS number is 2 bytes and each network number is 4 bytes,
give an estimate for the amount of data a BGP speaker must receive to keep
track of the AS path to every network. Express your answer in terms of A, D,
and the number of networks N.
34 Suppose IP routers learned about IP networks and subnets the way Ethernet learning
bridges learn about hosts: by noting the appearance of new ones and the interface
by which they arrive. Compare this with existing distance-vector router
learning
(a) for a leaf site with a single attachment to the Internet, and
(b) for internal use at an organization that did not connect to the Internet.
Exercises 365
Rest of Internet
A
C R1 RB R2 D
B
Figure 4.57 Site for Exercise 39.
Assume that routers only receive new-network notices from other routers, and that
the originating routers receive their IP network information via configuration.
35 IP hosts that are not designated routers are required to drop packets misaddressed
to them, even if they would otherwise be able to forward them correctly. In the
absence of this requirement, what would happen if a packet addressed to IP address
A were inadvertently broadcast at the link layer? What other justifications for this
requirement can you think of?
36 Read the man page or other documentation for the Unix/Windows utility netstat.
Use netstat to display the current IP routing table on your host. Explain the purpose
of each entry. What is the practical minimum number of entries?
37 Use the Unix utility traceroute (Windows tracert) to determine how many
hops it is from your host to other hosts in the Internet (e.g., cs.princeton.edu or
www.cisco.com). How many routers do you traverse just to get out of your local
site? Read the man page or other documentation for traceroute and explain how
it is implemented.
38 What will happen if traceroute is used to find the path to an unassigned address?
Does it matter if the network portion or only the host portion is unassigned?
39 A site is shown in Figure 4.57. R1 and R2 are routers; R2 connects to the outside
world. Individual LANs are Ethernets. RB is a bridge router; it routes traffic
addressed to it and acts as a bridge for other traffic. Subnetting is used inside the
site; ARP is used on each subnet. Unfortunately, host A has been misconfigured
and doesn’t use subnets. Which of B, C, D can A reach?
366 4 Internetworking
C
A B
Figure 4.58 Network for Exercise 41.
40 An organization has a class C network 200.1.1 and wants to form subnets for
four departments, with hosts as follows:
A 72 hosts
B 35 hosts
C 20 hosts
D 18 hosts
There are 145 hosts in all.
(a) Give a possible arrangement of subnet masks to make this possible.
(b) Suggest what the organization might do if department D grows to 34 hosts.
41 Suppose hosts A and B are on an Ethernet LAN with class C IP network address
200.0.0. It is desired to attach a host C to the network via a direct connection
to B (see Figure 4.58). Explain how to do this with subnets; give sample subnet
assignments. Assume that an additional network address is not available. What
does this do to the size of the Ethernet LAN?
42 An alternative method for connecting host C in Exercise 41 is to use proxy ARP
and routing: B agrees to route traffic to and from C and also answers ARP queries
for C received over the Ethernet.
(a) Give all packets sent, with physical addresses, as A uses ARP to locate and
then send one packet to C.
(b) Give B’s routing table. What peculiarity must it contain?
43 Propose a plausible addressing plan for IPv6 that runs out of bits. Specifically,
provide a diagram such as Figure 4.33, perhaps with additional ID fields, that
adds up to more than 128 bits, together with plausible justifications for the size
of each field. You may assume fields are divided on byte boundaries and that
Exercises 367
NetMaskLength NextHop
C4.50.0.0/12 A
C4.5E.10.0/20 B
C4.60.0.0/12 C
C4.68.0.0/14 D
80.0.0.0/1 E
40.0.0.0/2 F
00.0.0.0/2 G
Table 4.16 Routing table for Exercise 45.
the InterfaceID is 64 bits. Hint: Consider fields that would approach maximum
allocation only under unusual circumstances. Can you do this if the InterfaceID is
48 bits?
44 Suppose two subnets share the same physical LAN; hosts on each subnet will see
the other subnet’s broadcast packets.
(a) How will DHCP fare if two servers, one for each subnet, coexist on the shared
LAN? What problems might [do!] arise?
(b) Will ARP be affected by such sharing?
45 Table 4.16 is a routing table using CIDR. Address bytes are in hexadecimal.
The notation “/12” in C4.50.0.0/12 denotes a netmask with 12 leading 1 bits,
that is, FF.F0.0.0. Note that the last three entries cover every address and thus
serve in lieu of a default route. State to what next hop the following will be
delivered.
(a) C4.5E.13.87
(b) C4.5E.22.09
(c) C3.41.80.02
(d) 5E.43.91.12
(e) C4.6D.31.2E
(f) C4.6B.31.2E
368 4 Internetworking
NetMaskLength NextHop
C4.5E.2.0/23 A
C4.5E.4.0/22 B
C4.5E.C0.0/19 C
C4.5E.40.0/18 D
C4.4C.0.0/14 E
C0.0.0.0/2 F
80.0.0.0/1 G
Table 4.17 Routing table for Exercise 46.
46 Table 4.17 is a routing table using CIDR. Address bytes are in hexadecimal. The
notation “/12” in C4.50.0.0/12 denotes a netmask with 12 leading 1 bits, that is,
FF.F0.0.0. State to what next hop the following will be delivered:
(a) C4.4B.31.2E
(b) C4.5E.05.09
(c) C4.4D.31.2E
(d) C4.5E.03.87
(e) C4.5E.7F.12
(f) C4.5E.D1.02
47 Suppose P, Q, and R are network service providers, with respective CIDR
address allocations (using the notation of Exercise 45) C1.0.0.0/8, C2.0.0.0/8,
and C3.0.0.0/8. Each provider’s customers initially receive address allocations
that are a subset of the provider’s. P has the following customers:
PA, with allocation C1.A3.0.0/16, and
PB, with allocation C1.B0.0.0/12.
Q has the following customers:
QA, with allocation C2.0A.10.0/20, and
QB, with allocation C2.0B.0.0/16.
Assume there are no other providers or customers.
Exercises 369
(a) Give routing tables for P, Q, and R, assuming each provider connects to both
of the others.
(b) Now assume P is connected to Q and Q is connected to R, but P and R are
not directly connected. Give tables for P and R.
(c) Suppose customer PA acquires a direct link to Q, and QA acquires a direct
link to P, in addition to existing links. Give tables for P and Q, ignoring R.
48 In the previous problem, assume each provider connects to both others. Suppose
customer PA switches to provider Q and customer QB switches to provider R.
Use the CIDR longest match rule to give routing tables for all three providers that
allow PA and QB to switch without renumbering.
49 Suppose most of the Internet uses some form of geographical addressing, but that
a large international organization has a single IP network address and routes its
internal traffic over its own links.
(a) Explain the routing inefficiency for the organization’s inbound traffic inherent
in this situation.
(b) Explain how the organization might solve this problem for outbound traffic.
(c) For your method above to work for inbound traffic, what would have to
happen?
(d) Suppose the large organization now changes its addressing to separate geographical
addresses for each office. What will its internal routing structure
have to look like if internal traffic is still to be routed internally?
50 The telephone system uses geographical addressing. Why do you think this wasn’t
adopted as a matter of course by the Internet?
51 Suppose a site A is multihomed, in that it has two Internet connections from two
different providers, P and Q. Provider-based addressing as in Exercise 47 is used,
and A takes its address assignment from P. Q has a CIDR longest match routing
entry for A.
(a) Describe what inbound traffic might flow on the A–Q connection. Consider
cases where Q does and does not advertise A to the world using BGP.
(b) What is the minimum advertising of its route to A that Q must do in order for
all inbound traffic to reach A via Q if the P–A link breaks?
(c) What problems must be overcome if A is to use both links for its outbound
traffic?
370 4 Internetworking
52 An ISP with a class B address is working with a new company to allocate it a
portion of address space based on CIDR. The new company needs IP addresses
for machines in three divisions of its corporate network: Engineering, Marketing,
and Sales. These divisions plan to grow as follows: Engineering has 5 machines
as of the start of year 1 and intends to add 1 machine every week; Marketing
will never need more than 16 machines; and Sales needs 1 machine for every
two clients. As of the start of year 1, the company has no clients, but the sales
model indicates that by the start of year 2, the company will have six clients
and each week thereafter gets one new client with probability 60%, loses one
client with probability 20%, or maintains the same number with probability
20%.
(a) What address range would be required to support the company’s growth plans
for at least seven years if marketing uses all 16 of its addresses and the sales
and engineering plans behave as expected?
(b) How long would this address assignment last? At the time when the company
runs out of address space, how would the addresses be assigned to the three
groups?
(c) If CIDR addressing were not available for the seven-year plan, what options
would the new company have in terms of getting address space?
53 Propose a lookup algorithm for a CIDR fowarding table that does not require a
linear search of the entire table to find the longest match.
54 Suppose a network N within a larger organization A acquires its own direct connection
to an Internet service provider, in addition to an existing connection via
A. Let R1 be the router connecting N to its own provider, and let R2 be the router
connecting N to the rest of A.
(a) Assuming N remains a subnet of A, how should R1 and R2 be configured?
What limitations would still exist with N’s use of its separate connection?
Would A be prevented from using N’s connection? Specify your configuration
in terms of what R1 and R2 should advertise, and with what paths. Assume
a BGP-like mechanism is available.
(b) Now suppose N gets its own network number; how does this change your
answer in (a)?
(c) Describe a router configuration that would allow A to use N’s link when its
own link is down.
Exercises 371
R1
R2
R3 R4 R5
R6 R7
D
E
Figure 4.59 Example internet for Exercise 55.
55 Consider the example internet shown in Figure 4.59, in which sources D and E
send packets to multicast group G, whose members are shaded in gray. Show the
shortest-path multicast trees for each source.
56 Consider the example internet shown in Figure 4.60 in which sources S1 and S2
send packets to multicast group G, whose members are shaded in gray. Show the
shortest-path multicast trees for each source.
57 Suppose host A is sending to a multicast group; the recipients are leaf nodes of
a tree rooted at A with depth N and with each nonleaf node having k children;
there are thus kN recipients.
(a) How many individual link transmissions are involved if A sends a multicast
message to all recipients?
372 4 Internetworking
R1
S1
S2
R2
R4 R5
R6
R7
R8
Figure 4.60 Example network for Exercise 56.
(b) How many individual link transmissions are involved if A sends unicast messages
to each individual recipient?
(c) Suppose A sends to all recipients, but some messages are lost and retransmission
is necessary. Unicast retransmissions to what fraction of the recipients is
equivalent, in terms of individual link transmissions, to a multicast retransmission
to all recipients?
58 Determine whether or not the following IPv6 address notations are correct.
(a) ::0F53:6382:AB00:67DB:BB27:7332
(b) 7803:42F2:::88EC:D4BA:B75D:11CD
(c) ::4BA8:95CC::DB97:4EAB
(d) 74DC::02BA
(e) ::00FF:128.112.92.116
Exercises 373
59 Determine if your site is connected to the MBone. If so, investigate and experiment
with any MBone tools, such as sdr, vat, and vic.
60 MPLS labels are usually 20 bits long. Explain why this provides enough labels
when MPLS is used for destination-based forwarding.
61 MPLS has sometimes been claimed to improve router performance. Explain why
this might be true, and suggest reasons why in practice this may not be the case.
62 Assume that it takes 32 bits to carry each MPLS label that is added to a packet
when the “shim” header of Figure 4.42(b) is used.
(a) How many additional bytes are needed to tunnel a packet using the MPLS
techniques described in Section 4.5.3?
(b) How many additional bytes are needed, at a minimum, to tunnel a packet
using an additional IP header as described in Section 4.1.8?
(c) Calculate the efficiency of bandwidth usage for each of the two tunneling
approaches when the average packet size is 300 bytes. Repeat for 64-byte
packets. Bandwidth efficiency is defined as (payload bytes carried) ÷ (total
bytes carried).
63 RFC 791 describes the Internet Protocol and includes two options for source
routing. Describe three disadvantages of using IP source route options compared
to using MPLS for explicit routing. (Hint: The IP header including options may
be at most 15 words long.)
End-to-End Protocols
Victory is the beautiful, bright coloured flower. Transport is the
stem without which it could never have blossomed.
—Winston Churchill
The previous three chapters have described various technologies that can be
used to connect together a collection of computers: direct links (including
LAN technologies like Ethernet and token ring), packet-switched networks
(including cell-based networks like ATM), and internetworks. The next problem is to
turn this host-to-host packet delivery service into a process-to-process communication
P R O B L E M
Getting Processes to
Communicate
channel. This is the role played by the
transport level of the network architecture,
which, because it supports
communication between the end
application programs, is sometimes
called the end-to-end protocol.
Two forces shape the end-to-end
protocol. From above, the application-level processes that use its services have certain
requirements. The following list itemizes some of the common properties that a
transport protocol can be expected to provide:
¦ guarantees message delivery
¦ delivers messages in the same order they are sent
¦ delivers at most one copy of each message
¦ supports arbitrarily large messages
¦ supports synchronization between the sender and the receiver
¦ allows the receiver to apply flow control to the sender
¦ supports multiple application processes on each host
5 Note that this list does not include all the functionality
that application processes might want from the network.
For example, it does not include security, which is typically
provided by protocols that sit above the transport level.
From below, the underlying network upon which the
transport protocol operates has certain limitations in the
level of service it can provide. Some of the more typical
limitations of the network are that it may
¦ drop messages
¦ reorder messages
¦ deliver duplicate copies of a given message
¦ limit messages to some finite size
¦ deliver messages after an arbitrarily long delay
Such a network is said to provide a best-effort level of
service, as exemplified by the Internet.
The challenge, therefore, is to develop algorithms
that turn the less-than-desirable properties of the underlying
network into the high level of service required by application
programs. Different transport protocols employ
different combinations of these algorithms. This chapter
looks at these algorithms in the context of three representative
services—a simple asynchronous demultiplexing
service, a reliable byte-stream service, and a request/reply
service.
In the case of the demultiplexing and byte-stream
services, we use the Internet’s UDP and TCP protocols,
respectively, to illustrate how these services are provided
in practice. In the third case, we first give a collection of
algorithms that implement the request/reply (plus other related)
services and then show how these algorithms can be
combined to implement a Remote Procedure Call (RPC)
protocol. This discussion is capped off with a description
of two widely used RPC protocols—SunRPC and DCERPC—
in terms of these component algorithms. Finally,
the chapter concludes with a section that discusses the
performance of the different transport protocols.
376 5 End-to-End Protocols
5.1 Simple Demultiplexer (UDP)
The simplest possible transport protocol is one that extends the host-to-host delivery
service of the underlying network into a process-to-process communication service.
There are likely to be many processes running on any given host, so the protocol needs
to add a level of demultiplexing, thereby allowing multiple application processes on
each host to share the network. Aside from this requirement, the transport protocol
adds no other functionality to the best-effort service provided by the underlying network.
The Internet’s User Datagram Protocol (UDP) is an example of such a transport
protocol.
The only interesting issue in such a protocol is the form of the address used to
identify the target process. Although it is possible for processes to directly identify
each other with an OS-assigned process id (pid), such an approach is only practical
in a closed distributed system in which a single OS runs on all hosts and assigns each
process a unique id. A more common approach, and the one used by UDP, is for
processes to indirectly identify each other using an abstract locator, often called a port
or mailbox. The basic idea is for a source process to send a message to a port and for
the destination process to receive the message from a port.
The header for an end-to-end protocol that implements this demultiplexing function
typically contains an identifier (port) for both the sender (source) and the receiver
(destination) of the message. For example, the UDP header is given in Figure 5.1. Notice
that the UDP port field is only 16 bits long. This means that there are up to 64K possible
ports, clearly not enough to identify all the processes on all the hosts in the Internet.
Fortunately, ports are not interpreted across the entire Internet, but only on a single
host. That is, a process is really identified by a port on some particular host—a port,
host pair. In fact, this pair constitutes the demultiplexing key for the UDP protocol.
The next issue is how a process learns the port for the process to which it wants
to send a message. Typically, a client process initiates a message exchange with a server
SrcPort DstPort
Length Checksum
Data
0 16 31
Figure 5.1 Format for UDP header.
5.1 Simple Demultiplexer (UDP) 377
process. Once a client has contacted a server, the server knows the client’s port (it was
contained in the message header) and can reply to it. The real problem, therefore, is how
the client learns the server’s port in the first place. A common approach is for the server
to accept messages at a well-known port. That is, each server receives its messages at
some fixed port that is widely published, much like the emergency telephone service
available at the well-known phone number 911. In the Internet, for example, the
Domain Name Server (DNS) receives messages at well-known port 53 on each host,
the mail service listens for messages at port 25, and the Unix talk program accepts
messages at well-known port 517, and so on. This mapping is published periodically
in an RFC and is available on most Unix systems in file /etc/services. Sometimes a
well-known port is just the starting point for communication: The client and server
use the well-known port to agree on some other port that they will use for subsequent
communication, leaving the well-known port free for other clients.
An alternative strategy is to generalize this idea, so that there is only a single
well-known port—the one at which the “Port Mapper” service accepts messages. A
client would send a message to the Port Mapper’s well-known port asking for the
port it should use to talk to the “whatever” service, and the Port Mapper returns
the appropriate port. This strategy makes it easy to change the port associated with
different services over time, and for each host to use a different port for the same
service.
As just mentioned, a port is purely an abstraction. Exactly how it is implemented
differs from system to system, or more precisely, from OS to OS. For example, the
socket API described in Chapter 1 is an implementation of ports. Typically, a port is
implemented by a message queue, as illustrated in Figure 5.2. When a message arrives,
the protocol (e.g., UDP) appends the message to the end of the queue. Should the
queue be full, the message is discarded. There is no flow-control mechanism that tells
the sender to slow down. When an application process wants to receive a message,
one is removed from the front of the queue. If the queue is empty, the process blocks
until a message becomes available.
Finally, although UDP does not implement flow control or reliable/ordered delivery,
it does a little more work than to simply demultiplex messages to some application
process—it also ensures the correctness of the message by the use of a checksum. (The
UDP checksum is optional in the current Internet, but it will become mandatory with
IPv6.) UDP computes its checksum over the UDP header, the contents of the message
body, and something called the pseudoheader. The pseudoheader consists of three fields
from the IP header—protocol number, source IP address, and destination IP address—
plus the UDP length field. (Yes, the UDP length field is included twice in the checksum
calculation.) UDP uses the same checksum algorithm as IP, as defined in Section 2.4.2.
The motivation behind having the pseudoheader is to verify that this message has been
378 5 End-to-End Protocols
Application
process
Application
process
Application
process
UDP
Packets arrive
Ports
Queues
Packets
demultiplexed
Figure 5.2 UDP message queue.
delivered between the correct two endpoints. For example, if the destination IP address
was modified while the packet was in transit, causing the packet to be misdelivered,
this fact would be detected by the UDP checksum.
5.2 Reliable Byte Stream (TCP)
In contrast to a simple demultiplexing protocol like UDP, a more sophisticated transport
protocol is one that offers a reliable, connection-oriented, byte-stream service.
Such a service has proven useful to a wide assortment of applications because it frees
the application from having to worry about missing or reordered data. The Internet’s
Transmission Control Protocol (TCP) is probably the most widely used protocol of
this type; it is also the most carefully tuned. It is for these two reasons that this section
studies TCP in detail, although we identify and discuss alternative design choices at
the end of the section.
In terms of the properties of transport protocols given in the problem statement
at the start of this chapter, TCP guarantees the reliable, in-order delivery of a stream
of bytes. It is a full-duplex protocol, meaning that each TCP connection supports a
5.2 Reliable Byte Stream (TCP) 379
pair of byte streams, one flowing in each direction. It also includes a flow-control
mechanism for each of these byte streams that allows the receiver to limit how much
data the sender can transmit at a given time. Finally, like UDP, TCP supports a demultiplexing
mechanism that allows multiple application programs on any given host
to simultaneously carry on a conversation with their peers. In addition to the above
features, TCP also implements a highly tuned congestion-control mechanism. The idea
of this mechanism is to throttle how fast TCP sends data, not for the sake of keeping
the sender from overrunning the receiver, but to keep the sender from overloading
the network. A description of TCP’s congestion-control mechanism is postponed until
Chapter 6, where we discuss it in the larger context of how network resources are
fairly allocated.
? Since many people confuse congestion control and flow control, we restate the
difference. Flow control involves preventing senders from overrunning the capacity of
receivers. Congestion control involves preventing too much data from being injected
into the network, thereby causing switches or links to become overloaded. Thus, flow
control is an end-to-end issue, while congestion control is concerned with how hosts
and networks interact.
5.2.1 End-to-End Issues
At the heart of TCP is the sliding window algorithm. Even though this is the same basic
algorithm we saw in Section 2.5.2, because TCP runs over the Internet rather than a
point-to-point link, there are many important differences. This subsection identifies
these differences and explains how they complicate TCP. The following subsections
then describe how TCP addresses these and other complications.
First, whereas the sliding window algorithm presented in Section 2.5.2 runs over a
single physical link that always connects the same two computers, TCP supports logical
connections between processes that are running on any two computers in the Internet.
This means that TCP needs an explicit connection establishment phase during which
the two sides of the connection agree to exchange data with each other. This difference
is analogous to having to dial up the other party, rather than having a dedicated phone
line. TCP also has an explicit connection teardown phase. One of the things that
happens during connection establishment is that the two parties establish some shared
state to enable the sliding window algorithm to begin. Connection teardown is needed
so each host knows it is OK to free this state.
Second, whereas a single physical link that always connects the same two computers
has a fixed RTT, TCP connections are likely to have widely different round-trip
times. For example, a TCP connection between a host in San Francisco and a host
in Boston, which are separated by several thousand kilometers, might have an RTT
380 5 End-to-End Protocols
of 100 ms, while a TCP connection between two hosts in the same room, only a few
meters apart, might have an RTT of only 1 ms. The same TCP protocol must be able
to support both of these connections. To make matters worse, the TCP connection
between hosts in San Francisco and Boston might have an RTT of 100 ms at 3 a.m.,
but an RTT of 500 ms at 3 p.m. Variations in the RTT are even possible during a
single TCP connection that lasts only a few minutes. What this means to the sliding
window algorithm is that the timeout mechanism that triggers retransmissions must be
adaptive. (Certainly, the timeout for a point-to-point link must be a settable parameter,
but it is not necessary to adapt this timer for a particular pair of nodes.)
A third difference is that packets may be reordered as they cross the Internet,
but this is not possible on a point-to-point link where the first packet put into one
end of the link must be the first to appear at the other end. Packets that are slightly
out of order do not cause a problem since the sliding window algorithm can reorder
packets correctly using the sequence number. The real issue is how far out-of-order
packets can get, or said another way, how late a packet can arrive at the destination.
In the worst case, a packet can be delayed in the Internet until IP’s time to live (TTL)
field expires, at which time the packet is discarded (and hence there is no danger of
it arriving late). Knowing that IP throws packets away after their TTL expires, TCP
assumes that each packet has a maximum lifetime. The exact lifetime, known as the
maximum segment lifetime (MSL), is an engineering choice. The current recommended
setting is 120 seconds. Keep in mind that IP does not directly enforce this 120-second
value; it is simply a conservative estimate that TCP makes of how long a packet might
live in the Internet. The implication is significant—TCP has to be prepared for very old
packets to suddenly show up at the receiver, potentially confusing the sliding window
algorithm.
Fourth, the computers connected to a point-to-point link are generally engineered
to support the link. For example, if a link’s delay × bandwidth product is computed
to be 8 KB—meaning that a window size is selected to allow up to 8 KB of data to be
unacknowledged at a given time—then it is likely that the computers at either end of
the link have the ability to buffer up to 8 KB of data. Designing the system otherwise
would be silly. On the other hand, almost any kind of computer can be connected to the
Internet, making the amount of resources dedicated to any one TCP connection highly
variable, especially considering that any one host can potentially support hundreds of
TCP connections at the same time. This means that TCP must include a mechanism
that each side uses to “learn” what resources (e.g., how much buffer space) the other
side is able to apply to the connection. This is the flow-control issue.
Fifth, because the transmitting side of a directly connected link cannot send any
faster than the bandwidth of the link allows, and only one host is pumping data into
the link, it is not possible to unknowingly congest the link. Said another way, the load
5.2 Reliable Byte Stream (TCP) 381
on the link is visible in the form of a queue of packets at the sender. In contrast, the
sending side of a TCP connection has no idea what links will be traversed to reach
the destination. For example, the sending machine might be directly connected to a
relatively fast Ethernet—and so, capable of sending data at a rate of 100 Mbps—but
somewhere out in the middle of the network, a 1.5-Mbps T1 link must be traversed.
And to make matters worse, data being generated by many different sources might be
trying to traverse this same slow link. This leads to the problem of network congestion.
Discussion of this topic is delayed until Chapter 6.
We conclude this discussion of end-to-end issues by comparing TCP’s approach to
providing a reliable/ordered delivery service with the approach used by X.25 networks.
In TCP, the underlying IP network is assumed to be unreliable and to deliver messages
out of order; TCP uses the sliding window algorithm on an end-to-end basis to provide
reliable/ordered delivery. In contrast, X.25 networks use the sliding window protocol
within the network, on a hop-by-hop basis. The assumption behind this approach is
that if messages are delivered reliably and in order between each pair of nodes along
the path between the source host and the destination host, then the end-to-end service
also guarantees reliable/ordered delivery.
The problem with this latter approach is that a sequence of hop-by-hop guarantees
does not necessarily add up to an end-to-end guarantee. First, if a heterogeneous
link (say, an Ethernet) is added to one end of the path, then there is no guarantee
that this hop will preserve the same service as the other hops. Second, just because
the sliding window protocol guarantees that messages are delivered correctly from
node A to node B, and then from node B to node C, it does not guarantee that node B
behaves perfectly. For example, network nodes have been known to introduce errors
into messages while transferring them from an input buffer to an output buffer. They
have also been known to accidentally reorder messages. As a consequence of these
small windows of vulnerability, it is still necessary to provide true end-to-end checks
to guarantee reliable/ordered service, even though the lower levels of the system also
implement that functionality.
? This discussion serves to illustrate one of the most important principles in system
design—the end-to-end argument. In a nutshell, the end-to-end argument says that a
function (in our example, providing reliable/ordered delivery) should not be provided
in the lower levels of the system unless it can be completely and correctly implemented
at that level. Therefore, this rule argues in favor of the TCP/IP approach. This rule is
not absolute, however. It does allow for functions to be incompletely provided at a
low level as a performance optimization. This is why it is perfectly consistent with the
end-to-end argument to perform error detection (e.g., CRC) on a hop-by-hop basis;
detecting and retransmitting a single corrupt packet across one hop is preferable to
having to retransmit an entire file end-to-end.
382 5 End-to-End Protocols
Application process
Write
bytes
TCP
Send buffer
Segment Segment Segment
Transmit segments
Application process
Read
bytes
TCP
Receive buffer
…
…
…
Figure 5.3 How TCP manages a byte stream.
5.2.2 Segment Format
TCP is a byte-oriented protocol, which means that the sender writes bytes into a TCP
connection and the receiver reads bytes out of the TCP connection. Although “byte
stream” describes the service TCP offers to application processes, TCP does not, itself,
transmit individual bytes over the Internet. Instead, TCP on the source host buffers
enough bytes from the sending process to fill a reasonably sized packet and then sends
this packet to its peer on the destination host. TCP on the destination host then empties
the contents of the packet into a receive buffer, and the receiving process reads from
this buffer at its leisure. This situation is illustrated in Figure 5.3, which, for simplicity,
shows data flowing in only one direction. Remember that, in general, a single TCP
connection supports byte streams flowing in both directions.
The packets exchanged between TCP peers in Figure 5.3 are called segments,
since each one carries a segment of the byte stream. Each TCP segment contains the
header schematically depicted in Figure 5.4. The relevance of most of these fields will
become apparent throughout this section. For now, we simply introduce them.
The SrcPort and DstPort fields identify the source and destination ports, respectively,
just as in UDP. These two fields, plus the source and destination IP addresses,
combine to uniquely identify each TCP connection. That is, TCP’s demux key is given
by the 4-tuple
 SrcPort, SrcIPAddr, DstPort, DstIPAddr 
Note that because TCP connections come and go, it is possible for a connection between
a particular pair of ports to be established, used to send and receive data, and
closed, and then at a later time for the same pair of ports to be involved in a second
5.2 Reliable Byte Stream (TCP) 383
Options (variable)
Data
Checksum
SrcPort DstPort
HdrLen 0 Flags
UrgPtr
AdvertisedWindow
SequenceNum
Acknowledgment
0 4 10 16 31
Figure 5.4 TCP header format.
Sender
Data (SequenceNum)
Acknowledgment +
AdvertisedWindow
Receiver
Figure 5.5 Simplified illustration (showing only one direction) of the TCP process,
with data flow in one direction and ACKs in the other.
connection. We sometimes refer to this situation as two different incarnations of the
same connection.
The Acknowledgment, SequenceNum, and AdvertisedWindow fields are all involved
in TCP’s sliding window algorithm. Because TCP is a byte-oriented protocol,
each byte of data has a sequence number; the SequenceNum field contains the sequence
number for the first byte of data carried in that segment. The Acknowledgment and
AdvertisedWindow fields carry information about the flow of data going in the other
direction. To simplify our discussion, we ignore the fact that data can flow in both
directions, and we concentrate on data that has a particular SequenceNum flowing
in one direction and Acknowledgment and AdvertisedWindow values flowing in the
opposite direction, as illustrated in Figure 5.5. The use of these three fields is described
more fully in Section 5.2.4.
The 6-bit Flags field is used to relay control information between TCP peers. The
possible flags include SYN, FIN, RESET, PUSH, URG, and ACK. The SYN and FIN flags
384 5 End-to-End Protocols
are used when establishing and terminating a TCP connection, respectively. Their use
is described in Section 5.2.3. The ACK flag is set any time the Acknowledgment field is
valid, implying that the receiver should pay attention to it. The URG flag signifies that
this segment contains urgent data. When this flag is set, the UrgPtr field indicates where
the nonurgent data contained in this segment begins. The urgent data is contained at
the front of the segment body, up to and including a value of UrgPtr bytes into the
segment. The PUSH flag signifies that the sender invoked the push operation, which
indicates to the receiving side of TCP that it should notify the receiving process of
this fact. We discuss these last two features more in Section 5.2.7. Finally, the RESET
flag signifies that the receiver has become confused—for example, because it received
a segment it did not expect to receive—and so wants to abort the connection.
Finally, the Checksum field is used in exactly the same way as for UDP—it is
computed over the TCP header, the TCP data, and the pseudoheader, which is made
up of the source address, destination address, and length fields from the IP header. The
checksum is required for TCP in both IPv4 and IPv6. Also, since the TCP header is of
variable length (options can be attached after the mandatory fields), a HdrLen field is
included that gives the length of the header in 32-bit words. This field is also known
as the Offset field, since it measures the offset from the start of the packet to the start
of the data.
5.2.3 Connection Establishment and Termination
A TCP connection begins with a client (caller) doing an active open to a server (callee).
Assuming that the server had earlier done a passive open, the two sides engage in
an exchange of messages to establish the connection. (Recall from Chapter 1 that a
party wanting to initiate a connection performs an active open, while a party willing
to accept a connection does a passive open.) Only after this connection establishment
phase is over do the two sides begin sending data. Likewise, as soon as
a participant is done sending data, it closes one direction of the connection, which
causes TCP to initiate a round of connection termination messages. Notice that while
connection setup is an asymmetric activity (one side does a passive open and the
other side does an active open), connection teardown is symmetric (each side has to
close the connection independently).1 Therefore, it is possible for one side to have
done a close, meaning that it can no longer send data, but for the other side to
keep the other half of the bidirectional connection open and to continue sending
data.
1To be more precise, connection setup can be symmetric, with both sides trying to open the connection at the same
time, but the common case is for one side to do an active open and the other side to do a passive open.
5.2 Reliable Byte Stream (TCP) 385
Active participant
(client)
Passive participant
(server)
SYN, SequenceNum = x
SYN + ACK, SequenceNum = y,
ACK, Acknowledgment = y + 1
Acknowledgment = x + 1
Figure 5.6 Timeline for three-way handshake algorithm.
Three-Way Handshake
The algorithm used by TCP to establish and terminate a connection is called a three-
way handshake.We first describe the basic algorithm and then show how it is used by
TCP. The three-way handshake involves the exchange of three messages between the
client and the server, as illustrated by the timeline given in Figure 5.6.
The idea is that two parties want to agree on a set of parameters, which, in the
case of opening a TCP connection, are the starting sequence numbers the two sides plan
to use for their respective byte streams. In general, the parameters might be any facts
that each side wants the other to know about. First, the client (the active participant)
sends a segment to the server (the passive participant) stating the initial sequence
number it plans to use (Flags = SYN, SequenceNum = x). The server then responds
with a single segment that both acknowledges the client’s sequence number (Flags =
ACK, Ack = x + 1) and states its own beginning sequence number (Flags = SYN,
SequenceNum = y). That is, both the SYN and ACK bits are set in the Flags field of this
second message. Finally, the client responds with a third segment that acknowledges
the server’s sequence number (Flags = ACK, Ack = y + 1). The reason that each
side acknowledges a sequence number that is one larger than the one sent is that
the Acknowledgment field actually identifies the “next sequence number expected,”
thereby implicitly acknowledging all earlier sequence numbers. Although not shown
in this timeline, a timer is scheduled for each of the first two segments, and if the
expected response is not received, the segment is retransmitted.
You may be asking yourself why the client and server have to exchange starting
sequence numbers with each other at connection setup time. It would be simpler if
each side simply started at some “well-known” sequence number, such as 0. In fact,
386 5 End-to-End Protocols
the TCP specification requires that each side of a connection select an initial starting
sequence number at random. The reason for this is to protect against two incarnations
of the same connection reusing the same sequence numbers too soon, that is, while
there is still a chance that a segment from an earlier incarnation of a connection might
interfere with a later incarnation of the connection.
State Transition Diagram
TCP is complex enough that its specification includes a state transition diagram. A
copy of this diagram is given in Figure 5.7. This diagram shows only the states involved
in opening a connection (everything above ESTABLISHED) and in closing a
connection (everything below ESTABLISHED). Everything that goes on while a connection
is open—that is, the operation of the sliding window algorithm—is hidden in
the ESTABLISHED state.
CLOSED
LISTEN
SYN_RCVD SYN_SENT
ESTABLISHED
CLOSE_WAIT
CLOSING LAST_ACK
TIME_WAIT
FIN_WAIT_2
FIN_WAIT_1
Passive open Close
Send/SYN
SYN/SYN + ACK
SYN + ACK/ACK
SYN/SYN + ACK
ACK
Close/FIN
Close/FIN FIN/ACK
FIN/ACK
ACK + FIN/ACK
Timeout after two
segment lifetimes
FIN/ACK
ACK
ACK
ACK
Close/FIN
Close
CLOSED
Active open/SYN
Figure 5.7 TCP state transition diagram.
5.2 Reliable Byte Stream (TCP) 387
TCP’s state transition diagram is fairly easy to understand. Each circle denotes
a state that one end of a TCP connection can find itself in. All connections start in the
CLOSED state. As the connection progresses, the connection moves from state to state
according to the arcs. Each arc is labelled with a tag of the form event/action. Thus, if
a connection is in the LISTEN state and a SYN segment arrives (i.e., a segment with
the SYN flag set), the connection makes a transition to the SYN RCVD state and takes
the action of replying with an ACK + SYN segment.
Notice that two kinds of events trigger a state transition: (1) a segment arrives
from the peer (e.g., the event on the arc from LISTEN to SYN RCVD), or (2) the local
application process invokes an operation on TCP (e.g., the active open event on the arc
from CLOSE to SYN SENT). In other words, TCP’s state transition diagram effectively
defines the semantics of both its peer-to-peer interface and its service interface, as
defined in Section 1.3.1. The syntax of these two interfaces is given by the segment
format (as illustrated in Figure 5.4) and by some application programming interface
(an example of which is given in Section 1.4.1), respectively.
Now let’s trace the typical transitions taken through the diagram in Figure 5.7.
Keep in mind that at each end of the connection, TCP makes different transitions
from state to state. When opening a connection, the server first invokes a passive open
operation on TCP, which causes TCP to move to the LISTEN state. At some later time,
the client does an active open, which causes its end of the connection to send a SYN
segment to the server and to move to the SYN SENT state. When the SYN segment
arrives at the server, it moves to the SYN RCVD state and responds with a SYN+ACK
segment. The arrival of this segment causes the client to move to the ESTABLISHED
state and to send an ACK back to the server. When this ACK arrives, the server finally
moves to the ESTABLISHED state. In other words, we have just traced the three-way
handshake.
There are three things to notice about the connection establishment half of the
state transition diagram. First, if the client’s ACK to the server is lost, corresponding to
the third leg of the three-way handshake, then the connection still functions correctly.
This is because the client side is already in the ESTABLISHED state, so the local
application process can start sending data to the other end. Each of these data segments
will have the ACK flag set, and the correct value in the Acknowledgment field, so the
server will move to the ESTABLISHED state when the first data segment arrives.
This is actually an important point about TCP—every segment reports what sequence
number the sender is expecting to see next, even if this repeats the same sequence
number contained in one or more previous segments.
The second thing to notice about the state transition diagram is that there is a
funny transition out of the LISTEN state whenever the local process invokes a send
operation on TCP. That is, it is possible for a passive participant to identify both ends
388 5 End-to-End Protocols
of the connection (i.e., itself and the remote participant that it is willing to have connect
to it), and then to change its mind about waiting for the other side and instead actively
establish the connection. To the best of our knowledge, this is a feature of TCP that
no application process actually takes advantage of.
The final thing to notice about the diagram is the arcs that are not shown. Specifically,
most of the states that involve sending a segment to the other side also schedule
a timeout that eventually causes the segment to be resent if the expected response does
not happen. These retransmissions are not depicted in the state transition diagram. If
after several tries the expected response does not arrive, TCP gives up and returns to
the CLOSED state.
Turning our attention now to the process of terminating a connection, the important
thing to keep in mind is that the application process on both sides of the
connection must independently close its half of the connection. If only one side closes
the connection, then this means it has no more data to send, but it is still available
to receive data from the other side. This complicates the state transition diagram because
it must account for the possibility that the two sides invoke the close operator
at the same time, as well as the possibility that first one side invokes close and then,
at some later time, the other side invokes close. Thus, on any one side there are three
combinations of transitions that get a connection from the ESTABLISHED state to the
CLOSED state:
¦ This side closes first:
ESTABLISHED › FIN WAIT 1 › FIN WAIT 2 › TIME WAIT ›
CLOSED.
¦ The other side closes first:
ESTABLISHED › CLOSE WAIT › LAST ACK › CLOSED.
¦ Both sides close at the same time:
ESTABLISHED › FIN WAIT 1 › CLOSING › TIME WAIT ›
CLOSED.
There is actually a fourth, although rare, sequence of transitions that leads to the
CLOSED state; it follows the arc from FIN WAIT 1 to TIME WAIT.We leave it as an
exercise for you to figure out what combination of circumstances leads to this fourth
possibility.
The main thing to recognize about connection teardown is that a connection in
the TIME WAIT state cannot move to the CLOSED state until it has waited for two
times the maximum amount of time an IP datagram might live in the Internet (i.e.,
120 seconds). The reason for this is that while the local side of the connection has
sent an ACK in response to the other side’s FIN segment, it does not know that the
ACK was successfully delivered. As a consequence, the other side might retransmit its
5.2 Reliable Byte Stream (TCP) 389
FIN segment, and this second FIN segment might be delayed in the network. If the
connection were allowed to move directly to the CLOSED state, then another pair of
application processes might come along and open the same connection (i.e., use the
same pair of port numbers), and the delayed FIN segment from the earlier incarnation
of the connection would immediately initiate the termination of the later incarnation
of that connection.
5.2.4 Sliding Window Revisited
We are now ready to discuss TCP’s variant of the sliding window algorithm, which
serves several purposes: (1) it guarantees the reliable delivery of data, (2) it ensures
that data is delivered in order, and (3) it enforces flow control between the sender
and the receiver. TCP’s use of the sliding window algorithm is the same as we saw in
Section 2.5.2 in the case of the first two of these three functions. Where TCP differs
from the earlier algorithm is that it folds the flow-control function in as well. In
particular, rather than having a fixed-size sliding window, the receiver advertises a
window size to the sender. This is done using the AdvertisedWindow field in the TCP
header. The sender is then limited to having no more than a value of AdvertisedWindow
bytes of unacknowledged data at any given time. The receiver selects a suitable value
for AdvertisedWindow based on the amount of memory allocated to the connection
for the purpose of buffering data. The idea is to keep the sender from overrunning the
receiver’s buffer. We discuss this at greater length below.
Reliable and Ordered Delivery
To see how the sending and receiving sides of TCP interact with each other to implement
reliable and ordered delivery, consider the situation illustrated in Figure 5.8.
TCP on the sending side maintains a send buffer. This buffer is used to store data
Sending application
LastByteWritten
TCP
LastByteAcked LastByteSent
Receiving application
LastByteRead
TCP
NextByteExpected LastByteRcvd
(a) (b)
Figure 5.8 Relationship between TCP send buffer (a) and receive buffer (b).
390 5 End-to-End Protocols
that has been sent but not yet acknowledged, as well as data that has been written by
the sending application, but not transmitted. On the receiving side, TCP maintains a
receive buffer. This buffer holds data that arrives out of order, as well as data that is
in the correct order (i.e., there are no missing bytes earlier in the stream) but that the
application process has not yet had the chance to read.
To make the following discussion simpler to follow, we initially ignore the fact
that both the buffers and the sequence numbers are of some finite size and hence will
eventually wrap around. Also, we do not distinguish between a pointer into a buffer
where a particular byte of data is stored and the sequence number for that byte.
Looking first at the sending side, three pointers are maintained into the send buffer,
each with an obvious meaning: LastByteAcked, LastByteSent, and LastByteWritten.
Clearly,
LastByteAcked ? LastByteSent
since the receiver cannot have acknowledged a byte that has not yet been sent, and
LastByteSent ? LastByteWritten
since TCP cannot send a byte that the application process has not yet written. Also
note that none of the bytes to the left of LastByteAcked need to be saved in the buffer
because they have already been acknowledged, and none of the bytes to the right of
LastByteWritten need to be buffered because they have not yet been generated.
A similar set of pointers (sequence numbers) are maintained on the receiving side:
LastByteRead, NextByteExpected, and LastByteRcvd. The inequalities are a little less intuitive,
however, because of the problem of out-of-order delivery. The first relationship
LastByteRead < NextByteExpected
is true because a byte cannot be read by the application until it is received and all preceding
bytes have also been received. NextByteExpected points to the byte immediately
after the latest byte to meet this criterion. Second,
NextByteExpected ? LastByteRcvd + 1
since, if data has arrived in order, NextByteExpected points to the byte after LastByte-
Rcvd, whereas if data has arrived out of order, NextByteExpected points to the start of
the first gap in the data, as in Figure 5.8. Note that bytes to the left of LastByteRead
need not be buffered because they have already been read by the local application
process, and bytes to the right of LastByteRcvd need not be buffered because they have
not yet arrived.
Flow Control
Most of the above discussion is similar to that found in Section 2.5.2; the only real
difference is that this time we elaborated on the fact that the sending and receiving application
processes are filling and emptying their local buffer, respectively. (The earlier
5.2 Reliable Byte Stream (TCP) 391
discussion glossed over the fact that data arriving from an upstream node was filling
the send buffer, and data being transmitted to a downstream node was emptying the
receive buffer.)
You should make sure you understand this much before proceeding because
now comes the point where the two algorithms differ more significantly. In what
follows, we reintroduce the fact that both buffers are of some finite size, denoted
MaxSendBuffer and MaxRcvBuffer, although we don’t worry about the details of how
they are implemented. In other words, we are only interested in the number of bytes
being buffered, not in where those bytes are actually stored.
Recall that in a sliding window protocol, the size of the window sets the amount
of data that can be sent without waiting for acknowledgment from the receiver. Thus,
the receiver throttles the sender by advertising a window that is no larger than the
amount of data that it can buffer. Observe that TCP on the receive side must keep
LastByteRcvd - LastByteRead ? MaxRcvBuffer
to avoid overflowing its buffer. It therefore advertises a window size of
AdvertisedWindow = MaxRcvBuffer - ((NextByteExpected - 1) - LastByteRead)
which represents the amount of free space remaining in its buffer. As data arrives,
the receiver acknowledges it as long as all the preceding bytes have also arrived. In
addition, LastByteRcvd moves to the right (is incremented), meaning that the advertised
window potentially shrinks. Whether or not it shrinks depends on how fast the local
application process is consuming data. If the local process is reading data just as fast as
it arrives (causing LastByteRead to be incremented at the same rate as LastByteRcvd),
then the advertised window stays open (i.e., AdvertisedWindow = MaxRcvBuffer).
If, however, the receiving process falls behind, perhaps because it performs a very
expensive operation on each byte of data that it reads, then the advertised window
grows smaller with every segment that arrives, until it eventually goes to 0.
TCP on the send side must then adhere to the advertised window it gets from
the receiver. This means that at any given time, it must ensure that
LastByteSent - LastByteAcked ? AdvertisedWindow
Said another way, the sender computes an effective window that limits how much data
it can send:
EffectiveWindow = AdvertisedWindow - (LastByteSent - LastByteAcked)
Clearly, EffectiveWindow must be greater than 0 before the source can send more data.
It is possible, therefore, that a segment arrives acknowledging x bytes, thereby allowing
the sender to increment LastByteAcked by x, but because the receiving process was not
reading any data, the advertised window is now x bytes smaller than the time before.
392 5 End-to-End Protocols
In such a situation, the sender would be able to free buffer space, but not to send any
more data.
All the while this is going on, the send side must also make sure that the local
application process does not overflow the send buffer, that is, that
LastByteWritten - LastByteAcked ? MaxSendBuffer
If the sending process tries to write y bytes to TCP, but
(LastByteWritten - LastByteAcked) + y > MaxSendBuffer
then TCP blocks the sending process and does not allow it to generate more data.
It is now possible to understand how a slow receiving process ultimately stops
a fast sending process. First, the receive buffer fills up, which means the advertised
window shrinks to 0. An advertised window of 0 means that the sending side cannot
transmit any data, even though data it has previously sent has been successfully acknowledged.
Finally, not being able to transmit any data means that the send buffer
fills up, which ultimately causes TCP to block the sending process. As soon as the
receiving process starts to read data again, the receive-side TCP is able to open its window
back up, which allows the send-side TCP to transmit data out of its buffer. When
this data is eventually acknowledged, LastByteAcked is incremented, the buffer space
holding this acknowledged data becomes free, and the sending process is unblocked
and allowed to proceed.
There is only one remaining detail that must be resolved—how does the sending
side know that the advertised window is no longer 0? As mentioned above, TCP always
sends a segment in response to a received data segment, and this response contains the
latest values for the Acknowledge and AdvertisedWindow fields, even if these values
have not changed since the last time they were sent. The problem is this. Once the
receive side has advertised a window size of 0, the sender is not permitted to send
any more data, which means it has no way to discover that the advertised window
is no longer 0 at some time in the future. TCP on the receive side does not spontaneously
send nondata segments; it only sends them in response to an arriving data
segment.
TCP deals with this situation as follows. Whenever the other side advertises a
window size of 0, the sending side persists in sending a segment with 1 byte of data
every so often. It knows that this data will probably not be accepted, but it tries
anyway, because each of these 1-byte segments triggers a response that contains the
current advertised window. Eventually, one of these 1-byte probes triggers a response
that reports a nonzero advertised window.
? Note that the reason the sending side periodically sends this probe segment is
that TCP is designed to make the receive side as simple as possible—it simply responds
5.2 Reliable Byte Stream (TCP) 393
to segments from the sender, and it never initiates any activity on its own. This is
an example of a well-recognized (although not universally applied) protocol design
rule, which, for lack of a better name, we call the smart sender/dumb receiver rule.
Recall that we saw another example of this rule when we discussed the use of NAKs
in Section 2.5.2.
Protecting against Wraparound
This subsection and the next consider the size of the SequenceNum and AdvertisedWindow
fields and the implications of their sizes on TCP’s correctness and performance.
TCP’s SequenceNum field is 32 bits long, and its AdvertisedWindow field is 16 bits
long, meaning that TCP has easily satisfied the requirement of the sliding window algorithm
that the sequence number space be twice as big as the window size: 232 » 2×216.
However, this requirement is not the interesting thing about these two fields. Consider
each field in turn.
The relevance of the 32-bit sequence number space is that the sequence number
used on a given connection might wrap around—a byte with sequence number x could
be sent at one time, and then at a later time a second byte with the same sequence
number x might be sent. Once again, we assume that packets cannot survive in the
Internet for longer than the recommended MSL. Thus, we currently need to make
sure that the sequence number does not wrap around within a 120-second period of
time. Whether or not this happens depends on how fast data can be transmitted over
the Internet, that is, how fast the 32-bit sequence number space can be consumed.
(This discussion assumes that we are trying to consume the sequence number space as
fast as possible, but of course we will be if we are doing our job of keeping the pipe
full.) Table 5.1 shows how long it takes for the sequence number to wrap around on
networks with various bandwidths.
As you can see, the 32-bit sequence number space is adequate for today’s networks,
but given that OC-48 links currently exist in the Internet backbone, it won’t
be long until individual TCP connections want to run at 622-Mbps speeds or higher.
Fortunately, the IETF has already worked out an extension to TCP that effectively
extends the sequence number space to protect against the sequence number wrapping
around. This and related extensions are described in Section 5.2.8.
Keeping the Pipe Full
The relevance of the 16-bit AdvertisedWindow field is that it must be big enough
to allow the sender to keep the pipe full. Clearly, the receiver is free not to open
the window as large as the AdvertisedWindow field allows; we are interested in the
situation in which the receiver has enough buffer space to handle as much data as the
largest possible AdvertisedWindow allows.
394 5 End-to-End Protocols
Bandwidth Time until Wraparound
T1 (1.5 Mbps) 6.4 hours
Ethernet (10 Mbps) 57 minutes
T3 (45 Mbps) 13 minutes
FDDI (100 Mbps) 6 minutes
STS-3 (155 Mbps) 4 minutes
STS-12 (622 Mbps) 55 seconds
STS-24 (1.2 Gbps) 28 seconds
Table 5.1 Time until 32-bit sequence number space wraps around.
Bandwidth Delay × Bandwidth Product
T1 (1.5 Mbps) 18 KB
Ethernet (10 Mbps) 122 KB
T3 (45 Mbps) 549 KB
FDDI (100 Mbps) 1.2 MB
STS-3 (155 Mbps) 1.8 MB
STS-12 (622 Mbps) 7.4 MB
STS-24 (1.2 Gbps) 14.8 MB
Table 5.2 Required window size for 100-ms RTT.
In this case, it is not just the network bandwidth but the delay × bandwidth
product that dictates how big the AdvertisedWindow field needs to be—the window
needs to be opened far enough to allow a full delay × bandwidth product’s worth of
data to be transmitted. Assuming an RTT of 100 ms (a typical number for a crosscountry
connection in the U.S.), Table 5.2 gives the delay × bandwidth product for
several network technologies.
As you can see, TCP’s AdvertisedWindow field is in even worse shape than its
SequenceNum field—it is not big enough to handle even a T3 connection across the
continental United States, since a 16-bit field allows us to advertise a window of only
64 KB. The very same TCP extension mentioned above (see Section 5.2.8) provides a
mechanism for effectively increasing the size of the advertised window.
5.2 Reliable Byte Stream (TCP) 395
5.2.5 Triggering Transmission
We next consider a surprisingly subtle issue: how TCP decides to transmit a segment. As
described earlier, TCP supports a byte-stream abstraction, that is, application programs
write bytes into the stream, and it is up to TCP to decide that it has enough bytes to
send a segment. What factors govern this decision?
If we ignore the possibility of flow control—that is, we assume the window is
wide open, as would be the case when a connection first starts—then TCP has three
mechanisms to trigger the transmission of a segment. First, TCP maintains a variable,
typically called the maximum segment size (MSS), and it sends a segment as soon as it
has collected MSS bytes from the sending process. MSS is usually set to the size of the
largest segment TCP can send without causing the local IP to fragment. That is, MSS
is set to the MTU of the directly connected network, minus the size of the TCP and IP
headers. The second thing that triggers TCP to transmit a segment is that the sending
process has explicitly asked it to do so. Specifically, TCP supports a push operation,
and the sending process invokes this operation to effectively flush the buffer of unsent
bytes. The final trigger for transmitting a segment is that a timer fires; the resulting
segment contains as many bytes as are currently buffered for transmission. However,
as we will soon see, this “timer” isn’t exactly what you expect.
Silly Window Syndrome
Of course, we can’t just ignore flow control, which plays an obvious role in throttling
the sender. If the sender has MSS bytes of data to send and the window is open at least
that much, then the sender transmits a full segment. Suppose, however, that the sender
is accumulating bytes to send, but the window is currently closed. Now suppose an
ACK arrives that effectively opens the window enough for the sender to transmit, say,
MSS/2 bytes. Should the sender transmit a half-full segment or wait for the window
to open to a full MSS? The original specification was silent on this point, and early
implementations of TCP decided to go ahead and transmit a half-full segment. After
all, there is no telling how long it will be before the window opens further.
It turns out that the strategy of aggressively taking advantage of any available
window leads to a situation now known as the silly window syndrome. Figure 5.9
helps visualize what happens. If you think of a TCP stream as a conveyer belt with
“full” containers (data segments) going in one direction and empty containers (ACKs)
going in the reverse direction, then MSS-sized segments correspond to large containers
and 1-byte segments correspond to very small containers. If the sender aggressively fills
an empty container as soon as it arrives, then any small container introduced into the
system remains in the system indefinitely. That is, it is immediately filled and emptied
at each end, and never coalesced with adjacent containers to create larger containers.
396 5 End-to-End Protocols
Sender Receiver
Figure 5.9 Silly window syndrome.
This scenario was discovered when early implementations of TCP regularly found
themselves filling the network with tiny segments.
Note that the silly window syndrome is only a problem when either the sender
transmits a small segment or the receiver opens the window a small amount. If neither
of these happens, then the small container is never introduced into the stream. It’s
not possible to outlaw sending small segments; for example, the application might
do a push after sending a single byte. It is possible, however, to keep the receiver
from introducing a small container (i.e., a small open window). The rule is that after
advertizing a zero window, the receiver must wait for space equal to an MSS before it
advertises an open window.
Since we can’t eliminate the possibility of a small container being introduced into
the stream, we also need mechanisms to coalesce them. The receiver can do this by
delaying ACKs—sending one combined ACK rather than multiple smaller ones—but
this is only a partial solution because the receiver has no way of knowing how long it is
safe to delay waiting either for another segment to arrive or for the application to read
more data (thus opening the window). The ultimate solution falls to the sender, which
brings us back to our original issue: When does the TCP sender decide to transmit a
segment?
Nagle’s Algorithm
Returning to the TCP sender, if there is data to send but the window is open less than
MSS, then we may want to wait some amount of time before sending the available
data, but the question is, how long? If we wait too long, then we hurt interactive
applications like Telnet. If we don’t wait long enough, then we risk sending a bunch
of tiny packets and falling into the silly window syndrome. The answer is to introduce
a timer and to transmit when the timer expires.
While we could use a clock-based timer—for example, one that fires every 100
ms—Nagle introduced an elegant self-clocking solution. The idea is that as long as TCP
has any data in flight, the sender will eventually receive an ACK. This ACK can be
5.2 Reliable Byte Stream (TCP) 397
treated like a timer firing, triggering the transmission of more data. Nagle’s algorithm
provides a simple, unified rule for deciding when to transmit:
When the application produces data to send
if both the available data and the window ? MSS
send a full segment
else
if there is unACKed data in flight
buffer the new data until an ACK arrives
else
send all the new data now
In other words, it’s always OK to send a full segment if the window allows.
It’s also OK to immediately send a small amount of data if there are currently no
segments in transit, but if there is anything in flight, the sender must wait for an ACK
before transmiting the next segment. Thus, an interactive application like Telnet that
continually writes one byte at a time will send data at a rate of one segment per RTT.
Some segments will contain a single byte, while others will contain as many bytes as
the user was able to type in one round-trip time. Because some applications cannot
afford such a delay for each write they do to a TCP connection, the socket interface
allows applications to turn off Nagle’s algorithm by setting the TCP NODELAY option.
Setting this option means that data is transmitted as soon as possible.
5.2.6 Adaptive Retransmission
Because TCP guarantees the reliable delivery of data, it retransmits each segment if an
ACK is not received in a certain period of time. TCP sets this timeout as a function of
the RTT it expects between the two ends of the connection. Unfortunately, given the
range of possible RTTs between any pair of hosts in the Internet, as well as the variation
in RTT between the same two hosts over time, choosing an appropriate timeout
value is not that easy. To address this problem, TCP uses an adaptive retransmission
mechanism. We now describe this mechanism and how it has evolved over time as the
Internet community has gained more experience using TCP.
Original Algorithm
We begin with a simple algorithm for computing a timeout value between a pair of
hosts. This is the algorithm that was originally described in the TCP specification—
and the following description presents it in those terms—but it could be used by any
end-to-end protocol.
The idea is to keep a running average of the RTT and then to compute the timeout
as a function of this RTT. Specifically, every time TCP sends a data segment, it records
398 5 End-to-End Protocols
the time. When an ACK for that segment arrives, TCP reads the time again and then
takes the difference between these two times as a SampleRTT. TCP then computes
an EstimatedRTT as a weighted average between the previous estimate and this new
sample. That is,
EstimatedRTT = ? × EstimatedRTT + (1 - ?) × SampleRTT
The parameter ? is selected to smooth the EstimatedRTT. A small ? tracks changes in
the RTT but is perhaps too heavily influenced by temporary fluctuations. On the other
hand, a large ? is more stable but perhaps not quick enough to adapt to real changes.
The original TCP specification recommended a setting of ? between 0.8 and 0.9. TCP
then uses EstimatedRTT to compute the timeout in a rather conservative way:
TimeOut = 2 × EstimatedRTT
Karn/Partridge Algorithm
After several years of use on the Internet, a rather obvious flaw was discovered in
this simple algorithm. The problem was that an ACK does not really acknowledge a
transmission; it actually acknowledges the receipt of data. In other words, whenever
a segment is retransmitted and then an ACK arrives at the sender, it is impossible to
determine if this ACK should be associated with the first or the second transmission
of the segment for the purpose of measuring the sample RTT. It is necessary to know
which transmission to associate it with so as to compute an accurate SampleRTT. As
illustrated in Figure 5.10, if you assume that the ACK is for the original transmission
but it was really for the second, then the SampleRTT is too large (a), while if you
assume that the ACK is for the second transmission but it was actually for the first,
then the SampleRTT is too small (b).
Sender Receiver
Original transmission
ACK
SampleRTT
Retransmission
Sender Receiver
Original transmission
ACK
SampleRTT
Retransmission
(a) (b)
Figure 5.10 Associating the ACK with (a) original transmission versus
(b) retransmission.
5.2 Reliable Byte Stream (TCP) 399
The solution is surprisingly simple. Whenever TCP retransmits a segment, it
stops taking samples of the RTT; it only measures SampleRTT for segments that have
been sent only once. This solution is known as the Karn/Partridge algorithm, after its
inventors. Their proposed fix also includes a second small change to TCP’s timeout
mechanism. Each time TCP retransmits, it sets the next timeout to be twice the last
timeout, rather than basing it on the last EstimatedRTT. That is, Karn and Partridge
proposed that TCP use exponential backoff, similar to what the Ethernet does. The
motivation for using exponential backoff is simple: Congestion is the most likely cause
of lost segments, meaning that the TCP source should not react too aggressively to a
timeout. In fact, the more times the connection times out, the more cautious the source
should become. We will see this idea again, embodied in a much more sophisticated
mechanism, in Chapter 6.
Jacobson/Karels Algorithm
The Karn/Partridge algorithm was introduced at a time when the Internet was suffering
from high levels of network congestion. Their approach was designed to fix some of
the causes of that congestion, and although it was an improvement, the congestion was
not eliminated. A couple of years later, two other researchers—Jacobson and Karels—
proposed a more drastic change to TCP to battle congestion. The bulk of that proposed
change is described in Chapter 6. Here, we focus on the aspect of that proposal that
is related to deciding when to time out and retransmit a segment.
As an aside, it should be clear how the timeout mechanism is related to
congestion—if you time out too soon, you may unnecessarily retransmit a segment,
which only adds to the load on the network. As we will see in Chapter 6, the other
reason for needing an accurate timeout value is that a timeout is taken to imply congestion,
which triggers a congestion-control mechanism. Finally, note that there is nothing
about the Jacobson/Karels timeout computation that is specific to TCP. It could be used
by any end-to-end protocol.
The main problem with the original computation is that it does not take the
variance of the sample RTTs into account. Intuitively, if the variation among samples
is small, then the EstimatedRTT can be better trusted and there is no reason for multiplying
this estimate by 2 to compute the timeout. On the other hand, a large variance
in the samples suggests that the timeout value should not be too tightly coupled to the
EstimatedRTT.
In the new approach, the sender measures a new SampleRTT as before. It then
folds this new sample into the timeout calculation as follows:
Difference = SampleRTT - EstimatedRTT
EstimatedRTT = EstimatedRTT + (? × Difference)
400 5 End-to-End Protocols
Deviation = Deviation + ?(|Difference| - Deviation)
where ? is a fraction between 0 and 1. That is, we calculate both the mean RTT and
the variation in that mean.
TCP then computes the timeout value as a function of both EstimatedRTT and
Deviation as follows:
TimeOut = µ × EstimatedRTT + ? × Deviation
where based on experience, µ is typically set to 1 and ? is set to 4. Thus, when
the variance is small, TimeOut is close to EstimatedRTT; a large variance causes the
Deviation term to dominate the calculation.
Implementation
There are two items of note regarding the implementation of timeouts in TCP. The
first is that it is possible to implement the calculation for EstimatedRTT and Deviation
without using floating-point arithmetic. Instead, the whole calculation is scaled by
2n, with ? selected to be 1/2n. This allows us to do integer arithmetic, implementing
multiplication and division using shifts, thereby achieving higher performance. The
resulting calculation is given by the following code fragment, where n = 3 (i.e., ? =
1/8). Note that EstimatedRTT and Deviation are stored in their scaled-up forms, while
the value of SampleRTT at the start of the code and of TimeOut at the end are real,
unscaled values. If you find the code hard to follow, you might want to try plugging
some real numbers into it and verifying that it gives the same results as the equations
above.
{
SampleRTT -= (EstimatedRTT >> 3);
EstimatedRTT += SampleRTT;
if (SampleRTT < 0)
SampleRTT = -SampleRTT;
SampleRTT -= (Deviation >> 3);
Deviation += SampleRTT;
TimeOut = (EstimatedRTT >> 3) + (Deviation >> 1);
}
The second point of note is that the Jacobson/Karels algorithm is only as good
as the clock used to read the current time. On a typical Unix implementation, the
clock granularity is as large as 500 ms, which is significantly larger than the average
cross-country RTT of somewhere between 100 and 200 ms. To make matters worse,
the Unix implementation of TCP only checks to see if a timeout should happen every
time this 500-ms clock ticks, and it only takes a sample of the round-trip time once per
5.2 Reliable Byte Stream (TCP) 401
RTT. The combination of these two factors quite often means that a timeout happens
1 second after the segment was transmitted. Once again, the extensions to TCP include
a mechanism that makes this RTT calculation a bit more precise.
5.2.7 Record Boundaries
Since TCP is a byte-stream protocol, the number of bytes written by the sender are
not necessarily the same as the number of bytes read by the receiver. For example,
the application might write 8 bytes, then 2 bytes, then 20 bytes to a TCP connection,
while on the receiving side, the application reads 5 bytes at a time inside a loop that
iterates 6 times. TCP does not interject record boundaries between the 8th and 9th
bytes, nor between the 10th and 11th bytes. This is in contrast to a message-oriented
protocol, such as UDP, in which the message that is sent is exactly the same length as
the message that is received.
Even though TCP is a byte-stream protocol, it has two different features that
can be used by the sender to insert record boundaries into this byte stream, thereby
informing the receiver how to break the stream of bytes into records. (Being able to
mark record boundaries is useful, for example, in many database applications.) Both
of these features were originally included in TCP for completely different reasons; they
have only come to be used for this purpose over time.
The first mechanism is the urgent data feature, as implemented by the URG flag
and the UrgPtr field in the TCP header. Originally, the urgent data mechanism was
designed to allow the sending application to send out-of-band data to its peer. By “out
of band” we mean data that is separate from the normal flow of data (e.g., a command
to interrupt an operation already under way). This out-of-band data was identified in
the segment using the UrgPtr field and was to be delivered to the receiving process as
soon as it arrived, even if that meant delivering it before data with an earlier sequence
number. Over time, however, this feature has not been used, so instead of signifying
“urgent” data, it has come to be used to signify “special” data, such as a record marker.
This use has developed because, as with the push operation, TCP on the receiving side
must inform the application that “urgent data” has arrived. That is, the urgent data
in itself is not important. It is the fact that the sending process can effectively send a
signal to the receiver that is important.
The second mechanism for inserting end-of-record markers into a byte is the push
operation. Originally, this mechanism was designed to allow the sending process to
tell TCP that it should send (flush) whatever bytes it had collected to its peer. The push
operation can be used to implement record boundaries because the specification says
that TCP must send whatever data it has buffered at the source when the application
says push, and optionally, TCP at the destination notifies the application whenever
an incoming segment has the PUSH flag set. If the receiving side supports this option
402 5 End-to-End Protocols
(the socket interface does not), then the push operation can be used to break the TCP
stream into records.
Of course, the application program is always free to insert record boundaries
without any assistance from TCP. For example, it can send a field that indicates the
length of a record that is to follow, or it can insert its own record boundary markers
into the data stream.
5.2.8 TCP Extensions
We have mentioned at three different points in this section that there are now extensions
to TCP that help to mitigate some problem that TCP is facing as the underlying
network gets faster. These extensions are designed to have as small an impact on TCP
as possible. In particular, they are realized as options that can be added to the TCP
header. (We glossed over this point earlier, but the reason that the TCP header has a
HdrLen field is that the header can be of variable length; the variable part of the TCP
header contains the options that have been added.) The significance of adding these
extensions as options rather than changing the core of the TCP header is that hosts
can still communicate using TCP even if they do not implement the options. Hosts that
do implement the optional extensions, however, can take advantage of them. The two
sides agree that they will use the options during TCP’s connection establishment phase.
The first extension helps to improve TCP’s timeout mechanism. Instead of measuring
the RTT using a coarse-grained event, TCP can read the actual system clock
when it is about to send a segment, and put this time—think of it as a 32-bit time-
stamp—in the segment’s header. The receiver then echoes this timestamp back to the
sender in its acknowledgment, and the sender subtracts this timestamp from the current
time to measure the RTT. In essence, the timestamp option provides a convenient
place for TCP to “store” the record of when a segment was transmitted; it stores
the time in the segment itself. Note that the endpoints in the connection do not need
synchronized clocks, since the timestamp is written and read at the same end of the
connection.
The second extension addresses the problem of TCP’s 32-bit SequenceNum field
wrapping around too soon on a high-speed network. Rather than define a new 64-bit
sequence number field, TCP uses the 32-bit timestamp just described to effectively
extend the sequence number space. In other words, TCP decides whether to accept or
reject a segment based on a 64-bit identifier that has the SequenceNum field in the
low-order 32 bits and the timestamp in the high-order 32 bits. Since the timestamp
is always increasing, it serves to distinguish between two different incarnations of the
same sequence number. Note that the timestamp is being used in this setting only to
protect against wraparound; it is not treated as part of the sequence number for the
purpose of ordering or acknowledging data.
5.2 Reliable Byte Stream (TCP) 403
The third extension allows TCP to advertise a larger window, thereby allowing
it to fill larger delay × bandwidth pipes that are made possible by high-speed
networks. This extension involves an option that defines a scaling factor for the advertised
window. That is, rather than interpreting the number that appears in the
AdvertisedWindow field as indicating how many bytes the sender is allowed to have
unacknowledged, this option allows the two sides of TCP to agree that the Advertised-
Window field counts larger chunks (e.g., how many 16-byte units of data the sender can
have unacknowledged). In other words, the window scaling option specifies how many
bits each side should left-shift the AdvertisedWindow field before using its contents to
compute an effective window.
5.2.9 Alternative Design Choices
Although TCP has proven to be a robust protocol that satisfies the needs of a wide
range of applications, the design space for transport protocols is quite large. TCP is,
by no means, the only valid point in that design space. We conclude our discussion of
TCP by considering alternative design choices. While we offer an explanation for why
TCP’s designers made the choices they did, we leave it to you to decide if there might
be a place for alternative transport protocols.
First, we have suggested from the very first chapter of this book that there are at
least two interesting classes of transport protocols: stream-oriented protocols like TCP
and request/reply protocols like RPC. In other words, we have implicitly divided the
design space in half and placed TCP squarely in the stream-oriented half of the world.
We could further divide the stream-oriented protocols into two groups—reliable and
unreliable—with the former containing TCP and the latter being more suitable for
interactive video applications that would rather drop a frame than incur the delay
associated with a retransmission.
This exercise in building a transport protocol taxonomy is interesting and could
be continued in greater and greater detail, but the world isn’t as black and white as we
might like. Consider the suitability of TCP as a transport protocol for request/reply
applications, for example. TCP is a full-duplex protocol, so it would be easy to open
a TCP connection between the client and server, send the request message in one
direction, and send the reply message in the other direction. There are two complications,
however. The first is that TCP is a byte-oriented protocol rather than a
message-oriented protocol, and request/reply applications always deal with messages.
(We explore the issue of bytes versus messages in greater detail in a moment.) The
second complication is that in those situations where both the request message and
the reply message fit in a single network packet, a well-designed request/reply protocol
needs only two packets to implement the exchange, whereas TCP would need at least
nine: three to establish the connection, two for the message exchange, and four to tear
404 5 End-to-End Protocols
down the connection. Of course, if the request or reply messages are large enough to
require multiple network packets (e.g., it might take 100 packets to send a 100,000-
byte reply message), then the overhead of setting up and tearing down the connection
is inconsequential. In other words, it isn’t always the case that a particular protocol
cannot support a certain functionality; it’s sometimes the case that one design is more
efficient than another under particular circumstances.
Second, as just suggested, you might question why TCP chose to provide a reliable
byte-stream service rather than a reliable message-stream service; messages would be
the natural choice for a database application that wants to exchange records. There are
two answers to this question. The first is that a message-oriented protocol must, by definition,
establish an upper bound on message sizes. After all, an infinitely long message
is a byte stream. For any message size that a protocol selects, there will be applications
that want to send larger messages, rendering the transport protocol useless and forcing
the application to implement its own transportlike services. The second reason is that
while message-oriented protocols are definitely more appropriate for applications that
want to send records to each other, you can easily insert record boundaries into a byte
stream to implement this functionality, as described in Section 5.2.7.
Third, TCP chose to implement explicit setup/teardown phases, but this is not
required. In the case of connection setup, it would certainly be possible to send all
necessary connection parameters along with the first data message. TCP elected to
take a more conservative approach that gives the receiver the opportunity to reject the
connection before any data arrives. In the case of teardown, we could quietly close a
connection that has been inactive for a long period of time, but this would complicate
applications like Telnet that want to keep a connection alive for weeks at a time; such
applications would be forced to send out-of-band “keepalive” messages to keep the
connection state at the other end from disappearing.
Finally, TCP is a window-based protocol, but this is not the only possibility.
The alternative is a rate-based design, in which the receiver tells the sender the rate—
expressed in either bytes or packets per second—at which it is willing to accept incoming
data. For example, the receiver might inform the sender that it can accommodate
100 packets a second. There is an interesting duality between windows and rate, since
the number of packets (bytes) in the window, divided by the RTT, is exactly the rate.
For example, a window size of 10 packets and a 100-ms RTT implies that the sender is
allowed to transmit at a rate of 100 packets a second. It is by increasing or decreasing
the advertised window size that the receiver is effectively raising or lowering the rate
at which the sender can transmit. In TCP, this information is fed back to the sender
in the AdvertisedWindow field of the ACK for every segment. One of the key issues in
a rate-based protocol is how often the desired rate—which may change over time—is
relayed back to the source: Is it for every packet, once per RTT, or only when the
5.3 Remote Procedure Call 405
rate changes? While we have just now considered window versus rate in the context
of flow control, it is an even more hotly contested issue in the context of congestion
control, which we will discuss in Chapter 6.
5.3 Remote Procedure Call
As discussed in Chapter 1, a common pattern of communication used by application
programs is the request/reply paradigm, also called message transaction: A client sends
a request message to a server, the server responds with a reply message, and the client
blocks (suspends execution) waiting for this response. Figure 5.11 illustrates the basic
interaction between the client and server in such a message transaction.
A transport protocol that supports the request/reply paradigm is much more than
a UDP message going in one direction, followed by a UDP message going in the other
direction. It also involves overcoming all of the limitations of the underlying network
outlined in the problem statement at the beginning of this chapter. While TCP overcomes
these limitations by providing a reliable byte-stream service, it doesn’t match
the request/reply paradigm very well either since going to the trouble of establishing a
TCP connection just to exchange a pair of messages seems like overkill. This section describes
a third transport protocol—which we call Remote Procedure Call (RPC)—that
more closely matches the needs of an application involved in a request/reply message
exchange.
RPC is actually more than just a protocol—it is a popular mechanism for structuring
distributed systems. RPC is popular because it is based on the semantics of a
local procedure call—the application program makes a call into a procedure without
regard for whether it is local or remote and blocks until the call returns. While this
may sound simple, there are two main problems that make RPC more complicated
than local procedure calls:
Client Server
Request
Reply
Computing
Blocked
Blocked
Blocked
Figure 5.11 Timeline for RPC.
406 5 End-to-End Protocols
¦ The network between the calling process and the called process has much
more complex properties than the backplane of a computer. For example, it is
likely to limit message sizes and has a tendency to lose and reorder messages.
¦ The computers on which the calling and called processes run may have significantly
different architectures and data representation formats.
Thus, a complete RPC mechanism actually involves two major components:
1 A protocol that manages the messages sent between the client and the server processes
and that deals with the potentially undesirable properties of the underlying
network
2 Programming language and compiler support to package the arguments into a
request message on the client machine and then to translate this message back
into the arguments on the server machine, and likewise with the return value
(this piece of the RPC mechanism is usually called a stub compiler)
Figure 5.12 schematically depicts what happens when a client invokes a remote
procedure. First, the client calls a local stub for the procedure, passing it the arguments
required by the procedure. This stub hides the fact that the procedure is remote by
Caller
(client)
Client
stub
RPC
protocol
Return
value
Arguments
Request Reply
Callee
(server)
Server
stub
RPC
protocol
Return
value
Arguments
Request Reply
Figure 5.12 Complete RPC mechanism.
5.3 Remote Procedure Call 407
translating the arguments into a request message and then invoking an RPC protocol
to send the request message to the server machine. At the server, the RPC protocol
delivers the request message to the server stub, which translates it into the arguments to
the procedure and then calls the local procedure. After the server procedure completes,
it returns the answer to the server stub, which packages this return value in a reply
message that it hands off to the RPC protocol for transmission back to the client. The
RPC protocol on the client passes this message up to the client stub, which translates
it into a return value that it returns to the client program.
This section considers just the protocol-related aspects of an RPC mechanism.
That is, it ignores the stubs and focuses instead on the RPC protocol that transmits
messages between client and server; the transformation of arguments into messages
and vice versa is covered in Chapter 7. Furthermore, since RPC is a generic term—
rather than a specific standard like TCP—we are going to take a different approach
than we did in the previous section. Instead of organizing the discussion around an
existing standard (i.e., TCP) and then pointing out alternative designs at the end, we
are going to walk you through the thought process involved in designing an RPC
protocol. That is, we will design our own RPC protocol from scratch—considering
the design options at every step of the way—and then come back and describe some
widely used RPC protocols by comparing and contrasting them to the protocol we
just designed.
Before jumping in, however, we note that an RPC protocol performs a rather
complicated set of functions, and so instead of treating RPC as a single, monolithic
protocol, we develop it as a “stack” of three smaller protocols: BLAST, CHAN, and
SELECT. Each of these smaller protocols, which we sometimes call a microprotocol,
contains a single algorithm that addresses one of the problems outlined at the start of
this chapter. As a brief overview:
¦ BLAST: fragments and reassembles large messages
¦ CHAN: synchronizes request and reply messages
¦ SELECT: dispatches request messages to the correct process
These microprotocols are complete, self-contained protocols that can be used in different
combinations to provide different end-to-end services. Section 5.3.4 shows how
they can be combined to implement RPC.
Just to be clear, BLAST, CHAN, and SELECT are not standard protocols in the
sense that TCP, UDP, and IP are. They are simply protocols of our own invention, but
ones that demonstrate the algorithms needed to implement RPC. Because this section
is not constrained by the artifacts of what has been designed in the past, it provides a
particularly good opportunity to examine the principles of protocol design.
408 5 End-to-End Protocols
5.3.1 Bulk Transfer (BLAST)
The first problem we are going to tackle is
how to turn an underlying network that delivers
messages of some small size (say, 1 KB)
into a service that delivers messages of a
much larger size (say, 32 KB). While 32 KB
does not qualify as “arbitrarily large,” it is
large enough to be of practical use for many
applications, including most distributed file
systems. Ultimately, a stream-based protocol
like TCP (see Section 5.2) will be
needed to support an arbitrarily large message,
since any message-oriented protocol
will necessarily have some upper limit to the
size of the message it can handle, and you
can always imagine needing to transmit a
message that is larger than this limit.
We have already examined the basic
technique that is used to transmit a large
message over a network that can accommodate
only smaller messages—fragmentation
and reassembly. We now describe the
BLAST protocol, which uses this technique.
One of the unique properties of
BLAST is how hard it tries to deliver all
the fragments of a message. Unlike the
AAL segmentation/reassembly mechanism
used with ATM (see Section 3.3) or the IP
fragmentation/reassembly mechanism (see
Section 4.1), BLAST attempts to recover
from dropped fragments by retransmitting
them. However, BLAST does not go so far
as to guarantee message delivery. The significance
of this design choice will become
clear later in this section.
What Layer Is RPC?
Once again, the “What layer is
this?” issue raises its ugly head.
To many people, especially those
who adhere to the Internet architecture,
RPC is implemented on
top of a transport protocol (usually
UDP) and so cannot itself (by
definition) be a transport protocol.
It is equally valid, however, to argue
that the Internet should have
an RPC protocol, since it offers
a process-to-process service that is
fundamentally different from that
offered by TCP and UDP. The
usual response to such a suggestion,
however, is that the Internet
architecture does not prohibit network
designers from implementing
their own RPC protocol on top of
UDP. (In general, UDP is viewed
as the Internet architecture’s “escape
hatch,” since effectively it just
adds a layer of demultiplexing to
IP.) Whichever side of the issue of
whether the Internet should have
an official RPC protocol you support,
the important point is that
the way you implement RPC in
the Internet architecture says nothing
about whether RPC should be
BLAST Algorithm
The basic idea of BLAST is for the sender to break a large message passed to it by
some high-level protocol into a set of smaller fragments, and then for it to transmit
5.3 Remote Procedure Call 409
considered a transport protocol or
not.
Interestingly, there are other
people who believe that RPC is
the most interesting protocol in the
world and that TCP/IP is just what
you do when you want to go “off
site.” This is the predominant view
of the operating systems community,
which has built countless OS
kernels for distributed systems that
contain exactly one protocol—you
guessed it, RPC—running on top of
a network device driver.
The water gets even muddier
when you implement RPC as
a combination of three different
microprotocols, as is the case in this
section. In such a situation, which
of the three is the “transport” protocol?
Our answer to this question
is that any protocol that offers
process-to-process service, as opposed
to node-to-node or host-tohost
service, qualifies as a transport
protocol. Thus, RPC is a transport
protocol and, in fact, can be implemented
from a combination of
microprotocols that are themselves
valid transport protocols.
these fragments back-to-back over the
network. Hence the name BLAST—the protocol
does not wait for any of the fragments
to be acknowledged before sending
the next. The receiver then sends
a selective retransmission request (SRR)
back to the sender, indicating which fragments
arrived and which did not. (The
SRR message is sometimes called a par-
tial or selective acknowledgment.) Finally,
the sender retransmits the missing fragments.
In the case in which all the fragments
have arrived, the SRR serves to fully
acknowledge the message. Figure 5.13 gives
a representative timeline for the BLAST
protocol.
We now consider the send and receive
sides of BLAST in more detail. On
the sending side, after fragmenting the message
and transmitting each of the fragments,
the sender sets a timer called DONE. Whenever
an SRR arrives, the sender retransmits
the requested fragments and resets timer
DONE. Should the SRR indicate that all
the fragments have arrived, the sender frees
its copy of the message and cancels timer
DONE. If timer DONE ever expires, the
sender frees its copy of the message; that
is, it gives up.
On the receiving side, whenever the
first fragment of a message arrives, the receiver
initializes a data structure to hold the
individual fragments as they arrive and sets
a timer LAST FRAG. This timer counts the
time that has elapsed since the last fragment
arrived. Each time a fragment for that message arrives, the receiver adds it to this data
structure, and should all the fragments then be present, it reassembles them into a
complete message and passes this message up to the higher-level protocol. There are
four exceptional conditions, however, that the receiver watches for:
410 5 End-to-End Protocols
Sender Receiver
Fragment 1
Fragment 2
Fragment 3
Fragment 5
Fragment 4
Fragment 6
Fragment 3
Fragment 5
SRR
SRR
Figure 5.13 Representative timeline for BLAST.
¦ If the last fragment arrives (the last fragment is specially marked) but
the message is not complete, then the receiver determines which fragments
are missing and sends an SRR to the sender. It also sets a timer called
RETRY.
¦ If timer LAST FRAG expires, then the receiver determines which fragments
are missing and sends an SRR to the sender. It also sets timer RETRY.
¦ If timer RETRY expires for the first or second time, then the receiver determines
which fragments are still missing and retransmits an SRR message.
¦ If timer RETRY expires for the third time, then the receiver frees the fragments
that have arrived and cancels timer LAST FRAG; that is, it gives up.
There are three aspects of BLAST worth noting. First, two different events trigger
the initial transmission of an SRR: the arrival of the last fragment and the firing of the
LAST FRAG timer. In the case of the former, because the network may reorder packets,
5.3 Remote Procedure Call 411
the arrival of the last fragment does not necessarily imply that an earlier fragment is
missing (it may just be late in arriving), but since this is the most likely explanation,
BLAST aggressively sends an SRR message. In the latter case, we deduce that the last
fragment was either lost or seriously delayed.
Second, the performance of BLAST does not critically depend on how carefully
the timers are set. Timer DONE is used only to decide that it is time to give up
and delete the message that is currently being worked on. This timer can be set to a
fairly large value since its only purpose is to reclaim storage. Timer RETRY is only
used to retransmit an SRR message. Any time the situation is so bad that a protocol
is reexecuting a failure recovery process, performance is the last thing on its mind.
Finally, timer LAST FRAG has the potential to influence performance—it sometimes
triggers the sending by the receiver of an SRR message—but this is an unlikely event:
It only happens when the last fragment of the message happens to get dropped in the
network.
Third, while BLAST is persistent in asking for and retransmitting missing fragments,
it does not guarantee that the complete message will be delivered. To understand
this, suppose that a message consists of only one or two fragments and that these fragments
are lost. The receiver will never send an SRR, and the sender’s DONE timer
will eventually expire, causing the sender to release the message. To guarantee delivery,
BLAST would need for the sender to time out if it does not receive an SRR and
then retransmit the last set of fragments it had transmitted. While BLAST certainly
could have been designed to do this, we chose not to because the purpose of BLAST is
to deliver large messages, not to guarantee message delivery. Other protocols can be
configured on top of BLAST to guarantee message delivery. You might wonder why
we put any retransmission capability at all into BLAST if we need to put a guaranteed
delivery mechanism above it anyway. The reason is that we’d prefer to retransmit
only those fragments that were lost rather than having to retransmit the entire larger
message whenever one fragment is lost. So we get the guarantees from the higher-level
protocol but some improved efficiency by retransmitting fragments in BLAST.
BLAST Message Format
The BLAST header has to convey several pieces of information. First, it must contain
some sort of message identifier so that all the fragments that belong to the same
message can be identified. Second, there must be a way to identify where in the original
message the individual fragments fit, and likewise, an SRR must be able to indicate
which fragments have arrived and which are missing. Third, there must be a way to
distinguish the last fragment, so that the receiver knows when it is time to check to
see if all the fragments have arrived. Finally, it must be possible to distinguish a data
412 5 End-to-End Protocols
Data
ProtNum
MID
Length
NumFrags Type
FragMask
0 16 31
Figure 5.14 Format for BLAST message header.
message from an SRR message. Some of these items are encoded in a header field in an
obvious way, but others can be done in a variety of different ways. Figure 5.14 gives
the header format used by BLAST. The following discussion explains the various fields
and considers alternative designs.
The MID field uniquely identifies this message. All fragments that belong to the
same message have the same value in their MID field. The only question is how many
bits are needed for this field. This is similar to the question of how many bits are needed
in the SequenceNum field for TCP. The central issue in deciding how many bits to use
in the MID field has to do with how long it will take before this field wraps around
and the protocol starts using message ids over again. If this happens too soon—that
is, the MID field is only a few bits long—then it is possible for the protocol to become
confused by a message that was delayed in the network, so that an old incarnation of
some message id is mistaken for a new incarnation of that same id. So, how many bits
are enough to ensure that the amount of time it takes for the MID field to wrap around
is longer than the amount of time a message can potentially be delayed in the network?
In the worst-case scenario, each BLAST message contains a single fragment that is
1 byte long, which means that BLAST might need to generate a new MID for every byte
it sends. On a 10-Mbps Ethernet, this would mean generating a new MID roughly once
every microsecond, while on a 1.2-Gbps STS-24 link, a new MID would be required
once every 7 nanoseconds. Of course, this is a ridiculously conservative calculation—
the overhead involved in preparing a message is going to be more than a microsecond.
Thus, suppose a new MID is potentially needed once every microsecond, and a message
may be delayed in the network for up to 60 seconds (our standard worst-case
5.3 Remote Procedure Call 413
assumption for the Internet); then we need to ensure that there are more than 60
million MID values. While a 26-bit field would be sufficient (226 = 67,108,864), it is
easier to deal with header fields that are even multiples of a byte, so we will settle on
a 32-bit MID field.
? This conservative (you could say paranoid) analysis of the MID field illustrates an
important point. When designing a transport protocol, it is tempting to take shortcuts,
since not all networks suffer from all the problems listed in the problem statement at
the beginning of this chapter. For example, messages do not get stuck in an Ethernet
for 60 seconds, and similarly, it is physically impossible to reorder messages on an
Ethernet segment. The problem with this way of thinking, however, is that if you want
the transport protocol to work over any kind of network, then you have to design for
the worst case. This is because the real danger is that as soon as you assume that an
Ethernet does not reorder packets, someone will come along and put a bridge or a
router in the middle of it.
Let’s move on to the other fields in the BLAST header. The Type field indicates
whether this is a DATA message or an SRR message. Notice that while we certainly don’t
need 16 bits to represent these two types, as a general rule we like to keep the header
fields aligned on 32-bit (word) boundaries, so as to improve processing efficiency.
The ProtNum field identifies the high-level protocol that is configured on top of BLAST;
incoming messages are demultiplexed to this protocol. The Length field indicates how
many bytes of data are in this fragment; it has nothing to do with the length of the
entire message. The NumFrags field indicates how many fragments are in this message.
This field is used to determine when the last fragment has been received. An alternative
is to include a flag that is only set for the last fragment.
Finally, the FragMask field is used to distinguish among fragments. It is a 32-bit
field that is used as a bit mask. For messages of Type = DATA, the ith bit is 1 (all
others are 0) to indicate that this message carries the ith fragment. For messages of
Type = SRR, the ith bit is set to 1 to indicate that the ith fragment has arrived, and it
is set to 0 to indicate that the ith fragment is missing. Note that there are several ways
to identify fragments. For example, the header could have contained a simple “fragment
ID” field, with this field set to i to denote the ith fragment. The tricky part with
this approach, as opposed to a bit-vector, is how the SRR specifies which fragments
have arrived and which have not. If it takes an n-bit number to identify each missing
fragment—as opposed to a single bit in a fixed-size bit-vector—then the SRR message
will be of variable length, depending on how many fragments are missing. Variablelength
headers are allowed, but they are a little trickier to process. On the other hand,
one limitation of the BLAST header given above is that the length of the bit-vector
limits each message to only 32 fragments. If the underlying network has an MTU of
1 KB, then this is sufficient to send up to 32-KB messages.
414 5 End-to-End Protocols
5.3.2 Request/Reply (CHAN)
The next microprotocol, CHAN, implements the request/reply algorithm that is at the
core of RPC. In terms of the common properties of transport protocols given in the
problem statement at the beginning of this chapter, CHAN guarantees message delivery,
ensures that only one copy of each message is delivered, and allows the communicating
processes to synchronize with each other. In the case of this last item,
the synchronization we are after mimics the behavior of a procedure call—the caller
(client) blocks while waiting for a reply from the callee (server).
At-Most-Once Semantics
The name CHAN comes from the fact that the protocol implements a logical request/
reply channel between a pair of participants. At any given time, there can be only one
message transaction active on a given channel. Like the concurrent logical channel
protocol described in Section 2.5.3, the application programs have to open multiple
channels if they want to have more than one request/reply transaction between them
at the same time.
The most important property of each channel is that it preserves a semantics
known as at-most-once. This means that for every request message that the client
sends, at most one copy of that message is delivered to the server. Stated in terms of
the RPC mechanism that CHAN is designed to support, for each time the client calls a
remote procedure, that procedure is invoked at most one time on the server machine.
We say “at-most-once” rather than “exactly once” because it is always possible that
either the network or the server machine has failed, making it impossible to deliver
even one copy of the request message.
As obvious as at-most-once sounds, not all RPC protocols support this behavior.
Some support a semantics that is facetiously called zero-or-more semantics, that is,
each invocation on a client results in the remote procedure being invoked zero or
more times. It is not hard to understand how this would cause problems for a remote
procedure that changed some local state variable (e.g., incremented a counter) or
that had some externally visible side effect (e.g., launched a missile) each time it was
invoked. On the other hand, if the remote procedure being invoked is idempotent—
multiple invocations have the same effect as just one—then the RPC mechanism need
not support at-most-once semantics; a simpler (possibly faster) implementation will
suffice.
CHAN Algorithm
The request/reply algorithm has several subtle aspects; hence, we develop it in stages.
The basic algorithm is straightforward, as illustrated by the timeline given in Figure
5.15. The client sends a request message and the server acknowledges it. Then, after
5.3 Remote Procedure Call 415
Client Server
Request
ACK
Reply
ACK
Figure 5.15 Simple timeline for CHAN.
Client Server
Request 1
Request 2
Reply 2
Reply 1
…
Figure 5.16 Timeline for CHAN when using implicit ACKs.
executing the procedure, the server sends a reply message and the client acknowledges
the reply.
Because the reply message often comes back with very little delay, and it is sometimes
the case that the client turns around and makes a second request on the same
channel immediately after receiving the first reply, this basic scenario can be optimized
by using a technique called implicit acknowledgments. As illustrated in Figure 5.16,
the reply message serves to acknowledge the request message, and a subsequent request
acknowledges the preceding reply.
416 5 End-to-End Protocols
There are two factors that potentially complicate the rosy picture we have painted
so far. The first is that either a message carrying data (a request message or a reply
message) or the ACK sent to acknowledge that message may be lost in the network.
To account for this possibility, both client and server save a copy of each message
they send until an ACK for it has arrived. Each side also sets a RETRANSMIT
timer and resends the message should this timer expire. Both sides reset this timer
and try again some agreed-upon number of times before giving up and freeing the
message.
Recall from Section 2.5.1 that this acknowledgment/timeout strategy means that
it is possible for duplicate copies of a message to arrive—the original message arrives,
the ACK is lost, and then the retransmission arrives. Thus, the receiver must remember
what messages it has seen and discard any duplicates. This is done through the use of a
MID field in the header. Any message whose MID field does not match the next expected
MID is discarded instead of being passed up to the high-level protocol configured on
top of CHAN.
The second complication is that the server may take an arbitrarily long time
to produce the result, and worse yet, it may crash (either the process or the entire
machine) before generating the reply. Keep in mind that we are talking about the period
of time after the server has acknowledged the request but before it has sent the reply.
To help the client distinguish between a slow server and a dead server, CHAN’s client
side periodically sends an “Are you alive?” message to the server, and CHAN’s server
side responds with an ACK. Alternatively, the server could send “I am still alive”
messages to the client without the client having first solicited them, but we prefer the
client-initiated approach because it keeps the server as simple as possible (i.e., it has
one less timer to manage).
CHAN Message Format
The CHAN message format is given in Figure 5.17. As with BLAST, the Type field
specifies the type of the message; in this case, the possible types are REQ, REP, ACK,
and PROBE. (PROBE is the “Are you alive?” message discussed above.) Similarly, the
ProtNum field identifies the high-level protocol that depends on CHAN.
The CID field uniquely identifies the logical channel to which this message belongs.
This is a 16-bit field, meaning that CHAN supports up to 64K concurrent request/reply
transactions between any pair of hosts. Of course, a given host can be participating in
channels with many other hosts at the same time.
The MID field uniquely identifies each request/reply pair; the reply message has
the same MID as the request. Note that because CHAN permits only one message
transaction at a time on a given channel, you might think that a 1-bit MID field is
sufficient, just as for the stop-and-wait algorithm presented in Section 2.5.1. However,
5.3 Remote Procedure Call 417
Data
Type
MID
BID
Length
ProtNum
CID
0 16 31
Figure 5.17 Format for CHAN message header.
as with BLAST, we have to be concerned about messages that wander around the
network for an extended period of time and then suddenly appear at the destination,
confusing CHAN. Thus, using much the same reasoning as we used in Section 5.3.1,
CHAN uses a 32-bit MID field.
Finally, the BID field gives the boot id for the host. A machine’s boot id is a
number that is incremented each time the machine reboots; this number is read from
disk, incremented, and written back to disk during the machine’s startup procedure.
This number is then put in every message sent by that host. The role played by the
BID field is much the same as the role played by the large MID field—it protects against
old messages suddenly appearing at the destination—although in this case, the old
message is due not to an arbitrary delay in the network but rather to a machine that
has crashed and rebooted.
To understand the use of the boot id, consider the following pathological situation.
A client machine sends a request message with MID = 0, then crashes and
reboots, and then sends an unrelated request message, also with MID = 0. The server
may not have been aware that the client crashed and rebooted, and upon seeing a request
message with MID = 0, acknowledges it and discards it as a duplicate. To protect
against this possibility, each side of CHAN makes sure that the BID, MID pair, not
just the MID, matches what it is expecting. BID is also a 32-bit field, which means that
if we assume that it takes at least 10 minutes to reboot a machine, it will wrap around
once every 40 billion minutes (approximately 80,000 years). In effect, the BID and
MID combine to form a unique 64-bit id for each transaction; the low-order 32 bits
are incremented for each transaction but reset to 0 when the machine reboots, and the
high-order 32 bits are incremented each time the machine reboots.
418 5 End-to-End Protocols
Timeouts
CHAN involves three different timers: There is a RETRANSMIT timer on both the
client and server, and the client also manages a PROBE timer. The PROBE timer is
not critical to performance and thus can be set to a conservatively large value—on
the order of several seconds. The RETRANSMIT timer, however, does influence the
performance of CHAN. If it is set too large, then CHAN might wait an unnecessarily
long time before retransmitting a message that was lost by the network. This clearly
hurts performance. If the RETRANSMIT timer is set too small, however, then CHAN
may load the network with unnecessary traffic.
If CHAN is designed to run on a local area network only, or even over a campussize
extended LAN, then RETRANSMIT can be set to a fixed value. Something on the
order of 20 milliseconds would be reasonable. This is because the RTT of a LAN is not
that variable. If CHAN is expected to run over the Internet, however, then selecting
a suitable value for RETRANSMIT is similar to the problem faced by TCP. Thus,
CHAN would calculate the RETRANSMIT timeout using a mechanism similar to the
one described in Section 5.2.6. The only difference is that CHAN has to take into
account the fact that the message it is sending ranges in size from 1 byte to 32 KB,
whereas TCP is always transmitting segments of approximately the same size.
Synchronous versus Asynchronous Protocols
One way to characterize a protocol is by whether it is synchronous or asynchronous.
These two terms have significantly different meanings, depending on where in the
protocol hierarchy you use them. At the transport layer, it is most accurate to think
of synchrony as a spectrum of possibilities rather than as two alternatives, where
the key attribute of any point along the spectrum is how much the sender knows,
after the operation to send a message returns. In other words, if we assume that
an application program invokes a send operation on a transport protocol, then the
question is, Exactly what does the application know about the success of the operation
when the send operation returns?
At the asynchronous end of the spectrum, the application knows absolutely nothing
when send returns. It not only doesn’t know if the message was received by its peer,
but it doesn’t even know for sure that the message has successfully left the local machine.
At the synchronous end of the spectrum, the send operation typically returns a
reply message. That is, the application not only knows that the message it sent was received
by its peer, but it knows that the peer has returned an answer. Thus, synchronous
protocols implement the request/reply abstraction, while asynchronous protocols are
used if the sender wants to be able to transmit many messages without having to wait
for a response. Using this definition, CHAN is obviously a synchronous protocol.
5.3 Remote Procedure Call 419
Although we have not discussed them in this chapter, there are interesting points
between these two extremes. For example, the transport protocol might implement
send so that it blocks (does not return) until the message has been successfully received
at the remote machine, but returns before the sender’s peer on that machine has actually
processed and responded to it. This is sometimes called a reliable datagram protocol.
Implementation of CHAN
We conclude our discussion ofCHANby giving fragments of C code that implement its
client side. Reading code can be tedious, but if done judiciously, it can help to solidify
your understanding of how a system works. In the case of CHAN, it serves to illustrate
all the separate pieces that go into a protocol implementation—the function that sends
an outgoing message, the function that retransmits messages, and the function that
processes incoming messages—and how they interact with each other.
We begin with CHAN’s two key data structures: ChanHdr and ChanState. The
fields in ChanHdr have already been explained. The fields in ChanState will be explained
by the code that follows. Note that ChanState includes a hdr template field, which is
a copy of the CHAN header. Many of the fields in the CHAN header remain the same
for all messages sent out over this channel. These fields are filled in when the channel
is created (not shown); only the fields that change are modified before a given message
is transmitted.
typedef struct {
u_short Type; /* message type: REQ, REP, ACK, PROBE */
u_short CID; /* unique channel id */
int MID; /* unique message id */
int BID; /* unique boot id */
int Length; /* length of message */
int ProtNum; /* high-level protocol number */
} ChanHdr;
typedef struct {
u_char type; /* type of session: CLIENT or SERVER */
u_char status; /* status of session: BUSY or IDLE */
Event event; /* place to save timeout event */
int timeout; /* timeout value */
int retries; /* number of times retransmitted */
int ret_val; /* place to save return value */
Msg *request; /* place to save request message */
Msg *reply; /* place to save reply message */
Semaphore reply_sem; /* semaphore the client blocks on */
int mid; /* message id for this channel */
int bid; /* boot id for this channel */
420 5 End-to-End Protocols
ChanHdr hdr_template; /* header template for this channel */
BlastState blast; /* pointer to BLAST protocol */
} ChanState;
We now turn our attention to the function that sends request messages. Since
CHANexports a synchronous interface to higher-level protocols—the caller blocks until
a reply can be returned—the send operation we have been assuming since Chapter 1
is not going to work. Therefore, we introduce a new interface operation, which we
give the generic name call, that blocks until a reply message is available, and returns
that reply message to the caller. The first argument identifies the channel being
used; it effectively encapsulates all the information needed to send the message to
the correct destination. The second and third arguments correspond to the abstract
data type (ADT) for messages, and represent the request and reply messages, respectively.
We assume this ADT supports the obvious operations (e.g., msgSaveCopy and
msgLength).
int
callCHAN(ChanState *state, Msg *request, Msg *reply)
{
ChanHdr *hdr;
char hbuf[HLEN];
/* ensure only one transaction per channel */
if ((state->status != IDLE))
return FAILURE;
state->status = BUSY;
/* save a copy of request msg and pointer to reply msg*/
msgSaveCopy(&state->request, request);
state->reply = reply;
/* fill out header fields */
hdr = state->hdr_template;
hdr->Length = msgLength(request);
if (state->mid == MAX_MID)
state->mid = 0;
hdr->MID = ++state->mid;
/* attach header to msg and send it */
store_chan_hdr(hdr, hbuf);
msgAddHdr(request, hdr, HLEN);
sendBLAST(request);
/* schedule first timeout event */
state->retries = 1;
state->event = eventSchedule(retransmit, state, state->timeout);
5.3 Remote Procedure Call 421
/* block waiting for the reply msg */
semWait(&state->reply_sem);
/* clean up state and return */
flush_msg(state->request);
state->status = IDLE;
return state->ret_val;
}
The first thing to notice is that the ChanState passed as an argument to callCHAN
includes a field named status that indicates whether or not this channel is being used.
If the channel is currently in use, then callCHAN returns failure. An alternative design
would be to block the calling thread until the channel becomes idle. We have elected
to push responsibility for blocking threads that want to use busy channels onto the
higher-level protocol, in our case, SELECT.
The next thing to notice about call is that after filling out the message header
and transmitting the request message via BLAST, the calling process is blocked on a
semaphore (reply sem). When the reply message eventually arrives, it is processed by
CHAN’s deliverCHAN routine (see below), which copies the reply message into state
variable reply and signals this blocked process. The process then returns. Should the
reply message not arrive, then timeout routine retransmit is called (see below). This
event is scheduled in the body of callCHAN.
The next routine, retransmit, is called whenever the retransmit timer fires. It is
scheduled for the first time in callCHAN, but each time it is called, it reschedules itself.
Once the request message has been retransmitted four times,CHANgives up: It sets the
return value to FAILURE and wakes up the blocked client process. Finally, each time retransmit
executes and sends another copy of the request message, it needs to resave the
message in state variable request. This is because we assume that each time a protocol
sends a message to the lower-level protocol, it loses its reference to the message.
static void
retransmit(Event ev, int *arg)
{
ChanState *state = (ChanState *)arg;
Msg tmp;
/* unblock the client process if we have retried 4 times */
if (++state->retries > 4)
{
state->ret_val = FAILURE;
semSignal(state->rep_sem);
return;
}
/* retransmit request message */
422 5 End-to-End Protocols
msgSaveCopy(&tmp, &state->request);
sendBLAST(&tmp);
/* reschedule event with exponential backoff */
state->timeout = 2*state->timeout;
state->event = eventSchedule(retransmit, state, state->timeout);
}
Finally, we consider CHAN’s deliver routine. The first thing we observe is that
CHAN is an asymmetric protocol: The code that implements CHAN on the client
machine is completely distinct from the code that implements CHAN on the server
machine. This fact is stored in the CHAN state variable (type). Thus, the first thing
CHAN’s deliver routine does is check to see whether it is running on a server (i.e., it
expects REQ messages) or on a client (i.e., it expects REP messages), and then it calls the
appropriate client- or server-specific routine. In this case, we show the client-specific
routine, deliverClient.
static int
deliverClient(ChanState *state, Msg *msg)
{
ChanHdr hdr;
char *hbuf;
/* strip header and verify correctness */
hbuf = msgStripHdr(msg, HLEN);
load_chan_hdr(&hdr, hbuf);
if (!clnt_msg_ok(state, &hdr))
return FAILURE;
/* cancel retransmit timeout event */
eventCancel(state->event);
/* if this is an ACK, then schedule PROBE timer and exit*/
if (hdr.Type == ACK)
{
state->event = eventSchedule(probe, s, PROBE);
return SUCCESS;
}
/* msg is a REP; save it and signal blocked client */
msgSaveCopy(state->reply, msg);
state->ret_val = SUCCESS;
semSignal(&state->reply_sem);
return SUCCESS;
}
5.3 Remote Procedure Call 423
Routine deliverClient first checks to see if it has received the expected message,
for example, that it has the right MID, the right BID, and that the message is of type
REP or ACK. This check is made in subroutine clnt msg ok (not shown). If it is a valid
acknowledgment message, then deliverClient cancels the RETRANSMIT timer and
schedules the PROBE timer. The PROBE timer is not shown, but would be similar to
the RETRANSMIT timer given above. If the message is a valid reply, then deliverClient
cancels the RETRANSMIT timer, saves a copy of the reply message in state variable
reply, and wakes up the blocked client process. It is this client process that actually
returns the reply message to the high-level protocol; the process that called deliverClient
simply returns back down the protocol stack.
5.3.3 Dispatcher (SELECT)
The final microprotocol, called SELECT, dispatches request messages to the appropriate
procedure. It is the RPC protocol stack’s version of a demultiplexing protocol
like UDP; the main difference is that it is a synchronous protocol rather than an asynchronous
protocol. What this means is that on the client side, SELECT is given a procedure
number that the client wants to invoke, it puts this number in its header, and then
it invokes the call operation on a lower-level request/reply protocol like CHAN. When
this invocation returns, SELECT merely lets the return pass through to the client; it
has no real demultiplexing work to do. On the server side, SELECT uses the procedure
number it finds in its header to select the right local procedure to invoke. When this procedure
returns, SELECT simply returns to the low-level protocol that just invoked it.
It may seem that SELECT is so simple that it is not worthy of being treated as a
separate protocol. After all, CHAN already has its own demultiplexing field that could
be used to dispatch incoming request messages to the appropriate procedure. There
are two reasons why we elected to separate SELECT into a self-contained protocol.
The first is that doing so makes it possible to change the address space with
which procedures are identified simply by configuring a different version of SELECT
into the protocol graph. In some settings, it is sufficient to define a flat address space for
procedures—for example, a 16-bit selector field allows you to identify 64K different
procedures. In other settings, however, a flat address space is hard to manage—who
decides which procedure gets which procedure number? In this case, it might be better
to have a hierarchical address space, that is, a two-part procedure number. First,
each program could be given a program number, where a program corresponds to
something like a “file server” or a “name server.” Next, each program could be given
the responsibility to assign unique procedure numbers to its own procedures. For
example, within the file server program, read might be procedure 1, write might be
procedure 2, seek might be procedure 3, and so on, whereas within the name server
program, insert might be procedure 1 and lookup might be procedure 2.
424 5 End-to-End Protocols
The second reason we implement SELECT as its own protocol is that it provides
a good place to manage concurrency. Recall that CHAN supports at-most-once channels.
Suppose we want to allow applications running on this host to make multiple
outstanding calls to the same remote procedure. Since CHAN allows only one outstanding
call at a time, the only way to do this is to open multiple channels to the same
server. Each time a calling process invokes SELECT, it sends the process out on an
idle channel. If all the channels are currently active, then SELECT blocks the calling
process until a channel becomes idle.
5.3.4 Putting It All Together (SunRPC, DCE)
We are now ready to construct an RPC stack from the microprotocols described in the
three previous subsections. This section also explains two widely used RPC protocols—
SunRPC and DCE-RPC—in terms of our three microprotocols.
A Simple RPC Stack
Figure 5.18 depicts a simple protocol stack that implements RPC. At the bottom are the
protocols that implement the underlying network. Although this stack could contain
protocols corresponding to any of the networking technologies discussed in the three
previous chapters, we use IP running on top of an Ethernet for illustrative purposes.
On top of IP is BLAST, which turns the small message size of the underlying
network into a communication service that supports messages of up to 32 KB in
length. Notice that it is not strictly true that the underlying network provides for
only small messages; IP can handle messages of up to 64 KB. However, because IP
BLAST
ETH
IP
SELECT
CHAN
Figure 5.18 A simple RPC stack.
5.3 Remote Procedure Call 425
has to fragment such large messages before sending them out over the Ethernet, and
BLAST’s fragmentation/reassembly algorithm is superior to IP’s (because it is able to
selectively retransmit missing fragments), we prefer to treat IP as though it supports exactly
the same MTU as the underlying physical network. This puts the fragmentation/
reassembly burden on BLAST, unless IP has to perform fragmentation out in the middle
of the network somewhere.
Next, CHAN implements the request/reply algorithm. Recall that we chose not
to implement reliable delivery in BLAST, but instead postponed solving this issue until
a higher-level protocol. In this case, CHAN’s timeout and acknowledgment mechanism
makes sure messages are reliably delivered. Other protocols might use different
techniques to guarantee delivery or, for that matter, might choose not to implement
reliable delivery at all. This is an example of the end-to-end argument at work—do
not do at low levels of the system (e.g., BLAST) what has to be done at higher levels
(e.g., CHAN) anyway.
Finally, SELECT defines an address space for identifying remote procedures. As
suggested in Section 5.3.3, different versions of SELECT, each defining a different
method for identifying procedures, could be configured on top of CHAN. In fact, it
would even be possible to write a version of SELECT that mimics some existing RPC
package’s address space for procedures (such as SunRPC’s), and then to use CHAN
and BLAST underneath this new SELECT to implement the rest of the RPC stack.
This new stack would not interoperate with the original protocol, but it would allow
you to slide a new RPC system underneath an existing collection of remote procedures
without having to change the interface. SELECT also manages concurrency.
SunRPC
SELECT, CHAN, and BLAST, although complete and correctly functioning protocols,
have been neither standardized nor widely adopted. We now turn our discussion to a
widely used RPC protocol—SunRPC. Ironically, SunRPC has also not been approved
by any standardization body, but it has become a de facto standard, thanks to its wide
distribution with Sun workstations and to the central role it plays in Sun’s popular
Network File System (NFS). At the time of this writing, the IETF is considering officially
adopting SunRPC as a standard Internet protocol.
Fundamentally, any RPC protocol must worry about three issues: fragmenting
large messages, synchronizing request and reply messages, and dispatching request
messages to the appropriate procedure. SunRPC is no exception. Unlike the
SELECT/CHAN/BLAST stack, however, SunRPC addresses these three functions in
a different order and uses slightly different algorithms. The basic SunRPC protocol
graph is given in Figure 5.19.
426 5 End-to-End Protocols
IP
ETH
SunRPC
UDP
Figure 5.19 Protocol graph for SunRPC.
First, SunRPC implements the core request/reply algorithm; it is CHAN’s counterpart.
SunRPC differs from CHAN, however, in that it does not technically guarantee
at-most-once semantics; there are obscure circumstances under which a duplicate
copy of a request message is delivered to the server (see below). Second, the role of
SELECT is split between UDP and SunRPC—UDP dispatches to the correct program,
and SunRPC dispatches to the correct procedure within the program. (We discuss how
procedures are identified in more detail below.) Finally, the ability to send request and
reply messages that are larger than the network MTU, corresponding to the functionality
implemented in BLAST, is handled by IP. Keep in mind, however, that IP is
not as persistent as BLAST is in implementing fragmentation; BLAST uses selective
retransmission, whereas IP does not.
As just mentioned, SunRPC uses two-tier addresses to identify remote procedures:
a 32-bit program number and a 32-bit procedure number. (There is also a
32-bit version number, but we ignore that in the following discussion.) For example,
the NFS server has been assigned program number x00100003, and within this program,
getattr is procedure 1, setattr is procedure 2, read is procedure 6, write is
procedure 8, and so on. Each program is reachable by sending a message to some
UDP port. When a request message arrives at this port, SunRPC picks it up and calls
the appropriate procedure.
To determine which port corresponds to a particular SunRPC program number,
there is a separate SunRPC program, called the Port Mapper, that maps program numbers
to port numbers. The Port Mapper itself also has a program number (x00100000)
that must be translated into some UDP port. Fortunately, the Port Mapper is always
present at a well-known UDP port (111). The Port Mapper program supports several
procedures, one of which (procedure number 3) is the one that performs the programto-
port number mapping.
5.3 Remote Procedure Call 427
Thus, to send a request message to NFS’s read procedure, a client first sends a
request message to the Port Mapper at well-known UDP port 111, asking that procedure
3 be invoked to map program number x00100003 to the UDP port where the
NFS program currently resides. (In practice, NFS is such an important program that
it is given its own well-known UDP port, so the Port Mapper need not be involved in
finding it.) The client then sends a SunRPC request message with procedure number
6 to this UDP port, and the SunRPC module listening at that port calls the NFS read
procedure. The client also caches the program-to-port number mapping so that it need
not go back to the Port Mapper each time it wants to talk to the NFS program.
The actual SunRPC header is defined by a complex nesting of data structures.
Figure 5.20 gives the essential details for the case in which the call completes without
any problems. XID is a unique transaction id, much like CHAN’s MID field. The
reason that SunRPC cannot guarantee at-most-once semantics is that on the server
side, SunRPC does not remember that it has already seen a particular XID once it has
successfully completed the transaction. This is only a problem if the client retransmits
a request message as a result of a timeout and that request message is in transit at
exactly the same time as the reply to the original request is on its way from the server
back to the client. When the retransmitted request arrives at the server, it looks like a
new transaction, since the server thinks it has already completed the transaction with
Data
Data
MsgType = CALL
XID
(a) RPCVersion = 2
Program
Version
Procedure
Credentials (variable)
Verifier (variable)
MsgType = REPLY
XID
(b) Status = ACCEPTED
0 31 0 31
Figure 5.20 SunRPC header formats: (a) request; (b) reply.
428 5 End-to-End Protocols
this XID. Clearly, if the reply arrives at the client before the timeout, then the request
will not be retransmitted. Likewise, if the retransmitted request arrives at the server
before the reply has been generated, then the server will recognize that transaction XID
is already in progress, and it will discard the duplicate request message. So it is really
quite unlikely that this erroneous behavior will occur. Note that the server’s short-term
memory about XIDs also means that it cannot protect itself against messages that have
been delayed for a long time in the network. This has not been a serious problem with
SunRPC, however, because it was originally designed for use on a LAN.
Returning to the SunRPC header format, the request message contains variablelength
Credentials and Verifier fields, both of which are used by the client to authenticate
itself to the server, that is, to give evidence that the client has the right to invoke the
server. How a client authenticates itself to a server is a general issue that must be
addressed by any protocol that wants to provide a reasonable level of security. This
topic is discussed in more detail in Chapter 8.
DCE
The Distributed Computing Environment (DCE) defines another widely used RPC
protocol, which we call DCE-RPC. DCE is a set of standards and software for building
distributed systems. It was defined by the Open Software Foundation (OSF),
a consortium of computer companies that originally included IBM, Digital, and
Hewlett-Packard; today OSF goes by the name Open Group. DCE-RPC is the RPC
protocol at the core of the DCE system. It can be used with the Network Data
Representation (NDR) stub compiler described in Chapter 7, but it also serves as
the underlying RPC protocol for the Common Object Request Broker Architecture
(CORBA), which is an industrywide standard for building distributed, object-oriented
systems.
DCE-RPC is designed to run on top of UDP. It is similar to SunRPC in that it
defines a two-level addressing scheme: UDP demultiplexes to the correct server, DCERPC
dispatches to a particular procedure exported by that server, and clients consult an
“endpoint mapping service” (similar to SunRPC’s Port Mapper) to learn how to reach
a particular server. Unlike SunRPC, however, DCE-RPC implements at-most-once call
semantics. It does this in a single protocol that essentially combines the algorithms in
BLAST and CHAN. We focus our discussion on this aspect of DCE-RPC. (In truth,
DCE-RPC supports multiple call semantics, including an idempotent semantics similar
to SunRPC’s, but at-most-once is the default behavior.)
Figure 5.21 gives a timeline for the typical exchange of messages, where each
message is labelled by its DCE-RPC type. The pattern is similar to CHAN’s: The client
sends a Request message, the server eventually replies with a Response message, and
5.3 Remote Procedure Call 429
Client Server
Request
Ack
Response
Ping
Working
Ping
Working
Figure 5.21 Typical DCE-RPC message exchange.
the client acknowledges (Ack) the response. Instead of the server acknowledging the
request messages, however, the client periodically sends a Ping message to the server,
which responds with a Working message to indicate that the remote procedure is still
in progress. Although not shown in the figure, other message types are also supported.
For example, the client can send a Quit message to the server, asking it to abort an earlier
call that is still in progress; the server responds with a Quack (quit acknowledgment)
message. Also, the server can respond to a Request message with a Reject message
(indicating that a call has been rejected), and it can respond to a Ping message with a
Nocall message (indicating that the server has never heard of the caller).
In addition to the message type, request and reply messages include four key
fields that are used to implement both the fragmentation/reassembly aspects of BLAST
and the message transaction aspects of CHAN. These include ServerBoot, ActivityId,
SequenceNum, and FragmentNum.
The ServerBoot field serves the same purpose as CHAN’s BID (boot id) field: The
server records its boot time in a global variable each time it starts up, and it includes this
430 5 End-to-End Protocols
variable in each call it services. The ActivityId field is similar to CHAN’s CID (channel id)
field: It identifies a logical connection between the client and server on which a sequence
of calls can be made. The SequenceNum field then distinguishes between calls made
as part of the same activity; it serves the same purpose as CHAN’s MID (message id)
and SunRPC’s xid (transaction id) fields. Like CHAN (and unlike SunRPC), DCE-RPC
keeps track of the last sequence number used as part of a particular activity, so as to
ensure at-most-once semantics.
Because both request and response messages may be larger than the underlying
network packet size, they may be fragmented into multiple packets. The FragmentNum
field uniquely identifies each fragment that makes up a given request or reply message.
Unlike BLAST, which uses a bit-vector to identify fragments, each DCE-RPC fragment
is assigned a unique fragment number (e.g., 0, 1, 2, 3, and so on). Both the client and
server implement a selective acknowledgment mechanism, which works as follows.
(We describe the mechanism in terms of a client sending a fragmented request message
to the server; the same mechanism applies when a server sends a fragment response to
the client.)
First, each fragment that makes up the request message contains both a unique
FragmentNum and a flag indicating whether this packet is a fragment of a call (frag) or
the last fragment of a call (last frag); request messages that fit in a single packet carry a
no frag flag. The server knows it has received the complete request message when it has
the last frag packet and there are no gaps in the fragment numbers. Second, in response
to each arriving fragment, the server sends a Fack (fragment acknowledgment) message
to the client. This acknowledgment identifies the highest fragment number that the
server has successfully received. In other words, the acknowledgment is cumulative,
much like in TCP. In addition, however, the server selectively acknowledges any higher
fragment numbers it has received out of order. It does so with a bit-vector that identifies
these out-of-order fragments relative to the highest in-order fragment it has received.
Finally, the client responds by retransmitting the missing fragments.
Figure 5.22 illustrates how this all works. Suppose the server has successfully
received fragments up through number 20, plus fragments 23, 25, and 26. The server
responds with a Fack that identifies fragment 20 as the highest in-order fragment,
plus a bit-vector (SelAck) with the third (23 = 20 + 3), fifth (25 = 20 + 5), and
sixth (26 = 20 + 6) bits turned on. So as to support an (almost) arbitrarily long
bit-vector, the size of the vector (measured in 32-bit words) is given in the SelAckLen
field.
Given DCE-RPC’s support for very large messages—the FragmentNum field
is 16 bits long, meaning it can support 64K fragments—it is not appropriate for
the protocol to blast all the fragments that make up a message as fast as it can,
5.4 Performance 431
Client Server
Fack
19
20
21
22
23
24
25
26
21
22
24
Type = Fack
FragmentNum = 20
WindowSize = 10
SelAckLen = 1
SelAck[1] = 0x36 110100
6 + 20
5 + 20
3 + 20
FragmentNum
…
… … …
Figure 5.22 Fragmentation with selective acknowledgments.
as BLAST does, since doing so might overrun the receiver. Instead, DCE-RPC implements
a flow-control algorithm that is very similar to TCP’s. Specifically, each
Fack message not only acknowledges received fragments, but also informs the sender
of how many fragments it may now send. This is the purpose of the WindowSize
field in Figure 5.22, which serves exactly the same purpose as TCP’s Advertised-
Window field except it counts fragments rather than bytes. DCE-RPC also implements
a congestion-control mechanism that is similar to TCP’s, which we will see in
Chapter 6.
5.4 Performance
Recall that Chapter 1 introduced the two quantitative metrics by which network
performance is evaluated: latency and throughput. As mentioned in that discussion,
these metrics are influenced not only by the underlying hardware (e.g., propagation
delay and link bandwidth) but also by software overheads. Now that we have a
complete software-based protocol graph available to us that includes alternative
transport protocols, we can discuss how to meaningfully measure its performance.
432 5 End-to-End Protocols
Linux
kernel
Linux
kernel
100 Mbps
User process User process
Figure 5.23 Measured system: Two Pentium workstations running Linux connected by
a 100-Mbps Ethernet.
The importance of such measurements is that they represent the performance seen by
application programs.
We begin, as any report of experimental results should, by describing our experimental
method. This includes the apparatus used in the experiments. We ran our
experiments on a pair of 733-MHz Pentium workstations connected by an isolated
100-Mbps Ethernet. The Ethernet spanned a single machine room so propagation is
not an issue, making this a measure of processor/software overheads. Each workstation
was running the Linux operating system (2.4 kernel). A test program running on
top of the socket interface ping-pongs (reflects) messages between the two machines.
Figure 5.23 illustrates one round-trip between the two test programs.
Each experiment involved running three identical instances of the same test. Each
test, in turn, involved sending a message of some specified size back and forth between
the two machines 10,000 times. The system’s clock was read at the beginning and
end of each test, and the difference between these two times was divided by 10,000
to determine the time taken for each round-trip. The average of these three times
(the three runs of the test) is reported for each experiment below. Each experiment
involved a different-sized message. The latency numbers were for message sizes of
1 byte, 100 bytes, 200 bytes, . . . , 1000 bytes. The throughput results were for message
sizes of 1 KB, 2 KB, 4 KB, 8 KB, . . . , 32 KB.
Table 5.3 gives the results of the latency test. As you would expect, latency
increases with message size. Although there are sometimes special cases where you
might be interested in the latency of, say, a 200-byte message, typically the most
5.4 Performance 433
Message Size (Bytes) UDP TCP
1 58 66
100 76 84
200 93 104
300 111 124
400 132 136
500 150 159
600 167 176
700 184 194
800 203 210
900 223 228
1000 239 249
Table 5.3 Measured round-trip latencies (µs) for various message sizes and protocols.
important latency number is the 1-byte case. This is because the 1-byte case represents
the overhead involved in processing each message that does not depend on the amount
of data contained in the message. It is typically the lower bound on latency, representing
factors like the speed-of-light delay and the time taken to process headers. Note that
there is a small difference between the latency experienced by the two different protocol
stacks, with UDP round-trip times a bit less than for TCP. This is to be expected since
TCP provides more functionality.
The results of the throughput test are given in Figure 5.24. Here, we show only
the results for UDP. The key thing to notice in this graph is that throughput improves
as the messages get larger. This makes sense—each message involves a certain amount
of overhead, so a larger message means that this overhead is amortized over more
bytes. The throughput curve flattens off above 16 KB, at which point the per-message
overhead becomes insignificant when compared to the large number of bytes that the
protocol stack has to process.
A second thing to notice is that the throughput curve tops out before reaching
100 Mbps. Although it can’t be deduced from these measurements, it turns out that
the factor preventing our system from running at the full Ethernet speed is a limitation
of the network adaptor rather than the software.
434 5 End-to-End Protocols
1 2 4 8 16 32 64
Message size (KB)
Measured throughput (Mbps)
100
70
75
80
85
90
95
Figure 5.24 Measured throughput using UDP, for various message sizes.
5.5 Summary
This chapter has described three very different end-to-end protocols. The first protocol
we considered is a simple demultiplexer, as typified by UDP. All such a protocol does
is dispatch messages to the appropriate application process based on a port number.
It does not enhance the best-effort service model of the underlying network in any
way, or said another way, it offers an unreliable, connectionless datagram service to
application programs.
The second type is a reliable byte-stream protocol, and the specific example of
this type that we looked at is TCP. The challenges faced with such a protocol are to
recover from messages that may be lost by the network, to deliver messages in the same
order in which they are sent, and to allow the receiver to do flow control on the sender.
TCP uses the basic sliding window algorithm, enhanced with an advertised window, to
implement this functionality. The other item of note for this protocol is the importance
of an accurate timeout/retransmission mechanism. Interestingly, even though TCP is
a single protocol, we saw that it employs at least five different algorithms—sliding
Open Issue: Application-Specific Protocols 435
window, Nagle, three-way handshake, Karn/Partridge, and Jacobson/Karels—all of
which can be of value to any end-to-end protocol.
The third transport protocol we looked at is a request/reply protocol that forms
the basis for RPC. In this case, a combination of three different algorithms are employed
to implement the request/reply service: a selective retransmission algorithm that
is used to fragment and reassemble large messages, a synchronous channel algorithm
that pairs the request message with the reply message, and a dispatch algorithm that
causes the correct remote procedure to be invoked.
What should be clear after reading
this chapter is that transport protocol
design is a tricky business. As we
have seen, getting a transport protocol
right in the first place is hard
enough, but changing circumstances
make matters more complicated. The
O P E N I S S U E
Application-Specific Protocols
challenge is finding ways to adapt to these changes.
Our experience with using the protocol can change. As we saw with TCP’s timeout
mechanism, experience led to a series of refinements in how TCP decides to retransmit
a segment. None of these changes affected the format of the TCP header, however,
and so they could be incorporated into TCP one implementation at a time. That is,
there was no need for everyone to upgrade their version of TCP on the same day.
The characteristics of the underlying network can also change. For many years,
TCP’s 32-bit sequence number and 16-bit advertised window were more than adequate.
Recently, however, higher-bandwidth networks have meant that the sequence
number is not large enough to protect against wraparound, and the advertised window
is too small to allow the sender to fill the network pipe. While an obvious solution
would have been to redefine the TCP header to include a 64-bit sequence number field
and a 32-bit advertised window field, this would have introduced the very serious problem
of how several million Internet hosts would make the transition from the current
header to this new header. While such transitions have been performed on production
networks, including the telephone network, they are no trivial matter. It was decided,
therefore, to implement the necessary extensions as options and to allow hosts to negotiate
with each other as to whether or not they will use the options for each connection.
This approach will not work indefinitely, however, since the TCP header has room
for only 44 bytes of options. (This is because the HdrLen field is 4 bits long, meaning that
the total TCP header length cannot exceed 16×32 bit words, or 64 bytes.) Of course,
a TCP option that extends the space available for options is always a possibility, but
you have to wonder how far it is worth going for the sake of backward compatibility.
436 5 End-to-End Protocols
Perhaps the hardest changes to accommodate are the adaptations to the level of
service required by application programs. It is inevitable that some applications will
have a good reason for wanting a slight variation from the standard services. For example,
some applications want RPC most of the time, but occasionally want to be
able to send a stream of request messages without waiting for any of the replies. While
this is no longer technically the semantics of RPC, a common scenario is to modify
an existing RPC protocol to allow this flexibility. As another example, because video
is a stream-oriented application, it is tempting to use TCP as the transport protocol.
Unfortunately, TCP guarantees reliability, which is not important to the video application.
In fact, a video application would rather drop a frame (segment) than wait for it
to be retransmitted. Rather than invent a new transport protocol from scratch, however,
some designers have proposed that TCP should support an option that effectively
turns off its reliability feature. It seems that such a protocol could hardly be called TCP
anymore, but we are talking about the pragmatics of getting an application to run.
How to develop transport protocols that can evolve to satisfy diverse applications,
many of which have not yet been imagined, is a hard problem. It is possible
that the ultimate answer to this problem is the one-function-per-protocol style exemplified
by the microprotocols we used to implement RPC, or some similar mechanism
by which the application programmer is allowed to program, configure, or otherwise
customize the transport protocol.
F U R T H E R R E A D I N G
There is no doubt that TCP is a complex protocol and that in fact it has subtleties not
illuminated in this chapter. Therefore, the recommended reading list for this chapter
includes the original TCP specification. Our motivation for including this specification
is not so much to fill in the missing details as to expose you to what an honest-togoodness
protocol specification looks like. The other two papers in the recommended
reading list focus on RPC. The paper by Birrell and Nelson is the seminal paper on
the topic, while the article by O’Malley and Peterson describes the one-function-perprotocol
design philosophy in more detail.
¦ USC-ISI. Transmission Control Protocol. Request for Comments 793, September
1981.
¦ Birrell, A., and B. Nelson. Implementing remote procedure calls. ACM Trans-
actions on Computer Systems 2(1):39–59, February 1984.
¦ O’Malley, S., and L. Peterson. A dynamic network architecture. ACM Trans-
actions on Computer Systems 10(2):110–143, May 1992.
Exercises 437
Beyond the protocol specification, the most complete description of TCP, including
its implementation in Unix, can be found in [Ste94b] and [SW95]. Also, the third
volume of Comer and Stevens’s TCP/IP series of books describes how to write
client/server applications on top of TCP and UDP, using the Posix socket interface
[CS00], the Windows socket interface [CS97], and the System V Unix TLI interface
[CS94].
Several papers evaluate the performance of different transport protocols at a very
detailed level. For example, the article by Clark et al. [CJRS89] measures the processing
overheads of TCP, a paper by Mosberger et al. [MPBO96] explores the limitations of
protocol processing overheads, and Thekkath and Levy [TL93] and Schroeder and
Burrows [SB89] examine RPC’s performance in great detail.
The original TCP timeout calculation was described in the TCP specification (see
above), while the Karn/Partridge algorithm was described in [KP91] and the Jacobson/
Karels algorithm was proposed in [Jac88]. The TCP extensions are defined by Jacobson
et al. [JBB92], while O’Malley and Peterson [OP91] argue that extending TCP in this
way is not the right approach to solving the problem.
Finally, there are several distributed operating systems that have defined their
own RPC protocol. Notable examples include the V system, described by Cheriton and
Zwaenepoel [CZ85]; Sprite, described by Ousterhout et al. [OCD+88]; and Amoeba,
described by Mullender [Mul90]. The latest version of SunRPC, as defined by Srinivasan
[Sri95a], is a proposed standard for the Internet.
E X E R C I S E S
1 If a UDP datagram is sent from host A, port P to host B, port Q, but at host B there
is no process listening to port Q, then B is to send back an ICMP Port Unreachable
message to A. Like all ICMP messages, this is addressed to A as a whole, not to
port P on A.
(a) Give an example of when an application might want to receive such ICMP
messages.
(b) Find out what an application has to do, on the operating system of your choice,
to receive such messages.
(c) Why might it not be a good idea to send such messages directly back to the
originating port P on A?
2 Consider a simple UDP-based protocol for requesting files (based somewhat
loosely on the Trivial File Transport Protocol, TFTP). The client sends an
438 5 End-to-End Protocols
initial file request, and the server answers (if the file can be sent) with the first
data packet. Client and server then continue with a stop-and-wait transmission
mechanism.
(a) Describe a scenario by which a client might request one file but get another;
you may allow the client application to exit abruptly and be restarted with
the same port.
(b) Propose a change in the protocol that will make this situation much less
likely.
3 Design a simple UDP-based protocol for retrieving files from a server. No authentication
is to be provided. Stop-and-wait transmission of the data may be used.
Your protocol should address the following issues:
(a) Duplication of the first packet should not duplicate the “connection.”
(b) Loss of the final ACK should not necessarily leave the server in doubt as to
whether the transfer succeeded.
(c) A late-arriving packet from a past connection shouldn’t be interpretable as
part of a current connection.
4 This chapter explains three sequences of state transitions during TCP connection
teardown. There is a fourth possible sequence, which traverses an additional
arc (not shown in Figure 5.7) from FIN WAIT 1 to TIME WAIT and labelled
FIN + ACK/ACK. Explain the circumstances that result in this fourth teardown
sequence.
5 When closing a TCP connection, why is the two-segment-lifetime timeout not
necessary on the transition from LAST ACK to CLOSED?
6 A sender on a TCP connection that receives a 0 advertised window periodically
probes the receiver to discover when the window becomes nonzero. Why would the
receiver need an extra timer if it were responsible for reporting that its advertised
window had become nonzero (i.e., if the sender did not probe)?
7 Read the man page (orWindows equivalent) for the Unix/Windows utility netstat.
Use netstat to see the state of the local TCP connections. Find out how long closing
connections spend in TIME WAIT.
8 The sequence number field in the TCP header is 32 bits long, which is big enough
to cover over 4 billion bytes of data. Even if this many bytes were never transferred
Exercises 439
over a single connection, why might the sequence number still wrap around from
232 - 1 to 0?
9 You are hired to design a reliable byte-stream protocol that uses a sliding window
(like TCP). This protocol will run over a 100-Mbps network. The RTT of the
network is 100 ms, and the maximum segment lifetime is 60 seconds.
(a) How many bits would you include in the AdvertisedWindow and Sequence-
Num fields of your protocol header?
(b) How would you determine the numbers given above, and which values might
be less certain?
10 You are hired to design a reliable byte-stream protocol that uses a sliding window
(like TCP). This protocol will run over a 1-Gbps network. The RTT of the network
is 140 ms, and the maximum segment lifetime is 60 seconds. How many bits would
you include in the AdvertisedWindow and SequenceNum fields of your protocol
header?
11 Suppose a host wants to establish the reliability of a link by sending packets and
measuring the percentage that are received; routers, for example, do this. Explain
the difficulty of doing this over a TCP connection.
12 Suppose TCP operates over a 1-Gbps link.
(a) Assuming TCP could utilize the full bandwidth continuously, how long would
it take the sequence numbers to wrap around completely?
(b) Suppose an added 32-bit timestamp field increments 1000 times during the
wraparound time you found above. How long would it take for the timestamp
to wrap around?
13 Suppose TCP operates over a 40-Gbps STS-768 link.
(a) Assuming TCP could utilize the full bandwidth continuously, how long would
it take the sequence numbers to wrap around completely?
(b) Suppose an added 32-bit timestamp field increments 1000 times during the
wraparound time you found above. How long would it take for the timestamp
to wrap around?
14 If host A receives two SYN packets from the same port from remote host B, the
second may be either a retransmission of the original or else, if B has crashed and
rebooted, an entirely new connection request.
440 5 End-to-End Protocols
(a) Describe the difference as seen by host A between these two cases.
(b) Give an algorithmic description of what the TCP layer needs to do upon
receiving a SYN packet. Consider the duplicate/new cases above, and the possibility
that nothing is listening to the destination port.
15 Suppose x and y are two TCP sequence numbers. Write a function to determine
whether x comes before y (in the notation of Request for Comments 793, “x =<
y”) or after y; your solution should work even when sequence numbers wrap
around.
16 Suppose an idle TCP connection exists between sockets A and B. A third party
has eavesdropped and knows the current sequence number at both ends.
(a) Suppose the third party sends A a forged packet ostensibly from B and with
100 bytes of new data. What happens? Hint: Look up in Request for Com-
ments 793 what TCP does when it receives an ACK that is not an “acceptable
ACK.”
(b) Suppose the third party sends each end such a forged 100-byte data packet
ostensibly from the other end. What happens now? What would happen if A
later sent 200 bytes of data to B?
17 Suppose party A connects to the Internet via a dial-up IP server (e.g., using SLIP
or PPP), has several open Telnet connections (using TCP), and is cut off. Party B
then dials in and is assigned the same IP address that A had. Assuming B was
able to guess to what host(s) A had been connected, describe a sequence of probes
that could enable B to obtain sufficient state information to continue with A’s
connections.
18 Diagnostic programs are commonly available that record the first 100 bytes, say,
of every TCP connection to a certain host, port. Outline what must be done
with each received TCP packet, P, in order to determine if it contains data that
belongs to the first 100 bytes of a connection to host HOST, port PORT. Assume
the IP header is P.IPHEAD, the TCP header is P.TCPHEAD, and header fields are
as named in Figures 4.3 and 5.4. Hint: To get initial sequence numbers (ISNs)
you will have to examine every packet with the SYN bit set. Ignore the fact that
sequence numbers will eventually be reused.
19 If a packet arrives at host A with B’s source address, it could just as easily have
been forged by any third host C. If, however, A accepts a TCP connection from B,
then during the three-way handshake A sent ISNA to B’s address and received an
Exercises 441
acknowledgment of it. If C is not located so as to be able to eavesdrop on ISNA,
then it might seem that C could not have forged B’s response.
However, the algorithm for choosing ISNA does give other unrelated hosts a
fair chance of guessing it. Specifically, A selects ISNA based on a clock value at the
time of connection. Request for Comments 793 specifies that this clock value be
incremented every 4 µs; common Berkeley implementations once simplified this
to incrementing by 250,000 (or 256,000) once per second.
(a) Given this simplified increment-once-per-second implementation, explain how
an arbitrary host C could masquerade as B in at least the opening of a TCP
connection. You may assume that B does not respond to SYN + ACK packets
A is tricked into sending to it.
(b) Assuming real RTTs can be estimated to within 40 ms, about how many tries
would you expect it to take to implement the strategy of part (a) with the
unsimplified “increment every 4 µs” TCP implementation?
20 The Nagle algorithm, built into most TCP implementations, requires the sender to
hold a partial segment’s worth of data (even if PUSHed) until either a full segment
accumulates or the most recent outstanding ACK arrives.
(a) Suppose the letters abcdefghi are sent, one per second, over a TCP connection
with an RTT of 4.1 seconds. Draw a timeline indicating when each packet is
sent and what it contains.
(b) If the above were typed over a full-duplex Telnet connection, what would the
user see?
(c) Suppose that mouse position changes are being sent over the connection. Assuming
that multiple position changes are sent each RTT, how would a user
perceive the mouse motion with and without the Nagle algorithm?
21 Suppose a client C repeatedly connects via TCP to a given port on a server S, and
that each time it is C that initiates the close.
(a) How many TCP connections a second can C make here before it ties up all
its available ports in TIME WAIT state? Assume client ephemeral ports are in
the range 1024–5119, and that TIME WAIT lasts 60 seconds.
(b) Berkeley-derived TCP implementations typically allow a socket in TIME
WAIT state to be reopened before TIME WAIT expires, if the highest sequence
number used by the old incarnation of the connection is less than the
ISN used by the new incarnation. This solves the problem of old data accepted
as new; however, TIME WAIT also serves the purpose of handling late final
442 5 End-to-End Protocols
FINs. What would such an implementation have to do to address this and still
achieve strict compliance with the TCP requirement that a FIN sent anytime
before or during a connection’s TIME WAIT receive the same response?
22 Explain why TIME WAIT is a somewhat more serious problem if the server initiates
the close than if the client does. Describe a situation in which this might
reasonably happen.
23 What is the justification for the exponential increase in timeout value proposed by
Karn and Partridge? Why, specifically, might a linear (or slower) increase be less
desirable?
24 The Jacobson/Karels algorithm sets TimeOut to be 4 mean deviations above the
mean. Assume that individual packet round-trip times follow a statistical normal
distribution, for which 4 mean deviations are ? standard deviations. Using statistical
tables, for example, what is the probability that a packet will take more than
TimeOut time to arrive?
25 Suppose a TCP connection, with window size 1, loses every other packet. Those
that do arrive have RTT = 1 second. What happens? What happens to TimeOut?
Do this for two cases:
(a) After a packet is eventually received, we pick up where we left off, resuming
with EstimatedRTT initialized to its pretimeout value and TimeOut double that.
(b) After a packet is eventually received, we resume with TimeOut initialized to
the last exponentially backed-off value used for the timeout interval.
In the following four exercises, the calculations involved are straightforward
with a spreadsheet.
26 Suppose, in TCP’s adaptive retransmission mechanism, that EstimatedRTT is 4.0
at some point and subsequent measured RTTs all are 1.0. How long does it take
before the TimeOut value, as calculated by the Jacobson/Karels algorithm, falls
below 4.0? Assume a plausible initial value of Deviation; how sensitive is your
answer to this choice? Use ? = 1/8.
27 Suppose, in TCP’s adaptive retransmission mechanism, that EstimatedRTT is 90
at some point and subsequent measured RTTs all are 200. How long does it take
before the TimeOut value, as calculated by the Jacobson/Karels algorithm, falls
below 300? Assume initial Deviation value of 25; use ? = 1/8.
Exercises 443
28 Suppose TCP’s measured RTT is 1.0 except that every Nth RTT is 4.0. What
is the largest N, approximately, that doesn’t result in timeouts in the steady
state (i.e., for which the Jacobson/Karels TimeOut remains greater than 4.0)? Use
? = 1/8.
29 Suppose that TCP is measuring RTTs of 1.0 second, with a mean deviation of
0.1 second. Suddenly the RTT jumps to 5.0 seconds, with no deviation. Compare
the behaviors of the original and Jacobson/Karels algorithms for computing Time-
Out. Specifically, how many timeouts are encountered with each algorithm? What
is the largest TimeOut calculated? Use ? = 1/8.
30 Suppose that, when a TCP segment is sent more than once, we take SampleRTT to
be the time between the original transmission and the ACK, as in Figure 5.10(a).
Show that if a connection with a 1-packet window loses every other packet (i.e.,
each packet is transmitted twice), then EstimatedRTT increases to infinity. Assume
TimeOut = EstimatedRTT; both algorithms presented in the text always set
TimeOut even larger. Hint: EstimatedRTT = EstimatedRTT + ß × (SampleRTT -
EstimatedRTT).
31 Suppose that, when a TCP segment is sent more than once, we take SampleRTT
to be the time between the most recent transmission and the ACK, as in
Figure 5.10(b). Assume, for definiteness, that TimeOut = 2 × EstimatedRTT.
Sketch a scenario in which no packets are lost but EstimatedRTT converges to
a third of the true RTT, and give a diagram illustrating the final steady state.
Hint: Begin with a sudden jump in the true RTT to just over the established
TimeOut.
32 Consult Request for Comments 793 to find out how TCP is supposed to respond
if a FIN or an RST arrives with a sequence number other than NextByteExpected.
Consider both when the sequence number is within the receive window and when
it is not.
33 One of the purposes of TIME WAIT is to handle the case of a data packet from a
first incarnation of a connection arriving very late and being accepted as data for
the second incarnation.
(a) Explain why, for this to happen (in the absence of TIME WAIT), the hosts
involved would have to exchange several packets in sequence after the delayed
packet was sent but before it was delivered.
(b) Propose a network scenario that might account for such a late delivery.
444 5 End-to-End Protocols
34 Propose an extension to TCP by which one end of a connection can hand off
its end to a third host; that is, if A were connected to B, and A handed off its
connection to C, then afterwards C would be connected to B and A would not.
Specify the new states and transitions needed in the TCP state transition diagram,
and any new packet types involved. You may assume all parties will understand
this new option. What state should A go into immediately after the handoff?
35 TCP’s simultaneous open feature is seldom used.
(a) Propose a change to TCP in which this is disallowed. Indicate what changes
would be made in the state diagram (and if necessary in the undiagrammed
event responses).
(b) Could TCP reasonably disallow simultaneous close?
(c) Propose a change to TCP in which simultaneous SYNs exchanged by two
hosts lead to two separate connections. Indicate what state diagram changes
this entails, and also what header changes become necessary. Note that this
now means that more than one connection can exist over a given pair of
host, ports. (You might also look up the first “Discussion” item on page 87
of Request for Comments 1122.)
36 TCP is a very symmetric protocol, but the client/server model is not. Consider
an asymmetric TCP-like protocol in which only the server side is assigned a port
number visible to the application layers. Client-side sockets would simply be abstractions
that can be connected to server ports.
(a) Propose header data and connection semantics to support this. What will you
use to replace the client port number?
(b) What form does TIME WAIT now take? How would this be seen through the
programming interface? Assume that a client socket could now be reconnected
arbitrarily many times to a given server port, resources permitting.
(c) Look up the rsh/rlogin protocol. How would the above break this?
37 The following exercise is concerned with the TCP state FIN WAIT 2(see
Figure 5.7).
(a) Describe how a client might leave a suitable server in state FIN WAIT 2 indefinitely.
What feature of the server’s protocol is necessary here for this scenario?
(b) Try this with some appropriate existing server. Either write a stub client, or
use an existing Telnet client capable of connecting to an arbitrary port. Use
the netstat utility to verify that the server is in FIN WAIT 2 state.
Exercises 445
38 Request for Comments 1122 states (of TCP):
A hostMAY implement a “half-duplex” TCP close sequence, so that an application
that has called CLOSE cannot continue to read data from the connection.
If such a host issues a CLOSE call while received data is still pending in TCP, or
if new data is received after CLOSE is called, its TCP SHOULD send an RST to
show that data was lost.
Sketch a scenario involving the above in which data sent by (not to!) the closing
host is lost. You may assume that the remote host, upon receiving an RST, discards
all received data still unread in buffers.
39 When TCP sends a SYN, SequenceNum = x or FIN, SequenceNum = x, the
consequent ACK has Acknowledgment = x + 1; that is, SYNs and FINs each take
up one unit in sequence number space. Is this necessary? If so, give an example
of an ambiguity that would arise if the corresponding Acknowledgment were x
instead of x + 1; if not, explain why.
40 Find out the generic format for TCP header options from Request for Comments
793.
(a) Outline a strategy that would expand the space available for options beyond
the current limit of 44 bytes.
(b) Suggest an extension to TCP allowing the sender of an option a way of specifying
what the receiver should do if the option is not understood. List several
such receiver actions that might be useful, and try to give an example application
of each.
41 The TCP header does not have a BID field, like CHAN does. How does TCP
protect itself against the crash-and-reboot scenario that motivates CHAN’s BID?
Why doesn’t CHAN use this same strategy?
42 Suppose we were to implement remote file system mounting using an unreliable
RPC protocol that offers zero-or-more semantics. If a message reply is received,
this improves to at-least-once semantics. We define read() to return the specified
Nth block, rather than the next block in sequence; this way reading once is the
same as reading twice and at-least-once semantics is thus the same as exactly
once.
(a) For what other file system operations is there no difference between at-leastonce
and exactly once semantics? Consider open, create, write, seek, opendir,
readdir, mkdir, delete (aka unlink), and rmdir.
446 5 End-to-End Protocols
(b) For the remaining operations, which can have their semantics altered to achieve
equivalence of at-least-once and exactly once? What file system operations are
irreconcilable with at-least-once semantics?
(c) Suppose the semantics of the rmdir system call are now that the given directory
is removed if it exists, and nothing is done otherwise. How could you write a
program to delete directories that distinguishes between these two cases?
43 The RPC-based “NFS” remote file system is sometimes considered to have slower
than expected write performance. In NFS, a server’s RPC reply to a client write
request means that the data is physically written to the server’s disk, not just placed
in a queue.
(a) Explain the bottleneck we might expect, even with infinite bandwidth, if the
client sends all its write requests through a single logical CHAN channel, and
explain why using a pool of channels could help. Hint: You will need to know
a little about disk controllers.
(b) Suppose the server’s reply means only that the data has been placed in the
disk queue. Explain how this could lead to data loss that wouldn’t occur with
a local disk. Note that a system crash immediately after data was enqueued
doesn’t count because that would cause data loss on a local disk as well.
(c) An alternative would be for the server to respond immediately to acknowledge
the write request, and to send its own separate CHAN request later to confirm
the physical write. Propose different CHAN RPC semantics to achieve the
same effect, but with a single logical request/reply.
44 Both the BLAST and CHAN protocols have a MID field.
(a) Under what circumstances can these be equal, for several packets in a row?
(b) In the text, these fields were sequentially incremented. Can either of these fields
be a random number?
45 Suppose BLAST is used over a link with a 10% per-packet loss rate; losses are independent
events. Fragments that do arrive are not reordered, however. Messages
consist of six fragments.
(a) What is the probability, roughly, that LAST FRAG expires? Assume this happens
only when the last fragment is lost.
(b) What is the probability that the last fragment arrives but something else didn’t,
eliciting an SRR?
(c) What is the probability that no fragment arrives?
Exercises 447
46 Consider a client and server using an RPC mechanism that includes CHAN.
(a) Give a scenario involving server reboot in which an RPC request is sent
twice by the client and is executed twice by the server, with only a single
ACK.
(b) How might the client become aware this had happened? Would the client be
sure it had happened?
47 Suppose an RPC request is of the form “Increment the value of field X of disk
block N by 10%.” Specify a mechanism to be used by the executing server to
guarantee that an arriving request is executed exactly once, even if the server
crashes while in the middle of the operation. Assume that individual disk block
writes are either complete or else the block is unchanged. You may also assume
that some designated “undo log” blocks are available. Your mechanism should
include how the RPC server is to behave at restart.
48 Consider a SunRPC client sending a request to a server.
(a) Under what circumstances can the client be sure its request has executed exactly
once?
(b) Suppose we wished to add at-most-once semantics to SunRPC. What changes
would have to be made? Explain why adding one or more fields to the existing
headers would not be sufficient.
49 Suppose TCP were to be used as the underlying transport in an RPC protocol; one
TCP connection is to carry a stream of requests and replies. What are the analogs,
if any, to CHAN’s fields CID, MID, and BID, and Type values REQ, REP, ACK, and
PROBE? Which of these would the overlying RPC protocol have to provide?Would
some analog of implicit acknowledgments exist?
50 Suppose BLAST runs over a 10-Mbps Ethernet, sending 32K messages.
(a) If the Ethernet packets can hold 1500 bytes of data, and optionless IP headers
are used as well as BLAST headers, how many Ethernet packets are required
per message?
(b) Calculate the delay due to sending a 32K message over Ethernet
(i) directly
(ii) broken into pieces as in (a), with one bridge
Ignore propagation delays, headers, collisions, and interpacket gaps.
448 5 End-to-End Protocols
51 Write a test program that uses the socket interface to send messages between a pair
of Unix workstations connected by some LAN (e.g., Ethernet, ATM, or FDDI).
Use this test program to perform the following experiments.
(a) Measure the round-trip latency of TCP and UDP for different message sizes
(e.g., 1 byte, 100 bytes, 200 bytes, . . . , 1000 bytes).
(b) Measure the throughput of TCP and UDP for 1-KB, 2-KB, 3-KB, . . . , 32-KB
messages. Plot the measured throughput as a function of message size.
(c) Measure the throughput of TCP by sending 1 MB of data from one host to
another. Do this in a loop that sends a message of some size, for example,
1024 iterations of a loop that sends 1-KB messages. Repeat the experiment
with different message sizes and plot the results.
This Page Intentionally Left Blank
Congestion Control
and Resource
Allocation
The hand that hath made you fair hath made you good.
—William Shakespeare
By now we have seen enough layers of the network protocol hierarchy to understand
how data can be transferred among processes across heterogeneous
networks. We now turn to a problem that spans the entire protocol stack—
how to effectively and fairly allocate resources among a collection of competing users.
The resources being shared include the bandwidth of the links and the buffers on the
routers or switches where packets are queued awaiting transmission. Packets contend
at a router for the use of a link, with each contending packet placed in a queue waiting
its turn to be transmitted over the link. When too many packets are contending for
the same link, the queue overflows and packets have to be dropped. When such drops
become common events, the network is said to be congested. Most networks provide
a congestion-control mechanism to deal with just such a situation.
P R O B L E M
Allocating Resources
Congestion control and resource allocation
are two sides of the same
coin. On the one hand, if the network
takes an active role in allocating
resources—for example, scheduling
which virtual circuit gets to use
a given physical link during a certain
period of time—then congestion may
be avoided, thereby making congestion control unnecessary. Allocating network resources
with any precision is difficult, however, because the resources in question are
distributed throughout the network; multiple links connecting a series of routers need
6 to be scheduled. On the other hand, you can always let
packet sources send as much data as they want, and then
recover from congestion should it occur. This is the easier
approach, but it can be disruptive because many packets
may be discarded by the network before congestion can
be controlled. Furthermore, it is precisely at those times
when the network is congested—that is, resources have
become scarce relative to demand—that the need for resource
allocation among competing users is most keenly
felt. There are also solutions in the middle, whereby inexact
allocation decisions are made, but congestion can
still occur and hence some mechanism is still needed to
recover from it. Whether you call such a mixed solution
congestion control or resource allocation does not really
matter. In some sense, it is both.
Congestion control and resource allocation involve
both hosts and network elements such as routers. In network
elements, various queuing disciplines can be used
to control the order in which packets get transmitted and
which packets get dropped. The queuing discipline can
also segregate traffic, that is, to keep one user’s packets
from unduly affecting another user’s packets. At the end
hosts, the congestion-control mechanism paces how fast
sources are allowed to send packets. This is done in an
effort to keep congestion from occurring in the first place,
and should it occur, to help eliminate the congestion.
This chapter starts with an overview of congestion
control and resource allocation. We then discuss different
queuing disciplines that can be implemented on the
routers inside the network, followed by a description of
the congestion-control algorithm provided by TCP on the
hosts. The fourth section explores various techniques involving
both routers and hosts that aim to avoid congestion
before it becomes a problem. Finally, we examine the
broad area of “quality of service.” We consider the needs
of applications to receive different levels of resource allocation
in the network, and describe a number of ways in
which they can request these resources and the network
can meet the requests.
452 